{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\ndef get_only_true_data(X, key_options):\\n    \\n    training_keys_true = get_keys_for_fingerprinting_data(size=len(X), key_options=key_options)\\n    X_train_true = [x for x in X]\\n    y_train_true = get_fingerprints_for_ipds(len(X), sample_size=len(X[0]))#,training_keys=training_keys_true)\\n    \\n    \\n    X_train = np.expand_dims(X_train_true, axis=1).reshape((-1,900,1))\\n    y_train = np.expand_dims(y_train_true, axis=1).reshape((-1,900,1))\\n    training_keys = training_keys_true#,#np.expand_dims(training_keys_true, axis=1)\\n    \\n    return X_train, y_train, training_keys\\ndef get_false_training(key_length, X):\\n    false_value = 0\\n    k = [1]\\n    for i in range(key_length-1):\\n        k.append(0)\\n    k = np.array(k)\\n    y_train_non =  [np.zeros(len(x)) for x in X]#np.zeros(len(x))\\n    training_keys = [k for x in range(len(y_train_non))]\\n    \\n    return X, y_train_non, training_keys\\ndef get_false_true_training(X, key_options):\\n    n_false_train = int(len(X) / 50)\\n    n_true_train = len(X) - n_false_train\\n    \\n    training_keys_true = get_keys_for_fingerprinting_data(size=n_true_train, key_options=key_options)\\n    X_train_true = [x for x in X[0:n_true_train]]\\n    y_train_true = add_fignerprinting_to_ipds(n_true_train,sample_size=len(X[0]))#,training_keys=training_keys_true)\\n    \\n    X_train_false, y_train_false, training_keys_false = get_false_training(len(key_options[0]), X[-n_false_train:])\\n    \\n    print(len(X_train_false),len(X_train_true))\\n    X_train, y_train, training_keys = [], [], []\\n    f, t = 0, 0\\n    for i in range(n_false_train + n_true_train):\\n        rnd = random.randrange(0, 50)\\n        if rnd == 1 and f < len(X_train_false) :\\n            X_train.append(X_train_false[f])\\n            y_train.append(y_train_false[f])\\n            training_keys.append(training_keys_false[f])\\n            f += 1\\n        elif t < len(X_train_true):\\n            X_train.append(X_train_true[t])\\n            y_train.append(y_train_true[t])\\n            training_keys.append(training_keys_true[t])\\n            t += 1 \\n    \\n\\n    \\n    X_train = np.expand_dims(X_train, axis=1).reshape((-1, 900, 1))#np.array(X_train).reshape((-1, 900, 1))\\n    y_train = np.expand_dims(y_train, axis=1).reshape((-1, 900, 1))\\n    training_keys = np.array(training_keys)\\n    \\n    \\n    return X_train, y_train, training_keys\\ndef save_model_weights(model, name):\\n    model_json = model.to_json()\\n    with open(path + str(name) + \".json\", \"w\") as json_file:\\n        json_file.write(model_json)\\n    model.save_weights(path + str(name) + \".h5\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Activation, Dropout, Dense, Input, Add, Multiply, Concatenate, Lambda\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Flatten, Dot, Reshape\n",
    "from keras.models import Model\n",
    "import random, time, os\n",
    "import numpy as np\n",
    "from keras import losses\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.python import keras\n",
    "\n",
    "def create_sample_size_dataset(all_ipds, sample_size, n_sample):\n",
    "    #number_of_samples = int(len(all_ipds) / sample_size)\n",
    "    all_samples = []\n",
    "    for p in range(n_sample):\n",
    "        all_samples.append(all_ipds[p * sample_size:(p + 1) * sample_size])\n",
    "    return all_samples\n",
    "\n",
    "def write_array_to_file(array, target, delimiter):\n",
    "    for k in range(0, len(array)):\n",
    "        target.write(str(array[k]) + delimiter)\n",
    "    target.write(\"\\n\")\n",
    "\n",
    "def read_from_file(path):\n",
    "    with open(path, 'r') as content_file:\n",
    "        content = content_file.read()\n",
    "        return content\n",
    "\n",
    "\n",
    "def create_ipd_dataset(address):\n",
    "    files = os.listdir(address)\n",
    "    all_ipds = []\n",
    "    for f in files:\n",
    "            ipd = read_from_file(address + f).split(' ')\n",
    "            all_ipds.extend(convert_stringArrays_to_floatArray(ipd))\n",
    "    return all_ipds\n",
    "\n",
    "\n",
    "def isfloat(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def convert_stringArrays_to_floatArray(array):\n",
    "    intArray = []\n",
    "\n",
    "    for k in array:\n",
    "        if isfloat(k):\n",
    "            intArray.append(float(k))\n",
    "    return intArray\n",
    "\n",
    "\n",
    "def convert_stringArrays_to_intArray(array):\n",
    "    intArray = []\n",
    "\n",
    "    for k in array:\n",
    "        if isfloat(k):\n",
    "            intArray.append(int(k))\n",
    "    return intArray\n",
    "\n",
    "'''\n",
    "def create_fing_pattern(sample_size, keys):\n",
    "    all_patterns = {}\n",
    "    for i in range(len(keys)):\n",
    "        sample_pattern = []\n",
    "        k = \"\".join(str(x) for x in keys[i])\n",
    "        for j in range(0, sample_size):\n",
    "            rnd = random.randrange(0, 2)\n",
    "            sample_pattern.append(rnd)\n",
    "        all_patterns[k] = sample_pattern\n",
    "    return all_patterns\n",
    "\n",
    "def add_fignerprinting_to_ipds_sec(n_train, sample_size, training_keys):\n",
    "    \n",
    "    #Previous one was all + and the largest value was 50.\n",
    "    \n",
    "    key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "    patterns = create_fing_pattern(sample_size, key_options)\n",
    "    fingerprint_output = []\n",
    "    j = 0\n",
    "    while len(fingerprint_output) < n_train:\n",
    "        finger = [random.uniform(0, 250)]\n",
    "        neg_numbers = 0\n",
    "        k = \"\".join(str(x) for x in training_keys[j])\n",
    "        pat = patterns[k]\n",
    "        for i in range(sample_size - 1):\n",
    "            if pat [i] == 1:\n",
    "                finger.append(10)#random.uniform(0, 25))#random.uniform(0, 25))\n",
    "            else:\n",
    "                finger.append(-10)# * random.uniform(0, 25))# random.uniform(0, 25))\n",
    "        fingerprint_output.append(finger)\n",
    "        j += 1\n",
    "      \n",
    "    return fingerprint_output\n",
    "'''\n",
    "def get_fingerprints_for_ipds(n_train, sample_size, alpha):\n",
    "    \n",
    "    #Previous one was all + and the largest value was 50.\n",
    "   \n",
    "    fingerprint_output = []\n",
    "    while len(fingerprint_output) < n_train:\n",
    "        finger = [random.uniform(0, 250)]\n",
    "        neg_numbers = 0\n",
    "        #np.random.laplace(0, std, sample_size)\n",
    "        finger = np.random.laplace(0, 1, sample_size)\n",
    "        for i in range(sample_size - 1):\n",
    "            #if random.randrange(0, 3) == 0:\n",
    "            '''if random.randrange(0, 2) == 1:\n",
    "                finger.append(random.uniform(0, alpha))#random.uniform(0, 10)\n",
    "            else:\n",
    "                finger.append(-1 * random.uniform(0, alpha))#\n",
    "            if sum(finger) < 0:\n",
    "                neg_numbers += 1'''\n",
    "#             else:\n",
    "#                 finger.append(0) \n",
    "        #if neg_numbers < 50:  ## this can be a hyperparameter\n",
    "        fingerprint_output.append(finger)\n",
    "    return fingerprint_output\n",
    "\n",
    "def get_keys_for_fingerprinting_data(size, key_options):\n",
    "    selected_keys = []#np.array([key_options[0]])\n",
    "#     print(selected_keys,type(selected_keys))\n",
    "    for i in range(size - 1):\n",
    "        rnd = random.randrange(0, len(key_options))\n",
    "#         print([key_options[rnd]],type(key_options[0]))\n",
    "        #selected_keys = np.concatenate((selected_keys, np.array([key_options[rnd]])))\n",
    "        selected_keys.append(key_options[rnd])\n",
    "\n",
    "    return selected_keys\n",
    "\n",
    "\n",
    "def get_false_true_data(X, key_options,alpha):\n",
    "    \n",
    "    training_keys_true = get_keys_for_fingerprinting_data(size=len(X), key_options=key_options)\n",
    "    #X_train_true =X# [x for x in X]\n",
    "    y_train_true = get_fingerprints_for_ipds(len(X), sample_size=len(X[0]),alpha=alpha)#,training_keys=training_keys_true)\n",
    "    \n",
    "    ####### changing true trianing to false\n",
    "    \n",
    "    '''\n",
    "    false_key = np.zeros(len(key_options[0]))\n",
    "    false_key[0] = 1\n",
    "    ratio = len(key_options[0])\n",
    "    print(ratio, \"ratio\")\n",
    "    key_length = len(key_options[0])\n",
    "    num_false_train = int(len(X)/key_length )\n",
    "    print(\"finished getting true data\")\n",
    "    for i in range(num_false_train):\n",
    "        y_train_true[i * ratio] = np.zeros(len(X[0]))\n",
    "        training_keys_true[i * ratio] = false_key\n",
    "    \n",
    "    '''\n",
    "    X_train = np.expand_dims(X, axis=1).reshape((-1, len(X[0]), 1))\n",
    "    y_train = np.expand_dims(y_train_true, axis=1).reshape((-1, len(X[0]), 1))\n",
    "    #training_keys = training_keys_true#,#np.expand_dims(training_keys_true, axis=1)\n",
    "#     training_keys_true = np.array(training_keys_true)\n",
    "    \n",
    "    return X_train, y_train, training_keys_true\n",
    "\n",
    "def selecting_valid_fingerprints(key_length):\n",
    "    all_keys = []\n",
    "    #address = '/home/fatemeh/MyProjects/Fingerprint/Synthetic dataset/keys/' + str(key_length) + \"/\"\n",
    "    key_i =np.zeros(key_length)\n",
    "    #keys = os.listdir(address)\n",
    "    for k in range(key_length):\n",
    "        key_i =np.zeros(key_length)\n",
    "        key_i[k] = 1\n",
    "        #key_i = convert_stringArrays_to_intArray(read_from_file(address + k).split(\" \"))\n",
    "        #if key_i [0] == 1:\n",
    "         #   continue\n",
    "        all_keys.append(key_i)\n",
    "       # key_i[k]=0\n",
    "            \n",
    "    return all_keys\n",
    "\n",
    "'''\n",
    "\n",
    "def get_only_true_data(X, key_options):\n",
    "    \n",
    "    training_keys_true = get_keys_for_fingerprinting_data(size=len(X), key_options=key_options)\n",
    "    X_train_true = [x for x in X]\n",
    "    y_train_true = get_fingerprints_for_ipds(len(X), sample_size=len(X[0]))#,training_keys=training_keys_true)\n",
    "    \n",
    "    \n",
    "    X_train = np.expand_dims(X_train_true, axis=1).reshape((-1,900,1))\n",
    "    y_train = np.expand_dims(y_train_true, axis=1).reshape((-1,900,1))\n",
    "    training_keys = training_keys_true#,#np.expand_dims(training_keys_true, axis=1)\n",
    "    \n",
    "    return X_train, y_train, training_keys\n",
    "def get_false_training(key_length, X):\n",
    "    false_value = 0\n",
    "    k = [1]\n",
    "    for i in range(key_length-1):\n",
    "        k.append(0)\n",
    "    k = np.array(k)\n",
    "    y_train_non =  [np.zeros(len(x)) for x in X]#np.zeros(len(x))\n",
    "    training_keys = [k for x in range(len(y_train_non))]\n",
    "    \n",
    "    return X, y_train_non, training_keys\n",
    "def get_false_true_training(X, key_options):\n",
    "    n_false_train = int(len(X) / 50)\n",
    "    n_true_train = len(X) - n_false_train\n",
    "    \n",
    "    training_keys_true = get_keys_for_fingerprinting_data(size=n_true_train, key_options=key_options)\n",
    "    X_train_true = [x for x in X[0:n_true_train]]\n",
    "    y_train_true = add_fignerprinting_to_ipds(n_true_train,sample_size=len(X[0]))#,training_keys=training_keys_true)\n",
    "    \n",
    "    X_train_false, y_train_false, training_keys_false = get_false_training(len(key_options[0]), X[-n_false_train:])\n",
    "    \n",
    "    print(len(X_train_false),len(X_train_true))\n",
    "    X_train, y_train, training_keys = [], [], []\n",
    "    f, t = 0, 0\n",
    "    for i in range(n_false_train + n_true_train):\n",
    "        rnd = random.randrange(0, 50)\n",
    "        if rnd == 1 and f < len(X_train_false) :\n",
    "            X_train.append(X_train_false[f])\n",
    "            y_train.append(y_train_false[f])\n",
    "            training_keys.append(training_keys_false[f])\n",
    "            f += 1\n",
    "        elif t < len(X_train_true):\n",
    "            X_train.append(X_train_true[t])\n",
    "            y_train.append(y_train_true[t])\n",
    "            training_keys.append(training_keys_true[t])\n",
    "            t += 1 \n",
    "    \n",
    "\n",
    "    \n",
    "    X_train = np.expand_dims(X_train, axis=1).reshape((-1, 900, 1))#np.array(X_train).reshape((-1, 900, 1))\n",
    "    y_train = np.expand_dims(y_train, axis=1).reshape((-1, 900, 1))\n",
    "    training_keys = np.array(training_keys)\n",
    "    \n",
    "    \n",
    "    return X_train, y_train, training_keys\n",
    "def save_model_weights(model, name):\n",
    "    model_json = model.to_json()\n",
    "    with open(path + str(name) + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(path + str(name) + \".h5\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/fatemeh/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(key_options[0])\\nselected_keys=get_keys_for_fingerprinting_data(10, key_options)\\nprint(selected_keys.shape)\\nselected_keys = np.expand_dims(selected_keys, axis=1).reshape((-1, len(key_options[0]), 1))\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_keys_for_fingerprinting_data(size, key_options):\n",
    "    selected_keys = np.array([key_options[0]])\n",
    "#     print(selected_keys,type(selected_keys))\n",
    "    for i in range(size - 1):\n",
    "        rnd = random.randrange(0, len(key_options))\n",
    "#         print([key_options[rnd]],type(key_options[0]))\n",
    "        selected_keys = np.concatenate((selected_keys, np.array([key_options[rnd]])))\n",
    "#         selected_keys.append(key_options[rnd])\n",
    "\n",
    "    return selected_keys\n",
    "'''\n",
    "print(key_options[0])\n",
    "selected_keys=get_keys_for_fingerprinting_data(10, key_options)\n",
    "print(selected_keys.shape)\n",
    "selected_keys = np.expand_dims(selected_keys, axis=1).reshape((-1, len(key_options[0]), 1))\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_encoder_dense_slice(sample_size, key_length, chunk):\n",
    "    p = 0\n",
    "    Input_ipd = Input(shape=(sample_size, 1), name='input1')  # this is needed just for the decoding\n",
    "    Input_key = Input(shape=(key_length,), name='input2')\n",
    "    fingerprint_mult = Input(shape=(chunk,), name='input3')\n",
    "    fingerprint_sub = Input(shape=(chunk,), name='input4')\n",
    "    network_noise = Input(shape=(sample_size,), name='input5')\n",
    "    \n",
    "    ipd = Flatten(name =\"ipd_flatten1\")(Input_ipd)\n",
    "    outputs = []\n",
    "    \n",
    "    quant = int(sample_size/chunk)\n",
    "    def slice(x):\n",
    "        return x[:, p * chunk:(1 + p) * chunk]\n",
    "    \n",
    "    key1 = Dense(64, name='key1')(Input_key)\n",
    "\n",
    "    sliced_ipd = Lambda(slice)(ipd)\n",
    "    x_fingerprint = sliced_ipd\n",
    "    for i in range(0, quant):\n",
    "        sliced_ipd = Lambda(slice)(ipd)\n",
    "        print(sliced_ipd, x_fingerprint)\n",
    "        ss = Concatenate(name = 'concat'+ str(p))([x_fingerprint, sliced_ipd]) \n",
    "        ipd1 = Dense(32, name = 'dense'+ str(p))(ss)\n",
    "        batch_2 = BatchNormalization(name = 'batch'+ str(p))(ipd1)\n",
    "        relu_2 = Activation('relu', name = 'act'+ str(p))(batch_2)\n",
    "        \n",
    "        ipds_key_merge = Concatenate(name = 'concat_key_'+ str(p))([relu_2, key1])\n",
    "        dense_enc1 = Dense(64, name = 'dense_enc1' + str(p))(ipds_key_merge)\n",
    "        batch_2 = BatchNormalization(name = 'batch2_'+ str(p))(dense_enc1)\n",
    "        relu_2 = Activation('relu', name = 'act2_'+ str(p))(batch_2)\n",
    "        dense_drop_enc1 = Dropout(0.3, name = 'dense_drop_enc1' + str(p))(relu_2)\n",
    "        \n",
    "        x_fingerprint_sig = Dense(chunk, name = 'fingerprint_sig' + str(p), activation = 'sigmoid')(dense_drop_enc1)\n",
    "        x_fingerprint_mult = Multiply(name = 'fingerprint_mult' + str(p))([x_fingerprint_sig, fingerprint_mult])\n",
    "        x_fingerprint = Add(name = 'ipd_delay' + str(p))([x_fingerprint_mult, fingerprint_sub])\n",
    "        outputs.append(x_fingerprint)\n",
    "        p += 1\n",
    "    x_fingerprint = Concatenate(name = 'fingerprint2')(outputs)\n",
    "    x_fingerprint_output = Reshape((sample_size, 1), name='fingerprint')(x_fingerprint)\n",
    "\n",
    "def get_encoder_slice(key_length, chunk, p):\n",
    "    Input_ipd = Input(shape=(chunk, 1), name='input1')  # this is needed just for the decoding\n",
    "    Input_prev_fing = Input(shape=(chunk, 1), name='input2')\n",
    "    Input_key = Input(shape=(key_length,), name='input3')\n",
    "    fingerprint_mult = Input(shape=(chunk,), name='input4')\n",
    "    fingerprint_sub = Input(shape=(chunk,), name='input5')\n",
    "    \n",
    "    ipd = Flatten(name =\"ipd_flatten\")(Input_ipd)\n",
    "    fing = Flatten(name =\"fing_flatten\")(Input_prev_fing)\n",
    "    \n",
    "    key1 = Dense(64, name='key1')(Input_key)\n",
    "    ss = Concatenate(name = 'concat'+ str(p))([fing, ipd]) \n",
    "    ipd1 = Dense(32, name = 'dense'+ str(p))(ss)\n",
    "    \n",
    "    batch_2 = BatchNormalization(name = 'batch'+ str(p))(ipd1)\n",
    "    relu_2 = Activation('relu', name = 'act'+ str(p))(batch_2)\n",
    "\n",
    "    ipds_key_merge = Concatenate(name = 'concat_key_'+ str(p))([relu_2, key1])\n",
    "    dense_enc1 = Dense(64, name = 'dense_enc1' + str(p))(ipds_key_merge)\n",
    "    batch_2 = BatchNormalization(name = 'batch2_'+ str(p))(dense_enc1)\n",
    "    relu_2 = Activation('relu', name = 'act2_'+ str(p))(batch_2)\n",
    "    dense_drop_enc1 = Dropout(0.3, name = 'dense_drop_enc1' + str(p))(relu_2)\n",
    "\n",
    "    x_fingerprint_sig = Dense(chunk, name = 'fingerprint_sig' + str(p), activation = 'sigmoid')(dense_drop_enc1)\n",
    "    x_fingerprint_mult = Multiply(name = 'fingerprint_mult' + str(p))([x_fingerprint_sig, fingerprint_mult])\n",
    "    x_fingerprint = Add(name = 'ipd_delay' + str(p))([x_fingerprint_mult, fingerprint_sub])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_encoder_decoder_conv_dense_slice(sample_size, key_length, chunk,reg):\n",
    "    p = 0\n",
    "    Input_ipd = Input(shape=(sample_size, 1), name='input1')  # this is needed just for the decoding\n",
    "    Input_key = Input(shape=(key_length,), name='input2')\n",
    "    fingerprint_mult = Input(shape=(chunk,), name='input3')\n",
    "    fingerprint_sub = Input(shape=(chunk,), name='input4')\n",
    "    network_noise = Input(shape=(sample_size,), name='input5')\n",
    "    \n",
    "    ipd = Flatten(name =\"ipd_flatten1\")(Input_ipd)\n",
    "    outputs = []\n",
    "    \n",
    "    quant = int(sample_size/chunk)\n",
    "    def slice(x):\n",
    "        return x[:, p * chunk:(1 + p) * chunk]\n",
    "    \n",
    "    key1 = Dense(64, name='key1')(Input_key)\n",
    "\n",
    "    sliced_ipd = Lambda(slice)(ipd)\n",
    "    x_fingerprint = sliced_ipd\n",
    "    for i in range(0, quant):\n",
    "        sliced_ipd = Lambda(slice)(ipd)\n",
    "        #print(sliced_ipd, x_fingerprint)\n",
    "        ss = Concatenate(name = 'concat'+ str(p))([x_fingerprint, sliced_ipd]) \n",
    "        ipd1 = Dense(32,kernel_regularizer=l2(reg), name = 'dense'+ str(p))(ss)\n",
    "        batch_2 = BatchNormalization(name = 'batch'+ str(p))(ipd1)\n",
    "        relu_2 = Activation('relu', name = 'act'+ str(p))(batch_2)\n",
    "        \n",
    "        ipds_key_merge = Concatenate(name = 'concat_key_'+ str(p))([relu_2, key1])\n",
    "        dense_enc1 = Dense(64, kernel_regularizer=l2(reg), name = 'dense_enc1' + str(p))(ipds_key_merge)\n",
    "        batch_2 = BatchNormalization(name = 'batch2_'+ str(p))(dense_enc1)\n",
    "        relu_2 = Activation('relu', name = 'act2_'+ str(p))(batch_2)\n",
    "        dense_drop_enc1 = Dropout(0.3, name = 'dense_drop_enc1' + str(p))(relu_2)\n",
    "        \n",
    "        x_fingerprint_sig = Dense(chunk,kernel_regularizer=l2(reg), name = 'fingerprint_sig' + str(p), activation = 'sigmoid')(dense_drop_enc1)\n",
    "        x_fingerprint_mult = Multiply(name = 'fingerprint_mult' + str(p))([x_fingerprint_sig, fingerprint_mult])\n",
    "        x_fingerprint = Add(name = 'ipd_delay' + str(p))([x_fingerprint_mult, fingerprint_sub])\n",
    "        outputs.append(x_fingerprint)\n",
    "        p += 1\n",
    "    x_fingerprint = Concatenate(name = 'fingerprint2')(outputs)\n",
    "    x_fingerprint_output = Reshape((sample_size, 1), name='fingerprint')(x_fingerprint)\n",
    "\n",
    "    x_ipd = Add(name = 'x_ipd')([x_fingerprint, ipd, network_noise])\n",
    "        \n",
    "    x_ipd_reshape = Reshape((sample_size, 1),name = 'reshape_dec')(x_ipd)\n",
    "    \n",
    "    conv_dec_2 = Conv1D(filters = 20, kernel_size=10,kernel_regularizer=l2(reg), padding='same', name='conv_dec_2')(x_ipd_reshape)\n",
    "    conv_batch_2 = BatchNormalization(name='conv_batch_2_dec')(conv_dec_2)\n",
    "    conv_relu_2 = Activation('relu', name='conv_relu_2_dec')(conv_batch_2)\n",
    "    conv_drop_2 = Dropout(0.3, name='conv_drop_2_dec')(conv_relu_2)\n",
    "    max_pool_dec_2 = MaxPooling1D(pool_size=1, name=\"max_pool_dec_2\")(conv_drop_2)\n",
    "    \n",
    "    conv_dec_3 = Conv1D(filters = 10, kernel_size=10,kernel_regularizer=l2(reg), padding='same', name='conv_dec_3')(max_pool_dec_2)\n",
    "    conv_batch_3 = BatchNormalization(name='conv_batch_3_dec')(conv_dec_3)\n",
    "    conv_relu_3 = Activation('relu', name='conv_relu_3_dec')(conv_batch_3)\n",
    "    conv_drop_2 = Dropout(0.3, name='conv_drop_3_dec')(conv_relu_3)\n",
    "    max_pool_dec_3 = MaxPooling1D(pool_size=1, name=\"max_pool_dec_3\")(conv_drop_2)\n",
    "    max_pool_dec_3_f = Flatten(name =\"flate_max3_dec\")(max_pool_dec_3)\n",
    "\n",
    "    dense_dec_1 = Dense(256, kernel_regularizer=l2(reg),name='dense_dec_1')(max_pool_dec_3_f)\n",
    "    \n",
    "    dense_batch_dec1 = BatchNormalization(name='dense_batch_dec1')(dense_dec_1)\n",
    "    dense_relu_dec1 = Activation('relu', name='dense_relu_dec1')(dense_batch_dec1)\n",
    "    dense_drop_dec1 = Dropout(0.3, name='dense_drop_dec1')(dense_relu_dec1)    \n",
    "    \n",
    "    dense_dec_2 = Dense(64,kernel_regularizer=l2(reg), name='dense_dec_2')(dense_drop_dec1)\n",
    "    dense_batch_dec2 = BatchNormalization(name='dense_batch_dec2')(dense_dec_2)\n",
    "    dense_relu_dec2 = Activation('relu', name='dense_relu_dec2')(dense_batch_dec2)\n",
    "    dense_drop_dec2 = Dropout(0.3, name='dense_drop_dec2')(dense_relu_dec2)\n",
    "    \n",
    "    key_hat = Dense(key_length, activation='softmax', name='key_hat')(dense_drop_dec2)\n",
    "\n",
    "    return Model(inputs=[Input_ipd, Input_key, fingerprint_mult, fingerprint_sub, network_noise], outputs=[x_fingerprint_output, key_hat])#, key_hat])\n",
    "\n",
    "\n",
    "def load_decoder(key_length, sample_size):\n",
    "    \n",
    "    Input_ipd = Input(shape=(sample_size, 1), name='reshape_dec') \n",
    "    #x_ipd_reshape = Reshape((sample_size, 1), name = 'reshape')(Input_ipd)\n",
    "    conv_dec_2 = Conv1D(filters = 20, kernel_size=10, padding='same', name='conv_dec_2')(Input_ipd)\n",
    "    conv_batch_2 = BatchNormalization(name='conv_batch_2_dec')(conv_dec_2)\n",
    "    conv_relu_2 = Activation('relu', name='conv_relu_2_dec')(conv_batch_2)\n",
    "    conv_drop_2 = Dropout(0.3, name='conv_drop_2_dec')(conv_relu_2)\n",
    "    max_pool_dec_2 = MaxPooling1D(pool_size=1, name=\"max_pool_dec_2\")(conv_drop_2)\n",
    "    \n",
    "    conv_dec_3 = Conv1D(filters = 10, kernel_size=10, padding='same', name='conv_dec_3')(max_pool_dec_2)\n",
    "    conv_batch_3 = BatchNormalization(name='conv_batch_3_dec')(conv_dec_3)\n",
    "    conv_relu_3 = Activation('relu', name='conv_relu_3_dec')(conv_batch_3)\n",
    "    conv_drop_2 = Dropout(0.3, name='conv_drop_3_dec')(conv_relu_3)\n",
    "    max_pool_dec_3 = MaxPooling1D(pool_size=1, name=\"max_pool_dec_3\")(conv_drop_2)\n",
    "    max_pool_dec_3_f = Flatten(name =\"flate_max3_dec\")(max_pool_dec_3)\n",
    "\n",
    "    dense_dec_1 = Dense(256, name='dense_dec_1')(max_pool_dec_3_f)\n",
    "    dense_batch_dec1 = BatchNormalization(name='dense_batch_dec1')(dense_dec_1)\n",
    "    dense_relu_dec1 = Activation('relu', name='dense_relu_dec1')(dense_batch_dec1)\n",
    "    dense_drop_dec1 = Dropout(0.3, name='dense_drop_dec1')(dense_relu_dec1)    \n",
    "    \n",
    "    dense_dec_2 = Dense(64, name='dense_dec_2')(dense_drop_dec1)\n",
    "    dense_batch_dec2 = BatchNormalization(name='dense_batch_dec2')(dense_dec_2)\n",
    "    dense_relu_dec2 = Activation('relu', name='dense_relu_dec2')(dense_batch_dec2)\n",
    "    dense_drop_dec2 = Dropout(0.3, name='dense_drop_dec2')(dense_relu_dec2)\n",
    "    \n",
    "    key_hat = Dense(key_length, activation='softmax', name='key_hat')(dense_drop_dec2)\n",
    "    \n",
    "    \n",
    "    model_decoder = Model(inputs=[Input_ipd], outputs=[key_hat])\n",
    "    #model_decoder.set_weights(model.get_weights())\n",
    "    #model_decoder.load_weights(filepath=path + model_name + \".h5\", by_name=True)\n",
    "\n",
    "    return model_decoder\n",
    "\n",
    "def compute_extract_rate(keys, true_keys):\n",
    "    correct = 0 \n",
    "    for i in range(len(keys)): \n",
    "        if np.argmax(keys[i]) == np.argmax(true_keys[i]):\n",
    "            correct +=1\n",
    "    return correct/float(len(keys))\n",
    "\n",
    "\n",
    "def mean_pred_loss(y_true, y_pred):\n",
    "    #when the coeficent is smaller, performance is better. When increasing, noise improves\n",
    "    sum_abs = K.abs(y_pred)\n",
    "    tmp =  K.mean(sum_abs) - 0.3 * K.mean(y_pred)# + K.epsilon()\n",
    "    return 100 * (K.abs(K.mean(y_pred)))# 1/tmp  keras.losses.mean_absolute_error(y_true, y_pred) + \n",
    "\n",
    "def get_mult_sub_for_fingerprinting(n_data, max_delay, chunk, sample_size):\n",
    "    array_mult, array_sub = [], []\n",
    "    for x in range(0, n_data):\n",
    "        array_mult.append([max_delay] * chunk)\n",
    "        array_sub.append([-max_delay/2] * chunk)\n",
    "    array_mult = np.array(array_mult)\n",
    "    array_sub = np.array(array_sub)\n",
    "    return array_mult, array_sub\n",
    "\n",
    "def get_noise_simulation_array(n_data, std, sample_size):\n",
    "    noise = []\n",
    "    for x in range(0, n_data):\n",
    "        #noise.append(np.random.normal(0, std, sample_size))\n",
    "        #noise.append(np.random.uniform(0, std, sample_size))\n",
    "        noise.append(np.random.laplace(0, std, sample_size));\n",
    "    noise = np.array(noise)\n",
    "    return noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/home/fatemeh/MyProjects/Fingerprint/models/\"\n",
    "rate = '10'\n",
    "all_ipds_for_test = create_ipd_dataset(\n",
    "    address='/home/fatemeh/MyProjects/Fingerprint/Synthetic dataset/in/' + rate + '/test/')\n",
    "all_ipds_for_train = create_ipd_dataset(\n",
    "    address='/home/fatemeh/MyProjects/Fingerprint/Synthetic dataset/in/' + rate + '/train/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 5000 Numbre of training and testing data\n"
     ]
    }
   ],
   "source": [
    "n_true_train, n_test = 200000, 5000\n",
    "sample_size, key_length = 300, 1024# * 32\n",
    "alpha = 25\n",
    "\n",
    "X_train_all = create_sample_size_dataset(all_ipds_for_train, sample_size = sample_size,n_sample=n_true_train)\n",
    "X_test_all = create_sample_size_dataset(all_ipds_for_test, sample_size = sample_size,n_sample=n_test)\n",
    "print(len(X_train_all), len(X_test_all), \"Numbre of training and testing data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-33cba868fde7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mx_slice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train_all\u001b[0m\u001b[1;31m#[i*x_slice_size:(i+1)*x_slice_size]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey_options\u001b[0m\u001b[1;31m#[i*key_s:(i+1)*key_s]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_false_true_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_slice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#get_only_true_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-6faa0da653d8>\u001b[0m in \u001b[0;36mget_false_true_data\u001b[1;34m(X, key_options, alpha)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_false_true_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m     \u001b[0mtraining_keys_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_keys_for_fingerprinting_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m     \u001b[1;31m#X_train_true =X# [x for x in X]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[0my_train_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_fingerprints_for_ipds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#,training_keys=training_keys_true)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-52f60e87b857>\u001b[0m in \u001b[0;36mget_keys_for_fingerprinting_data\u001b[1;34m(size, key_options)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mrnd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#         print([key_options[rnd]],type(key_options[0]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mselected_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselected_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey_options\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrnd\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m#         selected_keys.append(key_options[rnd])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "beg = time.time()\n",
    "key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "x_slice_size= 10000#len(X_train_all)//64\n",
    "key_s = 500#key_length//64\n",
    "i = 0\n",
    "x_slice = X_train_all#[i*x_slice_size:(i+1)*x_slice_size]\n",
    "keys = key_options#[i*key_s:(i+1)*key_s]\n",
    "X_train, y_train, train_keys = get_false_true_data(x_slice, keys, alpha=alpha)#get_only_true_data\n",
    "print(time.time() - beg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std, max_fing_delay = 1, alpha\n",
    "chunk = 10\n",
    "array_mult_test, array_sub_test = get_mult_sub_for_fingerprinting(n_test, max_delay=max_fing_delay, chunk=chunk,\n",
    "                                                                  sample_size=sample_size)\n",
    "noise_for_test = get_noise_simulation_array(n_test, std=std, sample_size=sample_size)\n",
    "array_mult_train, array_sub_train = get_mult_sub_for_fingerprinting(n_true_train, max_delay=max_fing_delay, chunk=chunk,\n",
    "                                                                    sample_size=sample_size)\n",
    "noise_for_train = get_noise_simulation_array(n_true_train, std=std, sample_size=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date :  2019-10-31 14:56:26.216021\n",
      "Tensor(\"lambda_64/strided_slice:0\", shape=(?, ?), dtype=float32) Tensor(\"lambda_63/strided_slice:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"lambda_65/strided_slice:0\", shape=(?, ?), dtype=float32) Tensor(\"ipd_delay0_2/add:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"lambda_66/strided_slice:0\", shape=(?, ?), dtype=float32) Tensor(\"ipd_delay1_2/add:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"lambda_67/strided_slice:0\", shape=(?, ?), dtype=float32) Tensor(\"ipd_delay2_2/add:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"lambda_68/strided_slice:0\", shape=(?, ?), dtype=float32) Tensor(\"ipd_delay3_2/add:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"lambda_69/strided_slice:0\", shape=(?, ?), dtype=float32) Tensor(\"ipd_delay4_2/add:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"lambda_70/strided_slice:0\", shape=(?, ?), dtype=float32) Tensor(\"ipd_delay5_2/add:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"lambda_71/strided_slice:0\", shape=(?, ?), dtype=float32) Tensor(\"ipd_delay6_2/add:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"lambda_72/strided_slice:0\", shape=(?, ?), dtype=float32) Tensor(\"ipd_delay7_2/add:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"lambda_73/strided_slice:0\", shape=(?, ?), dtype=float32) Tensor(\"ipd_delay8_2/add:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"lambda_74/strided_slice:0\", shape=(?, ?), dtype=float32) Tensor(\"ipd_delay9_2/add:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"lambda_75/strided_slice:0\", shape=(?, ?), dtype=float32) Tensor(\"ipd_delay10_2/add:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"lambda_76/strided_slice:0\", shape=(?, ?), dtype=float32) Tensor(\"ipd_delay11_2/add:0\", shape=(?, 10), dtype=float32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-a96d58d133b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Date : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mmodel_10\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_encoder_decoder_conv_dense_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;31m# losses.mean_squared_error# 0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0madam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.999\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-33b8f989d71d>\u001b[0m in \u001b[0;36mget_encoder_decoder_conv_dense_slice\u001b[1;34m(sample_size, key_length, chunk, reg)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mipds_key_merge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'concat_key_'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrelu_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mdense_enc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ml2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'dense_enc1'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mipds_key_merge\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mbatch_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'batch2_'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdense_enc1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mrelu_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'act2_'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mdense_drop_enc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'dense_drop_enc1'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelu_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[1;31m# Actually call the layer,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[1;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/keras/layers/normalization.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m    195\u001b[0m         self.add_update([K.moving_average_update(self.moving_mean,\n\u001b[0;32m    196\u001b[0m                                                  \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m                                                  self.momentum),\n\u001b[0m\u001b[0;32m    198\u001b[0m                          K.moving_average_update(self.moving_variance,\n\u001b[0;32m    199\u001b[0m                                                  \u001b[0mvariance\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mmoving_average_update\u001b[1;34m(x, value, momentum)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \"\"\"\n\u001b[0;32m   1013\u001b[0m     return moving_averages.assign_moving_average(\n\u001b[1;32m-> 1014\u001b[1;33m         x, value, momentum, zero_debias=True)\n\u001b[0m\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/moving_averages.py\u001b[0m in \u001b[0;36massign_moving_average\u001b[1;34m(variable, value, decay, zero_debias, name)\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mdecay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mzero_debias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mupdate_delta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_zero_debias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mupdate_delta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvariable\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdecay\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/moving_averages.py\u001b[0m in \u001b[0;36m_zero_debias\u001b[1;34m(unbiased_var, value, decay)\u001b[0m\n\u001b[0;32m    210\u001b[0m       biased_var = variable_scope.get_variable(\n\u001b[0;32m    211\u001b[0m           \u001b[0m_maybe_get_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"biased\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbiased_initializer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m           trainable=False)\n\u001b[0m\u001b[0;32m    213\u001b[0m       local_step = variable_scope.get_variable(\n\u001b[0;32m    214\u001b[0m           \u001b[0m_maybe_get_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local_step\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1485\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1486\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1487\u001b[1;33m       aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1235\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1236\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1237\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1239\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    538\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 540\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    490\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    920\u001b[0m         \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 922\u001b[1;33m         aggregation=aggregation)\n\u001b[0m\u001b[0;32m    923\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_store_eager_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[1;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         aggregation=aggregation)\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m   def _variable_v2_call(cls,\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m                         aggregation=VariableAggregation.NONE):\n\u001b[0;32m    124\u001b[0m     \u001b[1;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     \u001b[0mprevious_getter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mgetter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[1;34m(next_creator, **kwargs)\u001b[0m\n\u001b[0;32m   2442\u001b[0m         \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2443\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariable_def\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvariable_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2444\u001b[1;33m         expected_shape=expected_shape, import_scope=import_scope)\n\u001b[0m\u001b[0;32m   2445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1328\u001b[0m           \u001b[0mexpected_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpected_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[1;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape, constraint)\u001b[0m\n\u001b[0;32m   1479\u001b[0m             self._try_guard_against_uninitialized_dependencies(\n\u001b[0;32m   1480\u001b[0m                 self._initial_value),\n\u001b[1;32m-> 1481\u001b[1;33m             validate_shape=validate_shape).op\n\u001b[0m\u001b[0;32m   1482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1483\u001b[0m         \u001b[1;31m# TODO(vrv): Change this class to not take caching_device, but\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[0;32m    219\u001b[0m     return gen_state_ops.assign(\n\u001b[0;32m    220\u001b[0m         \u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m         validate_shape=validate_shape)\n\u001b[0m\u001b[0;32m    222\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_state_ops.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m     60\u001b[0m         \u001b[1;34m\"Assign\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         use_locking=use_locking, name=name)\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    392\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m     \u001b[0minput_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m       \u001b[1;31m# Perform input type inference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   6026\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6027\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6028\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6029\u001b[0m       \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6030\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mname_scope\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   3998\u001b[0m         \u001b[1;31m# that are illegal as the initial character of an op name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3999\u001b[0m         \u001b[1;31m# (viz. '-', '\\', '/', and '_').\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4000\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_VALID_SCOPE_NAME_REGEX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4001\u001b[0m           \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'%s' is not a valid scope name\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4002\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "from keras import optimizers\n",
    "import datetime\n",
    "import time\n",
    "n_trains = [200000]#200\n",
    "key_len = [1024 * 32]# result for 1024 = 0.98\n",
    "regs = [1e-6]#0.01, 0.5, 1e-3, 1e-4, 1e-5, 1e-6 too. Looks like 1e-6 is the best (maybe 1e-7, 1e-8 too)\n",
    "models = []\n",
    "#key_length = 1024\n",
    "\n",
    "for r in regs:\n",
    "\n",
    "    n_true_train, n_test = 2000, 5000\n",
    "    alpha = 25\n",
    "    beg_time = time.time()\n",
    "    x_fing_w, key_hat_w, epoch, batch = 1, 100, 75, 64\n",
    "    std, max_fing_delay = 1, alpha\n",
    "    chunk = 10\n",
    "\n",
    "    \n",
    "    t = n_true_train\n",
    "\n",
    "    print(\"Date : \", datetime.datetime.now())\n",
    "\n",
    "    model_10 = get_encoder_decoder_conv_dense_slice(sample_size=sample_size, key_length=key_length, chunk=chunk, reg = r)\n",
    "    # losses.mean_squared_error# 0.001\n",
    "    adam = optimizers.Adam(lr = 1e-3, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0, amsgrad=False)\n",
    "\n",
    "    model_10.compile(optimizer=adam, loss={'fingerprint': mean_pred_loss, 'key_hat': losses.categorical_crossentropy},\n",
    "              loss_weights={'fingerprint': x_fing_w, 'key_hat': key_hat_w})\n",
    "\n",
    "    # model.summary()\n",
    "    print(\"Model %s is Built and Compiled in %f\" % (t, time.time() - beg_time))\n",
    "    beg_time = time.time()\n",
    "\n",
    "    model_10.fit([X_train[0:t], train_keys[0:t], array_mult_train[0:t], array_sub_train[0:t], noise_for_train[0:t]],\n",
    "          [y_train[0:t], train_keys[0:t]], epochs=epoch, validation_split=0.1,\n",
    "          batch_size=batch)  # callbacks=callbacks_list, verbose=0)\n",
    "    models.append(model_10)\n",
    "\n",
    "    print(\"Time to Fit the Model\", time.time() - beg_time)\n",
    "\n",
    "    #### This is when we test encoder and decoder together using the same model: model_encoder_decoder\n",
    "    x_test = np.array(X_test_all[0:n_test]).reshape((-1, sample_size, 1))\n",
    "    key_options = selecting_valid_fingerprints(key_length=key_length)  # we use 100 keys.\n",
    "    test_keys = np.array(get_keys_for_fingerprinting_data(size=n_test, key_options=key_options))\n",
    "    noise_for_test = np.squeeze(noise_for_test)\n",
    "    pred = model_10.predict([x_test, test_keys, array_mult_test, array_sub_test, noise_for_test])\n",
    "\n",
    "    fingerprint_x22, keys_true = pred[0], pred[1]\n",
    "    ext_rate = compute_extract_rate(keys_true, true_keys=test_keys)\n",
    "\n",
    "    print(\"Ext Rate:  \", ext_rate)\n",
    "# avg_d, max_d = compute_delay_on_each_packet(fingerprint_x2)\n",
    "\n",
    "#fingerprint_x2 = adjust_fingerprint_delays(fingerprint_x2)\n",
    "#avg_d, max_d = compute_delay_on_packets(fingerprint_x2)\n",
    "#learning_rates = [1e-3]# e-3 reached 0.21 when epochs = 50 [1e-2, e-3, e-5,e-4,1e-5, 5e-3, 2e-3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_keys = np.array(train_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Input_data = Input(shape=(60000, 1), name='input1')  # this is needed just for the decoding\n",
    "h1 = Dense(70)(Input_data)\n",
    "relu_1 = Activation('relu')(h1)\n",
    "h2 = Dense(50)(relu_1)\n",
    "relu_2 = Activation('relu')(h2)\n",
    "h3 = Dense(50)(relu_2)\n",
    "softmax_1 = Activation('softmax')(h3)\n",
    "adam = optimizers.Adam(lr = 1e-3, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-8, decay = 0.0, amsgrad=False)\n",
    "model=Model(inputs=[Input_data], outputs=[softmax_1])\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit([X_train], [y_train], epochs=150, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=1000, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "adam = optimizers.Adam(lr = 1e-3, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-8, decay = 0.0, amsgrad=False)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise = np.array(noise_for_test[0:n_test]).reshape((-1, sample_size, 1))\n",
    "fing_ipds = fingerprint_x22 + x_test\n",
    "non_fing_ipds = noise + x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.array(X_test_all[0:n_test]).reshape((-1, sample_size, 1))\n",
    "key_options = selecting_valid_fingerprints(key_length=key_length)  # we use 100 keys.\n",
    "test_keys = np.array(get_keys_for_fingerprinting_data(size=n_test, key_options=key_options))\n",
    "noise_for_test = np.squeeze(noise_for_test)\n",
    "pred = model_10.predict([x_test, test_keys, array_mult_test, array_sub_test, noise_for_test])\n",
    "\n",
    "fingerprint_x22, keys_true = pred[0], pred[1]\n",
    "ext_rate = compute_extract_rate(keys_true, true_keys=test_keys)\n",
    "\n",
    "print(\"Ext Rate:  \", ext_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "We have 0.947 ext_rate when we have 300 pkt, 200, 000 training data and 250 epochs (loss comes down to 0.18), and\n",
    "key_size is 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Time to Fit the Model 22027.91614151001, is ~6 hours.\n",
    "Ext Rate:   1.0 when 1000000 traiing wiht key_length = 500\n",
    "\n",
    "Same thing with key_length = 1000 is : It stoped before finishing, but it would be 1 since the loss reduced after 16\n",
    " epochs to 0.1 in epoch 30.    \n",
    "#### try the sample_size = 600....\n",
    ".9838 with 200, 000 training data and key_length = 1000\n",
    "%88 with 200000 training data and sample size = 300, key Length =1000, epoch = 50\n",
    "for the sample size 200 to work, we need 200 epochs. since it is improving 0.01 in each epoch after we reach epoch\n",
    "= 75. Or maybe we needed more training data. (we were using 200K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#alpha =25\n",
    "#pkt = 10,  ext_rate = 0.701, key_length = 20, epoch = 100, n_train = 5000\n",
    "#usiing uniform (0,alpha) for fingerprint same parameters as above line: ext_rate: 0.54\n",
    "\n",
    "Time to Fit the Model 2291.7441816329956 = 2291/3600 = 36 minutes?\n",
    "Ext Rate:   0.978 with 10000 training and 20 keys, with 20,000 it goes to ext_rate = 1\n",
    "    10000 training and 200 key  ext_rate =0.019\n",
    "    with 20000 and 200 key ext_rate =0.53\n",
    "    and with 40, 000 it goes to ext_rate = 1\n",
    ".\n",
    "\n",
    "#n_train = 24000 with key_length = 200, epoch = 50, pkt  =100 , ext_rate = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fingerprints_for_ipds(n_train, sample_size, alpha):\n",
    "    \n",
    "    #Previous one was all + and the largest value was 50.\n",
    "   \n",
    "    fingerprint_output = []\n",
    "    thrshold = 100\n",
    "    while len(fingerprint_output) < n_train:\n",
    "        finger = [0]#[random.uniform(0, 250)]\n",
    "        neg_numbers = 0\n",
    "        delay = 0\n",
    "        for i in range(sample_size - 1):\n",
    "            #if random.randrange(0, 3) == 0:\n",
    "            rnd = random.randrange(0, 2)\n",
    "            if rnd == 1 and delay < thrshold:\n",
    "                finger.append(alpha)\n",
    "            elif rnd ==1 and delay>= thrshold:\n",
    "                finger.append(-alpha)\n",
    "                \n",
    "            elif rnd == 0 and delay <= 0 and  delay >-thrshold: # random.randrange(0, 2) == 0:# and np.abs(delay) <50:\n",
    "                finger.append(-1 * alpha)#\n",
    "            elif rnd ==0 and delay <=0 and delay <=-thrshold:\n",
    "                finger.append(alpha)\n",
    "            elif rnd == 0 and delay > 0:\n",
    "                finger.append(-1 * alpha)\n",
    "            delay += finger[-1]\n",
    "            #print(delay)\n",
    "            if sum(finger) < 0:\n",
    "                neg_numbers += 1\n",
    "#             else:\n",
    "#                 finger.append(0) \n",
    "        #if neg_numbers < 50:  ## this can be a hyperparameter\n",
    "        fingerprint_output.append(finger)\n",
    "    return fingerprint_output\n",
    "\n",
    "\n",
    "\n",
    "y = get_fingerprints_for_ipds(1, 1500, alpha=25) \n",
    "y = np.expand_dims(y, axis=1).reshape((-1, 1500, 1))\n",
    "y = adjust_fingerprint_delays(y)\n",
    "avg_d, max_d = compute_delay_on_packets(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjust_fingerprint_delays(fingerprint_x2):\n",
    "    number_of_train, sample_size = len(fingerprint_x2), len(fingerprint_x2[0])\n",
    "    for f in range(0, len(fingerprint_x2)):\n",
    "            delay, min_delay = 0, 0\n",
    "            for p in range(0, len(fingerprint_x2[f])):\n",
    "                    delay += fingerprint_x2[f][p][0]\n",
    "                    #print(delay)\n",
    "                    if delay < min_delay:\n",
    "                        min_delay = delay \n",
    "            fingerprint_x2[f][0][0] -= 1.001 * min_delay\n",
    "           # print(\"min\",min_delay)\n",
    "           # break\n",
    "    return fingerprint_x2\n",
    "\n",
    "def compute_delay_on_packets(fingerprint_x2):\n",
    "#######Compute the average delay on each packet.        \n",
    "    average_delay = []\n",
    "    max_d, pos = 0, 0\n",
    "    for fing in fingerprint_x2:\n",
    "        delay, neg = 0, 0\n",
    "        delays = []\n",
    "        for n in fing:\n",
    "            delay += n[0]\n",
    "            if delay > max_d:\n",
    "                max_d = delay\n",
    "            delays.append(delay)\n",
    "\n",
    "            if delay < 0:\n",
    "                neg += 1\n",
    "        if neg == 0:\n",
    "            pos += 1\n",
    "            average_delay.append(sum(delays)/sample_size)\n",
    "   # print(sum(delays)/sample_size,\" Average Delay\")\n",
    "   # print(sample_size, number_of_train)\n",
    "    print(\"Negative delay: \",pos, \"Average delay for them:\",sum(average_delay)/pos, \"Max Delay: \", max_d)\n",
    "    return sum(average_delay)/pos, max_d\n",
    "fingerprint_x2 = adjust_fingerprint_delays(fingerprint_x2)\n",
    "avg_d, max_d = compute_delay_on_packets(fingerprint_x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(x_test),sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise_for_test = np.array(noise_for_test[0:n_test]).reshape((-1, sample_size, 1))\n",
    "fingerprinted_ipds =  x_test + noise_for_test+ fingerprint_x22 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    target = open(\"/home/fatemeh/MyProjects/Fingerprint/KS/fing/lap_\" + str(i) + \".txt\", 'w')\n",
    "    array = fingerprinted_ipds[i]#x_test[i] + fingerprint_x22[i]\n",
    "    array = np.squeeze(array)\n",
    "    write_array_to_file(array, target, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: I want to check to see if I 0 the negative fingerprints, I can have the same extraction rate.\n",
    "model_decoder = load_decoder(key_length, sample_size)\n",
    "#model_decoder.set_weights(model.get_weights())\n",
    "beg_time = time.time()\n",
    "i = 0\n",
    "for layer in model_10.layers:\n",
    "  #  w_target=layer.get_weights()\n",
    "    for dec_layer in model_decoder.layers:\n",
    "        if layer.name ==dec_layer.name:\n",
    "            print(layer.name, i)\n",
    "            dec_layer.set_weights(layer.get_weights())\n",
    "    i += 1\n",
    "print(\"Time it takes to load the decoder: \", time.time() - beg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_fingerprinted_ipds():\n",
    "    path = '/home/fatemeh/MyProjects/Fingerprint/encoder/ext_ipds/'\n",
    "    all_ipds = []\n",
    "    for i in range(11):\n",
    "        string_ipds = read_from_file(path + str(i) + \".txt\").split(\" \")\n",
    "        ipds = convert_stringArrays_to_floatArray(string_ipds)\n",
    "        all_ipds.append(ipds)\n",
    "    return all_ipds\n",
    "all_ipds = read_fingerprinted_ipds()\n",
    "fingerprinted_ipds=np.array(all_ipds).reshape((-1, sample_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "noise_for_test = np.squeeze(noise_for_test)\n",
    "noise_for_test = get_noise_simulation_array(n_test, std=1, sample_size=sample_size)\n",
    "noise_for_test=np.array(noise_for_test[0:n_test]).reshape((-1, sample_size, 1))\n",
    "fingerprinted_ipds = fingerprint_x2 + x_test + noise_for_test\n",
    "'''\n",
    "\n",
    "\n",
    "key_hat = model_decoder.predict(fingerprinted_ipds)\n",
    "ext_rate = compute_extract_rate(key_hat, true_keys=test_keys)\n",
    "print(ext_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "# avg_d, max_d = compute_delay_on_each_packet(fingerprint_x2)\n",
    "rate=100\n",
    "with open('sep_results.csv', mode='a') as csv_file:\n",
    "    fieldnames = ['sample_size', 'key_length', \"number_training\", 'ext_rate', 'average_delay', 'max_delay',\n",
    "                  'packet_rate', 'std', 'max_train_noise','date']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "    #writer.writeheader()\n",
    "    writer.writerow(\n",
    "        {'sample_size': sample_size, 'key_length': key_length, \"number_training\": n_true_train, 'ext_rate': ext_rate,\n",
    "         'average_delay': avg_d, 'max_delay': max_d, 'packet_rate': rate, 'std': std, 'max_train_noise': max_fing_delay,'date': datetime.datetime.now()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(X_train[0][0:100])\n",
    "# print(fingerprint_x2[0][0:100])\n",
    "array_mult_test2, array_sub_test2, noise_for_test2 = get_arrays_mult_noise_sub(10,max_delay=5,chunk=10,std=5,sample_size=sample_size)\n",
    "\n",
    "d = 0\n",
    "import numpy as np\n",
    "for f in noise_for_test2:\n",
    "    dd = []\n",
    "    for p in f:\n",
    "        dd.append(p)\n",
    "    std = np.std(dd, axis=0)\n",
    "    print(std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "    Ext Rate:   0.9986 for key_length = 20\n",
    "    5000 52.243792017179196 1500 Max Delay:  194.5090960264206\n",
    "\n",
    "    Ext Rate:   0.9594 for key_length = 100\n",
    "    5000 115.39134879171382 1500 Max Delay:  560.9818127155304\n",
    "\n",
    "    Ext Rate:   0.9344\n",
    "    5000 240.32056540118285 1500 Max Delay:  1266.3458748834673\n",
    "\n",
    "    Ext Rate:   0.709,    key_len = 500\n",
    "    92.47834091258049\n",
    "    1500 5000\n",
    "    5000 84.65399134224361 1500 Max Delay:  459.50667464733124\n",
    "\n",
    "    Ext Rate: 0.5602, key_len = 1000\n",
    "    28.44295782939593\n",
    "    1500 5000\n",
    "    5000 53.95885500007159 1500 Max Delay:  244.45183670520782\n",
    "'''\n",
    "\n",
    "'''\n",
    "Noise is uniform in these experiments:\n",
    "maxDelay = 5, rate =100\n",
    "    std = 1\n",
    "        Ext Rate:   1.0\n",
    "        34.646455238024394\n",
    "        1500 4000\n",
    "        4000 45.05813633905002 1500 Max Delay:  209.10854732990265\n",
    "    std = 5\n",
    "        Ext Rate:   1.0\n",
    "        22.97654592792193\n",
    "        1500 4000\n",
    "        4000 36.03301963561987 1500 Max Delay:  146.38326346874237\n",
    "    std = 100\n",
    "        Ext Rate:   0.0485\n",
    "        29.65366893227895\n",
    "        1500 4000\n",
    "        4000 28.555992046196685 1500 Max Delay:  157.52952599525452\n",
    "maxDelay =100, rate =100\n",
    "    std = 100\n",
    "        Ext Rate:   0.9985\n",
    "        422.7813040936788\n",
    "        1500 4000\n",
    "        4000 557.8424517351626 1500 Max Delay:  2539.529760360718   \n",
    "      \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ext_rate = compute_extract_rate(keys_true, true_keys = test_keys)\n",
    "\n",
    "print(\"Ext Rate:  \", ext_rate)\n",
    "compute_delay_on_each_packet(fingerprint_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras import optimizers\n",
    "n_false_train = 0\n",
    "x_fing_w, key_hat_w, epoch, batch = 1, 200, 50, 64\n",
    "\n",
    "beg_time = time.time()\n",
    "key_length = 100\n",
    "key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "sample_sizes = [1500, 1800]#, 600, 1200]\n",
    "#models = []\n",
    "trains = [60000, 10000]\n",
    "for sam in sample_sizes:\n",
    "    sample_size = sam\n",
    "    X_train_all = create_sample_size_dataset(all_ipds_for_train, sample_size = sample_size)\n",
    "    X_test_all = create_sample_size_dataset(all_ipds_for_test, sample_size = sample_size)\n",
    "    print(len(X_train_all),len(X_test_all), \"Numbre of training and testing data\")\n",
    "    model = get_encoder_decoder_conv_dense_slice(sample_size=sample_size, key_length=key_length, chunk=10)\n",
    "    ad = optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0, amsgrad=False)\n",
    "\n",
    "    model.compile(optimizer=ad, loss={'fingerprint':mean_pred_loss, 'key_hat': losses.categorical_crossentropy},\n",
    "                                    loss_weights={'fingerprint': x_fing_w, 'key_hat': key_hat_w})\n",
    "    X_train, y_train, train_keys = get_false_true_data(X_train_all, key_options)#get_only_true_data\n",
    "    train_keys = np.array(train_keys)\n",
    "    n_test = 4000\n",
    "    for t in trains:\n",
    "        # model.summary()\n",
    "        beg_time = time.time()\n",
    "        array_mult_train, array_sub_train, noise_for_train = get_arrays_mult_noise_sub(t,max_delay=5,chunk=10,std=1,sample_size=sample_size)\n",
    "        model.fit([X_train[0:t], train_keys[0:t], array_mult_train[0:t], array_sub_train[0:t], \n",
    "                   noise_for_train[0:t]], [y_train[0:t], train_keys[0:t]], epochs=epoch,\n",
    "                  validation_split=0.1, batch_size=batch,verbose =1)#, validation_split=0.1,callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "        print(\"Time to Fit the Model\", time.time() - beg_time)\n",
    "        models.append(model) \n",
    "        array_mult_test, array_sub_test, noise_for_test = get_arrays_mult_noise_sub(n_test,max_delay=5,chunk=10,std=1,sample_size=sample_size)\n",
    "\n",
    "        #### This is when we test encoder and decoder together using the same model: model_encoder_decoder\n",
    "        x_test = np.array(X_test_all[0:n_test]).reshape((-1, sample_size, 1))\n",
    "        key_options = selecting_valid_fingerprints(key_length = key_length)# we use 100 keys.\n",
    "        test_keys = np.array(get_keys_for_fingerprinting_data(size=n_test, key_options=key_options))\n",
    "        noise_for_test = np.squeeze(noise_for_test[0:n_test])\n",
    "        pred = model.predict([x_test, test_keys, array_mult_test, array_sub_test, noise_for_test])\n",
    "\n",
    "        fingerprint_x2, keys_true = pred[0],  pred[1]\n",
    "        ext_rate = compute_extract_rate(keys_true, true_keys = test_keys)\n",
    "\n",
    "        print(\"Ext Rate:  \", ext_rate)\n",
    "        compute_delay_on_each_packet(fingerprint_x2)\n",
    "'''\n",
    "\n",
    "    size = 1800\n",
    "    Ext Rate:   0.992\n",
    "    4000 95.84566262555747 1800 Max Delay:  415.03377401828766\n",
    "    Ext Rate:   0.9765\n",
    "    4000 93.27346397158448 1800 Max Delay:  380.78301668167114\n",
    "    \n",
    "    \n",
    "    size 1200\n",
    "    \n",
    "    Ext Rate:   0.948\n",
    "    4000 72.26921833757295 1200 Max Delay:  357.55537247657776\n",
    "\n",
    "    Ext Rate:   0.943\n",
    "    4000 72.2022527100891 1200 Max Delay:  356.89986884593964\n",
    "    \n",
    "    size = 600\n",
    "    Ext Rate: 0.788\n",
    "    4000 38.018    max delay: 235.928\n",
    "    \n",
    "    size 600, and 10000 training:\n",
    "    Ext Rate:   0.731\n",
    "    4000 37.918288900979874 600 Max Delay:  233.056\n",
    "    \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_test = 5000\n",
    "print(sample_size)\n",
    "array_mult_test, array_sub_test, noise_for_test = get_arrays_mult_noise_sub(n_test,max_delay=5,chunk=10,std=1,sample_size=sample_size)\n",
    "\n",
    "x_test = np.array(X_test_all[0:n_test]).reshape((-1, sample_size, 1))\n",
    "key_options = selecting_valid_fingerprints(key_length = 200)# we use 100 keys.\n",
    "test_keys = np.array(get_keys_for_fingerprinting_data(size=n_test, key_options=key_options))\n",
    "noise_for_test = np.squeeze(noise_for_test[0:n_test])\n",
    "pred = model.predict([x_test, test_keys, array_mult_test, array_sub_test, noise_for_test])\n",
    "\n",
    "fingerprint_x2, keys_true = pred[0],  pred[1]\n",
    "ext_rate = compute_extract_rate(keys_true, true_keys = test_keys)\n",
    "\n",
    "print(\"Ext Rate:  \", ext_rate)\n",
    "compute_delay_on_each_packet(fingerprint_x2)\n",
    "'''\n",
    "sample size = 1800, n_train = 10000, and key = 100:\n",
    "1800\n",
    "Ext Rate:   0.0842\n",
    "52.90532826509741\n",
    "1800 4444\n",
    "4444 47.894631279740935 1800 Max Delay:  182.74153697490692\n",
    "when we only change n_train = 60000:\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def decide_if_fingerprinted(keys, threshold):\n",
    "    fing = 0\n",
    "    for key in keys:\n",
    "        index = np.argmax(key)\n",
    "        if key[index] > threshold and index > 0:\n",
    "            fing += 1\n",
    "    return fing / float(len(keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Loading encoder takes too much time (hours), so we just use the model_encoder_decoder for encoding.\n",
    "model_decoder = load_decoder(key_length, sample_size)\n",
    "decoder_weights = []\n",
    "j = 0\n",
    "for i in range(0, 24):\n",
    "    if 'dec' in model.layers[-(24 - i)].name or 'key_hat' in model.layers[-(24 - i)].name:\n",
    "        model_decoder.layers[j].set_weights(model.layers[-(24 - i)].get_weights())\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise_for_test = noise_for_test.reshape((-1, sample_size, 1))\n",
    "\n",
    "output_fin = noise_for_test[0:n_test] + x_test\n",
    "keys_true_fp = model_decoder.predict([output_fin])\n",
    "\n",
    "###### True positve:\n",
    "output_fin = noise_for_test[0:n_test] + x_test + fingerprint_x2\n",
    "\n",
    "keys_true_tp = model_decoder.predict([output_fin])\n",
    "ext_rate = compute_extract_rate(keys_true_tp, test_keys)\n",
    "thresholds = [0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]\n",
    "\n",
    "\n",
    "for t in thresholds:\n",
    "    fp = decide_if_fingerprinted(keys_true_fp, t)\n",
    "    tp = decide_if_fingerprinted(keys_true_tp, t)\n",
    "    \n",
    "    print(fp, tp)\n",
    "print(ext_rate, 'Extraction Rate')\n",
    "'''\n",
    "\n",
    "key = 100 training with 1/100 number of false data. sample size = 1800, number of training=20000\n",
    "0.2395 0.9845\n",
    "0.156 0.9695\n",
    "0.093 0.9535\n",
    "0.043 0.931\n",
    "0.014 0.8975\n",
    "0.0055 0.867\n",
    "0.0005 0.766\n",
    "0.9685 Extraction Rate\n",
    "Ext Rate:   0.969\n",
    "1800 2000\n",
    "2000 138.32507355565957 1800 Max Delay:  556.0820367336273\n",
    "##############################################################################################\n",
    "\n",
    "\n",
    "sample size 3300, training data = 48000, key = 1000\n",
    "\n",
    "0.293 0.9925\n",
    "0.1925 0.98\n",
    "0.115 0.9685\n",
    "0.0655 0.941\n",
    "0.022 0.905\n",
    "0.007 0.8525\n",
    "0.0 0.734\n",
    "0.9695 Extraction Rate\n",
    "2000 80.0513701567251 3300 Max Delay:  369.6583148241043\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_false_train = 0\n",
    "x_fing_w, key_hat_w, epoch, batch = 1, 200, 100, 64\n",
    "model_name = str(sample_size) + \"_\" + str(key_length) + \"_\" + str(\n",
    "    n_true_train) + \"_\" + str(n_false_train) + \"_\" + str(epoch) + \"_\" + str(x_fing_w) + \"_\" + str(key_hat_w)\n",
    "\n",
    "beg_time = time.time()\n",
    "#models_key_length = []\n",
    "keys = [100]\n",
    "n_true_train = 30000\n",
    "for k in keys:\n",
    "\n",
    "    key_options = selecting_valid_fingerprints(key_length = k)# we use 100 keys.\n",
    "    X_train, y_train, train_keys = get_false_true_training(X_train_all[0:n_true_train], key_options)\n",
    "    train_keys = np.array(train_keys)\n",
    "    print(\"Finished radinf\")\n",
    "\n",
    "    model= get_encoder_decoder_conv_dense_slice(sample_size=sample_size, key_length=k)\n",
    "    #losses.mean_squared_error\n",
    "    ad = optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0, amsgrad=False)\n",
    "\n",
    "    model.compile(optimizer=ad, loss={'fingerprint':mean_pred_loss, 'key_hat': losses.categorical_crossentropy},\n",
    "                                    loss_weights={'fingerprint': x_fing_w, 'key_hat': key_hat_w})\n",
    "\n",
    "\n",
    "    # model.summary()\n",
    "    print(\"Model %s is Built and Compiled in %f\" % (model_name ,time.time() - beg_time))\n",
    "    beg_time = time.time()\n",
    "\n",
    "    model.fit([X_train, train_keys, array_mult_train[0:n_true_train], array_sub_train[0:n_true_train], noise_for_train[0:n_true_train]], [y_train, train_keys], epochs=epoch, validation_split=0.1, batch_size=batch)#, validation_split=0.1,callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "    print(\"Time to Fit the Model\", time.time() - beg_time)\n",
    "\n",
    "    \n",
    "    models_key_length.append(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_for_results = '/home/fatemeh/Dropbox/Fingerprint/Results/'\n",
    "def compute_ROC_data(n_train):\n",
    "    target_name = open(path_for_results + str(n_train)+\"_\" + str(sample_size)+\"_\"+str(key_length) + '.txt', 'w')\n",
    "    sample_size = 900\n",
    "    key_length = 100\n",
    "    n_test = 5000\n",
    "    thresholds = [0.6, 0.7, 0.8, 0.9]\n",
    "    X = create_sample_size_dataset(all_ipds, sample_size = sample_size)\n",
    "    key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    model_decoder, model_encoder = load_model_for_testing(key_length, sample_size, n_train)\n",
    "    X_test = np.expand_dims(X[n_train:n_train + n_test], axis=1)\n",
    "    test_keys = np.expand_dims(get_fingerprint_for_data(size = n_test, key_options = key_options), axis=1)\n",
    "    fingerprint_x = model_encoder.predict([test_keys])\n",
    "    false_poses, true_poses = [], []\n",
    "    \n",
    "     ########## True positve:\n",
    "    output_fin = add_gussian_noise_to_ipds_fingerprinted(fingerprint = fingerprint_x, x_test = X_test, std =10)\n",
    "    keys_true = model_decoder.predict([output_fin])\n",
    "\n",
    "    ########## False positve: \n",
    "    output_non = add_gussian_noise_to_ipds_non_fingerprinted(X_test, std =10)\n",
    "    keys_false = model_decoder.predict([output_non])\n",
    "    for t in thresholds:\n",
    "        true_pos = decide_if_fingerprinted(keys_true, threshold = t)\n",
    "        false_pos = decide_if_fingerprinted(keys_false, threshold = t)\n",
    "        false_poses.append(false_pos)\n",
    "        true_poses.append(true_pos)\n",
    "        \n",
    "    write_array_to_file(array = false_poses, target =target_name, delimiter =' ')\n",
    "    write_array_to_file(array = true_poses, target =target_name, delimiter =' ')\n",
    "    target_name.close()\n",
    "    \n",
    "compute_ROC_data(n_train=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_impact_of_jitter():\n",
    "    sample_size = 600\n",
    "    key_length = 10\n",
    "    n_train = 50000\n",
    "    n_test = 5000\n",
    "    X = create_sample_size_dataset(all_ipds, sample_size = sample_size)\n",
    "    key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "\n",
    "    target_name = open(path_for_results + str(n_train)+\"_\" + str(sample_size)+\"_\"+str(key_length) + '.txt', 'w')\n",
    "\n",
    "    jitters = [1, 10, 50, 100]\n",
    "   \n",
    "    model_decoder, model_encoder = load_model_for_testing(key_length, sample_size, n_train)\n",
    "    X_test = np.expand_dims(X[n_train:n_train + n_test], axis=1)\n",
    "    test_keys = np.expand_dims(get_fingerprint_for_data(size = n_test, key_options = key_options), axis=1)\n",
    "    fingerprint_x = model_encoder.predict([test_keys])\n",
    "    false_poses, true_poses, ext_rates = [], [], []\n",
    "\n",
    "    for std in jitters:      \n",
    "        ########## True positve:\n",
    "        output_fin = add_gussian_noise_to_ipds_fingerprinted(fingerprint = fingerprint_x, x_test = X_test, std =std)\n",
    "        keys_true = model_decoder.predict([output_fin])\n",
    "        true_pos = decide_if_fingerprinted(keys_true, threshold=4)\n",
    "        key_pred = extract_keys_from_key_hat(keys_true)\n",
    "        error_rate = compute_error_rate_flowwise(predict_key = key_pred, true_key = test_keys)\n",
    "\n",
    "        ########## False positve: \n",
    "        output_non = add_gussian_noise_to_ipds_non_fingerprinted(X_test, std =std)\n",
    "        keys_false = model_decoder.predict([output_non])\n",
    "        false_pos = decide_if_fingerprinted(keys_false, threshold=4)\n",
    "        false_poses.append(false_pos)\n",
    "        true_poses.append(true_pos)\n",
    "        ext_rates.append(1 - error_rate)\n",
    "    write_array_to_file(array = ext_rates, target =target_name, delimiter =' ')\n",
    "    write_array_to_file(array = false_poses, target =target_name, delimiter =' ')\n",
    "    write_array_to_file(array = true_poses, target =target_name, delimiter =' ')\n",
    "    target_name.close()\n",
    "# compute_impact_of_jitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_fing_w, key_hat_w, epoch = 1, 50, 100\n",
    "def call_fit_load_eval_Main():\n",
    "    n_all_true_trains =[5000]# [5000, 10000, 20000, 50000]#5000,, \n",
    "    sample_sizes = [600]#[400, 200, 600]\n",
    "    key_lengths = [10]#, 15, 20]\n",
    "\n",
    "    for sample_size in sample_sizes:\n",
    "        X = create_sample_size_dataset(all_ipds, sample_size = sample_size)\n",
    "        for key_length in key_lengths:\n",
    "            key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "            false_poses, true_poses, ext_rates = [], [], []\n",
    "            target_name = open(path_for_results + str(sample_size)+\"_\"+str(key_length) + '.txt', 'w')\n",
    "\n",
    "            for train_number in n_all_true_trains:\n",
    "                false_pos, true_pos, ext_rate = fit_model_load_evaulte(n_true_train =train_number, key_length=key_length, sample_size=sample_size,X=X,key_options=key_options)\n",
    "                false_poses.append(false_pos)\n",
    "                true_poses.append(true_pos)\n",
    "                ext_rates.append(ext_rate)\n",
    "                print(false_pos, true_pos, ext_rate)\n",
    "            write_array_to_file(array = ext_rates, target =target_name, delimiter =' ')\n",
    "            write_array_to_file(array = false_poses, target =target_name, delimiter =' ')\n",
    "            write_array_to_file(array = true_poses, target =target_name, delimiter =' ')\n",
    "            target_name.close()\n",
    "call_fit_load_eval_Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reload_model_for_more_epochs():\n",
    "    sample_size, key_length, n_true_train, epoch = 600, 10, 50000, 250\n",
    "    n_false_train = int(n_true_train/10)\n",
    "    x_fing_w, key_hat_w = 1, 50\n",
    "    model_name = \"march_10\" + str(sample_size) + \"_\" + str(key_length) + \"_\" + str(\n",
    "    n_true_train) + \"_\" + str(n_false_train) + \"_\" + str(epoch) + \"_\" + str(x_fing_w) + \"_\" + str(key_hat_w)\n",
    "\n",
    "    model = load_NN_model(path + model_name)\n",
    "    model_encoder_decoder.compile(optimizer='adam', \n",
    "                              loss=losses.mean_absolute_error,\n",
    "                                  loss_weights={'fingerprint':x_fing_w, 'key_hat':key_hat_w})\n",
    "\n",
    "    model_encoder_decoder.fit([X_train, training_keys], [y_train, training_keys],\n",
    "                        batch_size = 64, epochs = epoch + 250, verbose = 0)\n",
    "    #save_model_weights(model_encoder_decoder, name=model_name)\n",
    "# reload_model_for_more_epochs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_test_all_Main():\n",
    "    n_all_true_trains = [10000]# 5000, 10000, 20000, 50000]\n",
    "    sample_sizes = [600]\n",
    "    key_lengths = [10]\n",
    "    x_fing_w, key_hat_w, epoch = 1, 50, 100\n",
    "    n_test = 1000\n",
    "    for sample_size in sample_sizes:\n",
    "            \n",
    "            X = create_sample_size_dataset(all_ipds_for_test, sample_size = sample_size)\n",
    "            for key_length in key_lengths:\n",
    "                key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "                false_poses, true_poses, ext_rates = [], [], []\n",
    "                #target_name = open('/home/fatemeh/Dropbox/Fingerprint/Results/500_' + str(sample_size)+\"_\"+str(key_length) + '.txt', 'w')\n",
    "\n",
    "                for train_number in n_all_true_trains:\n",
    "                    model_decoder, model_encoder = load_model_for_testing(key_length, sample_size, train_number)\n",
    "                    false_pos, true_pos, ext_rate = evalute_encoder_decoder(model_decoder,model_encoder, X, sample_size, key_length,\n",
    "                                                                            key_options, train_number, int(train_number/10), n_test=n_test)\n",
    "                    false_poses.append(false_pos)\n",
    "                    true_poses.append(true_pos)\n",
    "                    ext_rates.append(ext_rate)\n",
    "                    print(sample_size, key_length, train_number, \"Result: \", false_pos, true_pos, ext_rate)\n",
    "#                 write_array_to_file(array = ext_rates, target =target_name, delimiter =' ')\n",
    "#                 write_array_to_file(array = false_poses, target =target_name, delimiter =' ')\n",
    "#                 write_array_to_file(array = true_poses, target =target_name, delimiter =' ')\n",
    "#                 target_name.close() \n",
    "load_test_all_Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_true_trains = [5000, 10000, 20000, 50000, 100000]\n",
    "sample_size, key_length = 600, 10\n",
    "epoch = 250\n",
    "for n_true_train in n_true_trains:\n",
    "    n_false_train = int(n_true_train/10)\n",
    "    key_hat_w, x_hat_w = 50, 1\n",
    "    model_name = \"march10_\" + str(sample_size) + \"_\" + str(key_length) + \"_\" + str(\n",
    "        n_true_train) + \"_\" + str(n_false_train) + \"_\" + str(epoch) + \"_\" + str(x_hat_w) + \"_\" + str(key_hat_w)\n",
    "\n",
    "    model = load_NN_model(path + model_name)\n",
    "\n",
    "    X_train, y_train, training_keys = get_false_true_training(n_true_train, n_false_train, key_length, X, key_options)\n",
    "    print(\"Finished reading dataset\")\n",
    "\n",
    "    model.compile(optimizer='adam', \n",
    "                                  loss=losses.mean_absolute_error,\n",
    "                                      loss_weights={'fingerprint':1, 'key_hat':50})\n",
    "    model.fit([X_train, training_keys], [y_train, training_keys],\n",
    "                            batch_size = 64, epochs = epoch, verbose = 0)\n",
    "    \n",
    "    model_name = \"march10_\" + str(sample_size) + \"_\" + str(key_length) + \"_\" + str(\n",
    "        n_true_train) + \"_\" + str(n_false_train) + \"_\" + str(500) + \"_\" + str(x_hat_w) + \"_\" + str(key_hat_w)\n",
    "\n",
    "    save_model_weights(model, name= model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_encoder(key_length, sample_size):\n",
    "    chunk, p = 10, 0\n",
    "    Input_ipd = Input(shape=(sample_size, 1), name='input1')  # this is needed just for the decoding\n",
    "    Input_key = Input(shape=(key_length,), name='input2')\n",
    "    fingerprint_mult = Input(shape=(chunk,), name='input3')\n",
    "    fingerprint_sub = Input(shape=(chunk,), name='input4')\n",
    "    \n",
    "    ipd = Flatten(name =\"ipd_flatten1\")(Input_ipd)\n",
    "    outputs = []\n",
    "    \n",
    "    quant = int(sample_size/chunk)\n",
    "    def slice(x):\n",
    "        return x[:, p * chunk:(1 + p) * chunk]\n",
    "    \n",
    "    key1 = Dense(32, name='key1')(Input_key)\n",
    "\n",
    "    sliced_ipd = Lambda(slice)(ipd)\n",
    "    x_fingerprint = sliced_ipd\n",
    "    for i in range(0, quant):\n",
    "        sliced_ipd = Lambda(slice)(ipd)\n",
    "        ss = Concatenate(name = 'concat'+ str(p))([x_fingerprint, sliced_ipd]) \n",
    "        ipd1 = Dense(32, name = 'dense'+ str(p))(ss)\n",
    "        batch_2 = BatchNormalization(name = 'batch'+ str(p))(ipd1)\n",
    "        relu_2 = Activation('relu', name = 'act'+ str(p))(batch_2)\n",
    "        \n",
    "        ipds_merged_all = Concatenate(name = 'concat_key_'+ str(p))([relu_2, key1])\n",
    "        dense_enc1 = Dense(64, name = 'dense_enc1' + str(p))(ipds_merged_all)\n",
    "        batch_2 = BatchNormalization(name = 'batch2_'+ str(p))(dense_enc1)\n",
    "        relu_2 = Activation('relu', name = 'act2_'+ str(p))(batch_2)\n",
    "        dense_drop_enc1 = Dropout(0.3, name = 'dense_drop_enc1' + str(p))(relu_2)\n",
    "        \n",
    "        x_fingerprint_sig = Dense(chunk, name = 'fingerprint_sig' + str(p), activation = 'sigmoid')(dense_drop_enc1)\n",
    "        x_fingerprint_mult = Multiply(name = 'fingerprint_mult' + str(p))([x_fingerprint_sig, fingerprint_mult])\n",
    "        x_fingerprint = Add(name = 'ipd_delay' + str(p))([x_fingerprint_mult, fingerprint_sub])\n",
    "        outputs.append(x_fingerprint)\n",
    "        p += 1\n",
    "    x_fingerprint = Concatenate(name = 'fingerprint2')(outputs)\n",
    "    x_fingerprint_output = Reshape((sample_size,1), name='fingerprint')(x_fingerprint)\n",
    "    model_encoder = Model(inputs=[Input_key,Input_ipd,  fingerprint_mult, fingerprint_sub], outputs=[x_fingerprint_output])\n",
    "    #model_encoder.load_weights(filepath=path + model_name + \".h5\", by_name=True)\n",
    "    return model_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
