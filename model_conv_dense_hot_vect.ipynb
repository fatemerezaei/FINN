{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Activation, Dropout, Dense, Input, Add, Multiply, Concatenate,Lambda\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D,  Flatten, Dot,Reshape\n",
    "from keras.models import Model\n",
    "import random, time, os\n",
    "import numpy as np\n",
    "from keras import losses\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.python import keras\n",
    "\n",
    "def create_sample_size_dataset(all_ipds, sample_size):\n",
    "    number_of_samples = int(len(all_ipds) / sample_size)\n",
    "    all_samples = []\n",
    "    for p in range(number_of_samples):\n",
    "        all_samples.append(all_ipds[p * sample_size:(p + 1) * sample_size])\n",
    "    return all_samples\n",
    "\n",
    "def write_array_to_file(array, target, delimiter):\n",
    "    for k in range(0, len(array)):\n",
    "        target.write(str(array[k]) + delimiter)\n",
    "    target.write(\"\\n\")\n",
    "\n",
    "\n",
    "def create_ipd_dataset(address):\n",
    "    files = os.listdir(address)\n",
    "    all_ipds = []\n",
    "    for f in files:\n",
    "            ipd = read_from_file(address + f).split(' ')\n",
    "            all_ipds.extend(convert_stringArrays_to_floatArray(ipd))\n",
    "    return all_ipds\n",
    "\n",
    "def read_from_file(path):\n",
    "    with open(path, 'r') as content_file:\n",
    "        content = content_file.read()\n",
    "        return content\n",
    "\n",
    "\n",
    "def isfloat(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def convert_stringArrays_to_floatArray(array):\n",
    "    intArray = []\n",
    "\n",
    "    for k in array:\n",
    "        if isfloat(k):\n",
    "            intArray.append(float(k))\n",
    "    return intArray\n",
    "\n",
    "\n",
    "def convert_stringArrays_to_intArray(array):\n",
    "    intArray = []\n",
    "\n",
    "    for k in array:\n",
    "        if isfloat(k):\n",
    "            intArray.append(int(k))\n",
    "    return intArray\n",
    "\n",
    "def create_fing_pattern(sample_size, keys):\n",
    "    all_patterns = {}\n",
    "    for i in range(len(keys)):\n",
    "        sample_pattern = []\n",
    "        k = \"\".join(str(x) for x in keys[i])\n",
    "        for j in range(0, sample_size):\n",
    "            rnd = random.randrange(0, 2)\n",
    "            sample_pattern.append(rnd)\n",
    "        all_patterns[k] = sample_pattern\n",
    "    return all_patterns\n",
    "\n",
    "def add_fignerprinting_to_ipds_sec(n_train, sample_size, training_keys):\n",
    "    '''\n",
    "    Previous one was all + and the largest value was 50.\n",
    "    '''\n",
    "    key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "    patterns = create_fing_pattern(sample_size, key_options)\n",
    "    fingerprint_output = []\n",
    "    j = 0\n",
    "    while len(fingerprint_output) < n_train:\n",
    "        finger = [random.uniform(0, 250)]\n",
    "        neg_numbers = 0\n",
    "        k = \"\".join(str(x) for x in training_keys[j])\n",
    "        pat = patterns[k]\n",
    "        for i in range(sample_size - 1):\n",
    "            if pat [i] == 1:\n",
    "                finger.append(10)#random.uniform(0, 25))#random.uniform(0, 25))\n",
    "            else:\n",
    "                finger.append(-10)# * random.uniform(0, 25))# random.uniform(0, 25))\n",
    "        fingerprint_output.append(finger)\n",
    "        j += 1\n",
    "      \n",
    "    return fingerprint_output\n",
    "def add_fignerprinting_to_ipds(n_train, sample_size):\n",
    "    '''\n",
    "    Previous one was all + and the largest value was 50.\n",
    "    '''\n",
    "    \n",
    "    fingerprint_output = []\n",
    "    while len(fingerprint_output) < n_train:\n",
    "        finger = [random.uniform(0, 250)]\n",
    "        neg_numbers = 0\n",
    "        for i in range(sample_size - 1):\n",
    "            #if random.randrange(0, 3) == 0:\n",
    "            if random.randrange(0, 2) == 1:\n",
    "                finger.append(10)#random.uniform(0, 10)\n",
    "            else:\n",
    "                finger.append(-1 * 10)#\n",
    "            if sum(finger) < 0:\n",
    "                neg_numbers += 1\n",
    "#             else:\n",
    "#                 finger.append(0) \n",
    "        #if neg_numbers < 50:  ## this can be a hyperparameter\n",
    "        fingerprint_output.append(finger)\n",
    "    return fingerprint_output\n",
    "\n",
    "def get_keys_for_fingerprinting_data(size, key_options):\n",
    "    selected_keys = []\n",
    "    for i in range(size):\n",
    "        rnd = random.randrange(0, len(key_options))\n",
    "        selected_keys.append(key_options[rnd])\n",
    "\n",
    "    return selected_keys\n",
    "\n",
    "\n",
    "def get_only_true_data(X, key_options):\n",
    "    \n",
    "    training_keys_true = get_keys_for_fingerprinting_data(size=len(X), key_options=key_options)\n",
    "    X_train_true = [x for x in X]\n",
    "    y_train_true = add_fignerprinting_to_ipds(len(X), sample_size=len(X[0]))#,training_keys=training_keys_true)\n",
    "    \n",
    "    \n",
    "    X_train = np.expand_dims(X_train_true, axis=1).reshape((-1,900,1))\n",
    "    y_train = np.expand_dims(y_train_true, axis=1).reshape((-1,900,1))\n",
    "    training_keys = training_keys_true#,#np.expand_dims(training_keys_true, axis=1)\n",
    "    \n",
    "    return X_train, y_train, training_keys\n",
    "\n",
    "def get_false_true_data(X, key_options):\n",
    "    \n",
    "    training_keys_true = get_keys_for_fingerprinting_data(size=len(X), key_options=key_options)\n",
    "    X_train_true = [x for x in X]\n",
    "    y_train_true = add_fignerprinting_to_ipds(len(X), sample_size=len(X[0]))#,training_keys=training_keys_true)\n",
    "    \n",
    "    ####### changing true trianing to false\n",
    "    false_key = np.zeros(len(key_options[0]))\n",
    "    false_key[0] = 1\n",
    "    ratio = 100\n",
    "    key_length = len(key_options[0])\n",
    "    num_false_train = int(len(X)/key_length )\n",
    "    print(\"finished getting true data\")\n",
    "    for i in range(num_false_train):\n",
    "        y_train_true[i * ratio] = np.zeros(len(X[0]))\n",
    "        training_keys_true[i * ratio] = false_key\n",
    "        \n",
    "        \n",
    "    X_train = np.expand_dims(X_train_true, axis=1).reshape((-1, len(X[0]), 1))\n",
    "    y_train = np.expand_dims(y_train_true, axis=1).reshape((-1, len(X[0]), 1))\n",
    "    training_keys = training_keys_true#,#np.expand_dims(training_keys_true, axis=1)\n",
    "    \n",
    "    return X_train, y_train, training_keys\n",
    "\n",
    "def selecting_valid_fingerprints(key_length):\n",
    "    all_keys = []\n",
    "    address = '/home/fatemeh/MyProjects/Fingerprint/Synthetic dataset/keys/' + str(key_length) + \"/\"\n",
    "    keys = os.listdir(address)\n",
    "    for k in keys:\n",
    "        key_i = convert_stringArrays_to_intArray(read_from_file(address + k).split(\" \"))\n",
    "        if key_i [0]==1:\n",
    "            continue\n",
    "        all_keys.append(key_i)\n",
    "            \n",
    "    return all_keys\n",
    "\n",
    "def get_false_training(key_length, X):\n",
    "    false_value = 0\n",
    "    k = [1]\n",
    "    for i in range(key_length-1):\n",
    "        k.append(0)\n",
    "    k = np.array(k)\n",
    "    y_train_non =  [np.zeros(len(x)) for x in X]#np.zeros(len(x))\n",
    "    training_keys = [k for x in range(len(y_train_non))]\n",
    "    \n",
    "    return X, y_train_non, training_keys\n",
    "\n",
    "def get_false_true_training(X, key_options):\n",
    "    n_false_train = int(len(X) / 50)\n",
    "    n_true_train = len(X) - n_false_train\n",
    "    \n",
    "    training_keys_true = get_keys_for_fingerprinting_data(size=n_true_train, key_options=key_options)\n",
    "    X_train_true = [x for x in X[0:n_true_train]]\n",
    "    y_train_true = add_fignerprinting_to_ipds(n_true_train,sample_size=len(X[0]))#,training_keys=training_keys_true)\n",
    "    \n",
    "    X_train_false, y_train_false, training_keys_false = get_false_training(len(key_options[0]), X[-n_false_train:])\n",
    "    \n",
    "    print(len(X_train_false),len(X_train_true))\n",
    "    X_train, y_train, training_keys = [], [], []\n",
    "    f, t = 0, 0\n",
    "    for i in range(n_false_train + n_true_train):\n",
    "        rnd = random.randrange(0, 50)\n",
    "        if rnd == 1 and f < len(X_train_false) :\n",
    "            X_train.append(X_train_false[f])\n",
    "            y_train.append(y_train_false[f])\n",
    "            training_keys.append(training_keys_false[f])\n",
    "            f += 1\n",
    "        elif t < len(X_train_true):\n",
    "            X_train.append(X_train_true[t])\n",
    "            y_train.append(y_train_true[t])\n",
    "            training_keys.append(training_keys_true[t])\n",
    "            t += 1 \n",
    "    \n",
    "\n",
    "    \n",
    "    X_train = np.expand_dims(X_train, axis=1).reshape((-1, 900, 1))#np.array(X_train).reshape((-1, 900, 1))\n",
    "    y_train = np.expand_dims(y_train, axis=1).reshape((-1, 900, 1))\n",
    "    training_keys = np.array(training_keys)\n",
    "    \n",
    "    \n",
    "    return X_train, y_train, training_keys\n",
    "\n",
    "\n",
    "def save_model_weights(model, name):\n",
    "    model_json = model.to_json()\n",
    "    with open(path + str(name) + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(path + str(name) + \".h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_decoder_conv_dense_slice(sample_size, key_length, chunk):\n",
    "    p = 0\n",
    "    Input_ipd = Input(shape=(sample_size, 1), name='input1')  # this is needed just for the decoding\n",
    "    Input_key = Input(shape=(key_length,), name='input2')\n",
    "    fingerprint_mult = Input(shape=(chunk,), name='input3')\n",
    "    fingerprint_sub = Input(shape=(chunk,), name='input4')\n",
    "    network_noise = Input(shape=(sample_size,), name='input5')\n",
    "    \n",
    "    ipd = Flatten(name =\"ipd_flatten1\")(Input_ipd)\n",
    "    outputs = []\n",
    "    \n",
    "    quant = int(sample_size/chunk)\n",
    "    def slice(x):\n",
    "        return x[:, p * chunk:(1 + p) * chunk]\n",
    "    \n",
    "    key1 = Dense(64, name='key1')(Input_key)\n",
    "\n",
    "    sliced_ipd = Lambda(slice)(ipd)\n",
    "    x_fingerprint = sliced_ipd\n",
    "    for i in range(0, quant):\n",
    "        sliced_ipd = Lambda(slice)(ipd)\n",
    "        ss = Concatenate(name = 'concat'+ str(p))([x_fingerprint, sliced_ipd]) \n",
    "        ipd1 = Dense(32, name = 'dense'+ str(p))(ss)\n",
    "        batch_2 = BatchNormalization(name = 'batch'+ str(p))(ipd1)\n",
    "        relu_2 = Activation('relu', name = 'act'+ str(p))(batch_2)\n",
    "        \n",
    "        ipds_merged_all = Concatenate(name = 'concat_key_'+ str(p))([relu_2, key1])\n",
    "        dense_enc1 = Dense(64, name = 'dense_enc1' + str(p))(ipds_merged_all)\n",
    "        batch_2 = BatchNormalization(name = 'batch2_'+ str(p))(dense_enc1)\n",
    "        relu_2 = Activation('relu', name = 'act2_'+ str(p))(batch_2)\n",
    "        dense_drop_enc1 = Dropout(0.3, name = 'dense_drop_enc1' + str(p))(relu_2)\n",
    "        \n",
    "        x_fingerprint_sig = Dense(chunk, name = 'fingerprint_sig' + str(p), activation = 'sigmoid')(dense_drop_enc1)\n",
    "        x_fingerprint_mult = Multiply(name = 'fingerprint_mult' + str(p))([x_fingerprint_sig, fingerprint_mult])\n",
    "        x_fingerprint = Add(name = 'ipd_delay' + str(p))([x_fingerprint_mult, fingerprint_sub])\n",
    "        outputs.append(x_fingerprint)\n",
    "        p += 1\n",
    "    x_fingerprint = Concatenate(name = 'fingerprint2')(outputs)\n",
    "    x_fingerprint_output = Reshape((sample_size,1), name='fingerprint')(x_fingerprint)\n",
    "\n",
    "    x_ipd = Add(name = 'x_ipd')([x_fingerprint, ipd, network_noise])\n",
    "        \n",
    "    x_ipd_reshape = Reshape((sample_size, 1),name = 'reshape_dec')(x_ipd)\n",
    "    \n",
    "    conv_dec_2 = Conv1D(filters = 20, kernel_size=10, padding='same', name='conv_dec_2')(x_ipd_reshape)\n",
    "    conv_batch_2 = BatchNormalization(name='conv_batch_2_dec')(conv_dec_2)\n",
    "    conv_relu_2 = Activation('relu', name='conv_relu_2_dec')(conv_batch_2)\n",
    "    conv_drop_2 = Dropout(0.3, name='conv_drop_2_dec')(conv_relu_2)\n",
    "    max_pool_dec_2 = MaxPooling1D(pool_size=1, name=\"max_pool_dec_2\")(conv_drop_2)\n",
    "    \n",
    "    conv_dec_3 = Conv1D(filters = 10, kernel_size=10, padding='same', name='conv_dec_3')(max_pool_dec_2)\n",
    "    conv_batch_3 = BatchNormalization(name='conv_batch_3_dec')(conv_dec_3)\n",
    "    conv_relu_3 = Activation('relu', name='conv_relu_3_dec')(conv_batch_3)\n",
    "    conv_drop_2 = Dropout(0.3, name='conv_drop_3_dec')(conv_relu_3)\n",
    "    max_pool_dec_3 = MaxPooling1D(pool_size=1, name=\"max_pool_dec_3\")(conv_drop_2)\n",
    "    max_pool_dec_3_f = Flatten(name =\"flate_max3_dec\")(max_pool_dec_3)\n",
    "\n",
    "    dense_dec_1 = Dense(256, name='dense_dec_1')(max_pool_dec_3_f)\n",
    "    \n",
    "    dense_batch_dec1 = BatchNormalization(name='dense_batch_dec1')(dense_dec_1)\n",
    "    dense_relu_dec1 = Activation('relu', name='dense_relu_dec1')(dense_batch_dec1)\n",
    "    dense_drop_dec1 = Dropout(0.3, name='dense_drop_dec1')(dense_relu_dec1)    \n",
    "    \n",
    "    dense_dec_2 = Dense(64, name='dense_dec_2')(dense_drop_dec1)\n",
    "    dense_batch_dec2 = BatchNormalization(name='dense_batch_dec2')(dense_dec_2)\n",
    "    dense_relu_dec2 = Activation('relu', name='dense_relu_dec2')(dense_batch_dec2)\n",
    "    dense_drop_dec2 = Dropout(0.3, name='dense_drop_dec2')(dense_relu_dec2)\n",
    "    \n",
    "    key_hat = Dense(key_length, activation='softmax', name='key_hat')(dense_drop_dec2)\n",
    "\n",
    "    return Model(inputs=[Input_ipd, Input_key, fingerprint_mult, fingerprint_sub, network_noise], outputs=[x_fingerprint_output, key_hat])#, key_hat])\n",
    "\n",
    "\n",
    "def load_decoder(key_length, sample_size):\n",
    "    \n",
    "    Input_ipd = Input(shape=(sample_size, 1), name='reshape_dec') \n",
    "    #x_ipd_reshape = Reshape((sample_size, 1), name = 'reshape')(Input_ipd)\n",
    "    conv_dec_2 = Conv1D(filters = 20, kernel_size=10, padding='same', name='conv_dec_2')(Input_ipd)\n",
    "    conv_batch_2 = BatchNormalization(name='conv_batch_2_dec')(conv_dec_2)\n",
    "    conv_relu_2 = Activation('relu', name='conv_relu_2_dec')(conv_batch_2)\n",
    "    conv_drop_2 = Dropout(0.3, name='conv_drop_2_dec')(conv_relu_2)\n",
    "    max_pool_dec_2 = MaxPooling1D(pool_size=1, name=\"max_pool_dec_2\")(conv_drop_2)\n",
    "    \n",
    "    conv_dec_3 = Conv1D(filters = 10, kernel_size=10, padding='same', name='conv_dec_3')(max_pool_dec_2)\n",
    "    conv_batch_3 = BatchNormalization(name='conv_batch_3_dec')(conv_dec_3)\n",
    "    conv_relu_3 = Activation('relu', name='conv_relu_3_dec')(conv_batch_3)\n",
    "    conv_drop_2 = Dropout(0.3, name='conv_drop_3_dec')(conv_relu_3)\n",
    "    max_pool_dec_3 = MaxPooling1D(pool_size=1, name=\"max_pool_dec_3\")(conv_drop_2)\n",
    "    max_pool_dec_3_f = Flatten(name =\"flate_max3_dec\")(max_pool_dec_3)\n",
    "\n",
    "    dense_dec_1 = Dense(256, name='dense_dec_1')(max_pool_dec_3_f)\n",
    "    dense_batch_dec1 = BatchNormalization(name='dense_batch_dec1')(dense_dec_1)\n",
    "    dense_relu_dec1 = Activation('relu', name='dense_relu_dec1')(dense_batch_dec1)\n",
    "    dense_drop_dec1 = Dropout(0.3, name='dense_drop_dec1')(dense_relu_dec1)    \n",
    "    \n",
    "    dense_dec_2 = Dense(64, name='dense_dec_2')(dense_drop_dec1)\n",
    "    dense_batch_dec2 = BatchNormalization(name='dense_batch_dec2')(dense_dec_2)\n",
    "    dense_relu_dec2 = Activation('relu', name='dense_relu_dec2')(dense_batch_dec2)\n",
    "    dense_drop_dec2 = Dropout(0.3, name='dense_drop_dec2')(dense_relu_dec2)\n",
    "    \n",
    "    key_hat = Dense(key_length, activation='softmax', name='key_hat')(dense_drop_dec2)\n",
    "    \n",
    "    \n",
    "    model_decoder = Model(inputs=[Input_ipd], outputs=[key_hat])\n",
    "    #model_decoder.set_weights(model.get_weights())\n",
    "    #model_decoder.load_weights(filepath=path + model_name + \".h5\", by_name=True)\n",
    "\n",
    "    return model_decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87800 5333 Numbre of training and testing data\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/fatemeh/MyProjects/Fingerprint/models/\"\n",
    "all_ipds_for_test = create_ipd_dataset(address='/home/fatemeh/MyProjects/Fingerprint/Synthetic dataset/in/10/test/')\n",
    "all_ipds_for_train = create_ipd_dataset(address='/home/fatemeh/MyProjects/Fingerprint/Synthetic dataset/in/10/train/')\n",
    "\n",
    "sample_size, key_length = 1500, 100\n",
    "X_train_all = create_sample_size_dataset(all_ipds_for_train, sample_size = sample_size)\n",
    "X_test_all = create_sample_size_dataset(all_ipds_for_test, sample_size = sample_size)\n",
    "print(len(X_train_all),len(X_test_all), \"Numbre of training and testing data\")\n",
    "key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "all_ipds_for_train2 = create_ipd_dataset(address='/home/fatemeh/MyProjects/Fingerprint/Synthetic dataset/in/10/rest of train/2/')\n",
    "X_train_all2 = create_sample_size_dataset(all_ipds_for_train2, sample_size = sample_size)\n",
    "X_train_all.extend(X_train_all2)\n",
    "n_true_train = len(X_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49999 3300\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_all),sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished getting true data\n",
      "finished generating training data!...\n"
     ]
    }
   ],
   "source": [
    "n_true_train = 48000\n",
    "\n",
    "X_train, y_train, train_keys = get_false_true_data(X_train_all, key_options)#get_only_true_data\n",
    "train_keys = np.array(train_keys)\n",
    "print(\"finished generating training data!...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_delay = 5\n",
    "chunk = 10\n",
    "std = 1\n",
    "\n",
    "array_mult_train, array_sub_train, noise_for_train = [], [], []\n",
    "for x in range(0, n_true_train):\n",
    "    array_mult_train.append([max_delay]* chunk)\n",
    "    array_sub_train.append([-max_delay/2]* chunk)\n",
    "    noise_for_train.append(np.random.uniform(0,std,sample_size))\n",
    "array_mult_train = np.array(array_mult_train)\n",
    "array_sub_train = np.array(array_sub_train)\n",
    "noise_for_train = np.array(noise_for_train)\n",
    "\n",
    "n_test = 2000\n",
    "array_mult_test, noise_for_test, array_sub_test = [], [], []\n",
    "for x in range(0, n_test):\n",
    "    array_mult_test.append([max_delay]* chunk)\n",
    "    array_sub_test.append([-max_delay/2]* chunk)\n",
    "    noise_for_test.append(np.random.uniform(0,std,sample_size))\n",
    "\n",
    "    \n",
    "noise_for_test = np.array(noise_for_test)\n",
    "array_sub_test = np.array(array_sub_test)\n",
    "array_mult_test = np.array(array_mult_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 48000 is Built and Compiled in 506.014981\n",
      "Train on 43200 samples, validate on 4800 samples\n",
      "Epoch 1/50\n",
      "43200/43200 [==============================] - 683s 16ms/step - loss: 943.8164 - fingerprint_loss: 0.5330 - key_hat_loss: 4.7164 - val_loss: 923.4001 - val_fingerprint_loss: 0.2161 - val_key_hat_loss: 4.6159\n",
      "Epoch 2/50\n",
      "43200/43200 [==============================] - 227s 5ms/step - loss: 904.7950 - fingerprint_loss: 0.5477 - key_hat_loss: 4.5212 - val_loss: 912.3777 - val_fingerprint_loss: 0.4341 - val_key_hat_loss: 4.5597\n",
      "Epoch 3/50\n",
      "43200/43200 [==============================] - 232s 5ms/step - loss: 865.1898 - fingerprint_loss: 0.5286 - key_hat_loss: 4.3233 - val_loss: 852.6389 - val_fingerprint_loss: 0.4951 - val_key_hat_loss: 4.2607\n",
      "Epoch 4/50\n",
      "43200/43200 [==============================] - 229s 5ms/step - loss: 663.0800 - fingerprint_loss: 0.4352 - key_hat_loss: 3.3132 - val_loss: 866.1774 - val_fingerprint_loss: 0.6454 - val_key_hat_loss: 4.3277\n",
      "Epoch 5/50\n",
      "43200/43200 [==============================] - 226s 5ms/step - loss: 282.3230 - fingerprint_loss: 0.3567 - key_hat_loss: 1.4098 - val_loss: 132.3699 - val_fingerprint_loss: 1.5352 - val_key_hat_loss: 0.6542\n",
      "Epoch 6/50\n",
      "43200/43200 [==============================] - 226s 5ms/step - loss: 177.4162 - fingerprint_loss: 0.3375 - key_hat_loss: 0.8854 - val_loss: 71.0924 - val_fingerprint_loss: 1.7476 - val_key_hat_loss: 0.3467\n",
      "Epoch 7/50\n",
      "43200/43200 [==============================] - 224s 5ms/step - loss: 132.6366 - fingerprint_loss: 0.3118 - key_hat_loss: 0.6616 - val_loss: 51.2522 - val_fingerprint_loss: 1.6499 - val_key_hat_loss: 0.2480\n",
      "Epoch 8/50\n",
      "43200/43200 [==============================] - 229s 5ms/step - loss: 106.9688 - fingerprint_loss: 0.3177 - key_hat_loss: 0.5333 - val_loss: 43.1587 - val_fingerprint_loss: 2.1387 - val_key_hat_loss: 0.2051\n",
      "Epoch 9/50\n",
      "43200/43200 [==============================] - 230s 5ms/step - loss: 91.4418 - fingerprint_loss: 0.3167 - key_hat_loss: 0.4556 - val_loss: 36.5587 - val_fingerprint_loss: 2.0287 - val_key_hat_loss: 0.1726\n",
      "Epoch 10/50\n",
      "43200/43200 [==============================] - 230s 5ms/step - loss: 79.7452 - fingerprint_loss: 0.3084 - key_hat_loss: 0.3972 - val_loss: 37.8149 - val_fingerprint_loss: 2.1036 - val_key_hat_loss: 0.1786\n",
      "Epoch 11/50\n",
      "43200/43200 [==============================] - 230s 5ms/step - loss: 70.9701 - fingerprint_loss: 0.3045 - key_hat_loss: 0.3533 - val_loss: 28.8263 - val_fingerprint_loss: 1.9680 - val_key_hat_loss: 0.1343\n",
      "Epoch 12/50\n",
      "43200/43200 [==============================] - 224s 5ms/step - loss: 65.0744 - fingerprint_loss: 0.3218 - key_hat_loss: 0.3238 - val_loss: 31.2357 - val_fingerprint_loss: 2.2711 - val_key_hat_loss: 0.1448\n",
      "Epoch 13/50\n",
      "43200/43200 [==============================] - 221s 5ms/step - loss: 58.2860 - fingerprint_loss: 0.3155 - key_hat_loss: 0.2899 - val_loss: 26.5534 - val_fingerprint_loss: 2.2221 - val_key_hat_loss: 0.1217\n",
      "Epoch 14/50\n",
      "43200/43200 [==============================] - 223s 5ms/step - loss: 55.2060 - fingerprint_loss: 0.2995 - key_hat_loss: 0.2745 - val_loss: 26.6261 - val_fingerprint_loss: 2.0684 - val_key_hat_loss: 0.1228\n",
      "Epoch 15/50\n",
      "43200/43200 [==============================] - 231s 5ms/step - loss: 51.6696 - fingerprint_loss: 0.3158 - key_hat_loss: 0.2568 - val_loss: 25.3985 - val_fingerprint_loss: 2.1579 - val_key_hat_loss: 0.1162\n",
      "Epoch 16/50\n",
      "43200/43200 [==============================] - 230s 5ms/step - loss: 49.4669 - fingerprint_loss: 0.3026 - key_hat_loss: 0.2458 - val_loss: 25.9475 - val_fingerprint_loss: 2.2163 - val_key_hat_loss: 0.1187\n",
      "Epoch 17/50\n",
      "43200/43200 [==============================] - 231s 5ms/step - loss: 46.0515 - fingerprint_loss: 0.2879 - key_hat_loss: 0.2288 - val_loss: 24.3772 - val_fingerprint_loss: 2.2272 - val_key_hat_loss: 0.1108\n",
      "Epoch 18/50\n",
      "43200/43200 [==============================] - 232s 5ms/step - loss: 43.2520 - fingerprint_loss: 0.2936 - key_hat_loss: 0.2148 - val_loss: 24.6260 - val_fingerprint_loss: 2.4613 - val_key_hat_loss: 0.1108\n",
      "Epoch 19/50\n",
      "43200/43200 [==============================] - 232s 5ms/step - loss: 41.7046 - fingerprint_loss: 0.2865 - key_hat_loss: 0.2071 - val_loss: 23.0667 - val_fingerprint_loss: 2.2243 - val_key_hat_loss: 0.1042\n",
      "Epoch 20/50\n",
      "43200/43200 [==============================] - 232s 5ms/step - loss: 41.3980 - fingerprint_loss: 0.3067 - key_hat_loss: 0.2055 - val_loss: 27.5349 - val_fingerprint_loss: 2.3986 - val_key_hat_loss: 0.1257\n",
      "Epoch 21/50\n",
      "43200/43200 [==============================] - 232s 5ms/step - loss: 39.5400 - fingerprint_loss: 0.3118 - key_hat_loss: 0.1961 - val_loss: 24.7351 - val_fingerprint_loss: 2.2821 - val_key_hat_loss: 0.1123\n",
      "Epoch 22/50\n",
      "43200/43200 [==============================] - 232s 5ms/step - loss: 36.8854 - fingerprint_loss: 0.3018 - key_hat_loss: 0.1829 - val_loss: 22.3476 - val_fingerprint_loss: 2.2855 - val_key_hat_loss: 0.1003\n",
      "Epoch 23/50\n",
      "43200/43200 [==============================] - 232s 5ms/step - loss: 35.8519 - fingerprint_loss: 0.3067 - key_hat_loss: 0.1777 - val_loss: 20.9382 - val_fingerprint_loss: 2.8067 - val_key_hat_loss: 0.0907\n",
      "Epoch 24/50\n",
      "43200/43200 [==============================] - 232s 5ms/step - loss: 35.1371 - fingerprint_loss: 0.2970 - key_hat_loss: 0.1742 - val_loss: 22.6582 - val_fingerprint_loss: 2.5019 - val_key_hat_loss: 0.1008\n",
      "Epoch 25/50\n",
      "43200/43200 [==============================] - 232s 5ms/step - loss: 33.3339 - fingerprint_loss: 0.3140 - key_hat_loss: 0.1651 - val_loss: 21.9767 - val_fingerprint_loss: 2.1306 - val_key_hat_loss: 0.0992\n",
      "Epoch 26/50\n",
      "43200/43200 [==============================] - 232s 5ms/step - loss: 32.7613 - fingerprint_loss: 0.3106 - key_hat_loss: 0.1623 - val_loss: 21.2542 - val_fingerprint_loss: 2.5994 - val_key_hat_loss: 0.0933\n",
      "Epoch 27/50\n",
      "43200/43200 [==============================] - 231s 5ms/step - loss: 32.1102 - fingerprint_loss: 0.3053 - key_hat_loss: 0.1590 - val_loss: 21.6534 - val_fingerprint_loss: 2.4589 - val_key_hat_loss: 0.0960\n",
      "Epoch 28/50\n",
      "43200/43200 [==============================] - 232s 5ms/step - loss: 31.5787 - fingerprint_loss: 0.3152 - key_hat_loss: 0.1563 - val_loss: 21.8716 - val_fingerprint_loss: 2.4226 - val_key_hat_loss: 0.0972\n",
      "Epoch 29/50\n",
      "43200/43200 [==============================] - 232s 5ms/step - loss: 30.3874 - fingerprint_loss: 0.3011 - key_hat_loss: 0.1504 - val_loss: 20.5054 - val_fingerprint_loss: 2.5867 - val_key_hat_loss: 0.0896\n",
      "Epoch 30/50\n",
      "43200/43200 [==============================] - 225s 5ms/step - loss: 29.2957 - fingerprint_loss: 0.3082 - key_hat_loss: 0.1449 - val_loss: 20.3650 - val_fingerprint_loss: 2.7185 - val_key_hat_loss: 0.0882\n",
      "Epoch 31/50\n",
      "43200/43200 [==============================] - 229s 5ms/step - loss: 28.5013 - fingerprint_loss: 0.2845 - key_hat_loss: 0.1411 - val_loss: 19.8642 - val_fingerprint_loss: 2.3962 - val_key_hat_loss: 0.0873\n",
      "Epoch 32/50\n",
      "43200/43200 [==============================] - 230s 5ms/step - loss: 29.2448 - fingerprint_loss: 0.3069 - key_hat_loss: 0.1447 - val_loss: 19.7231 - val_fingerprint_loss: 2.9667 - val_key_hat_loss: 0.0838\n",
      "Epoch 33/50\n",
      "43200/43200 [==============================] - 230s 5ms/step - loss: 27.6605 - fingerprint_loss: 0.2973 - key_hat_loss: 0.1368 - val_loss: 21.0350 - val_fingerprint_loss: 2.3112 - val_key_hat_loss: 0.0936\n",
      "Epoch 34/50\n",
      "43200/43200 [==============================] - 225s 5ms/step - loss: 27.6473 - fingerprint_loss: 0.3150 - key_hat_loss: 0.1367 - val_loss: 23.1892 - val_fingerprint_loss: 2.5681 - val_key_hat_loss: 0.1031\n",
      "Epoch 35/50\n",
      "43200/43200 [==============================] - 230s 5ms/step - loss: 26.7335 - fingerprint_loss: 0.3003 - key_hat_loss: 0.1322 - val_loss: 18.1169 - val_fingerprint_loss: 2.6468 - val_key_hat_loss: 0.0774\n",
      "Epoch 36/50\n",
      "43200/43200 [==============================] - 230s 5ms/step - loss: 25.5986 - fingerprint_loss: 0.3046 - key_hat_loss: 0.1265 - val_loss: 18.8775 - val_fingerprint_loss: 2.5407 - val_key_hat_loss: 0.0817\n",
      "Epoch 37/50\n",
      "43200/43200 [==============================] - 230s 5ms/step - loss: 25.7753 - fingerprint_loss: 0.2981 - key_hat_loss: 0.1274 - val_loss: 19.5907 - val_fingerprint_loss: 2.8917 - val_key_hat_loss: 0.0835\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43200/43200 [==============================] - 228s 5ms/step - loss: 26.1803 - fingerprint_loss: 0.2978 - key_hat_loss: 0.1294 - val_loss: 18.5637 - val_fingerprint_loss: 2.6375 - val_key_hat_loss: 0.0796\n",
      "Epoch 39/50\n",
      "43200/43200 [==============================] - 231s 5ms/step - loss: 26.6653 - fingerprint_loss: 0.3074 - key_hat_loss: 0.1318 - val_loss: 20.0537 - val_fingerprint_loss: 2.6267 - val_key_hat_loss: 0.0871\n",
      "Epoch 40/50\n",
      "43200/43200 [==============================] - 232s 5ms/step - loss: 24.9138 - fingerprint_loss: 0.3097 - key_hat_loss: 0.1230 - val_loss: 18.7575 - val_fingerprint_loss: 2.5771 - val_key_hat_loss: 0.0809\n",
      "Epoch 41/50\n",
      "43200/43200 [==============================] - 231s 5ms/step - loss: 24.1166 - fingerprint_loss: 0.3075 - key_hat_loss: 0.1190 - val_loss: 19.1964 - val_fingerprint_loss: 2.6260 - val_key_hat_loss: 0.0829\n",
      "Epoch 42/50\n",
      "43200/43200 [==============================] - 232s 5ms/step - loss: 23.2232 - fingerprint_loss: 0.3189 - key_hat_loss: 0.1145 - val_loss: 20.7444 - val_fingerprint_loss: 2.7534 - val_key_hat_loss: 0.0900\n",
      "Epoch 43/50\n",
      "43200/43200 [==============================] - 232s 5ms/step - loss: 23.8909 - fingerprint_loss: 0.2983 - key_hat_loss: 0.1180 - val_loss: 21.7249 - val_fingerprint_loss: 2.7712 - val_key_hat_loss: 0.0948\n",
      "Epoch 44/50\n",
      "43200/43200 [==============================] - 231s 5ms/step - loss: 24.2552 - fingerprint_loss: 0.3049 - key_hat_loss: 0.1198 - val_loss: 20.4827 - val_fingerprint_loss: 2.7224 - val_key_hat_loss: 0.0888\n",
      "Epoch 45/50\n",
      "43200/43200 [==============================] - 231s 5ms/step - loss: 23.2014 - fingerprint_loss: 0.2995 - key_hat_loss: 0.1145 - val_loss: 19.4490 - val_fingerprint_loss: 2.8813 - val_key_hat_loss: 0.0828\n",
      "Epoch 46/50\n",
      "43200/43200 [==============================] - 231s 5ms/step - loss: 22.6871 - fingerprint_loss: 0.3049 - key_hat_loss: 0.1119 - val_loss: 20.4257 - val_fingerprint_loss: 2.6517 - val_key_hat_loss: 0.0889\n",
      "Epoch 47/50\n",
      "43200/43200 [==============================] - 221s 5ms/step - loss: 21.5206 - fingerprint_loss: 0.3067 - key_hat_loss: 0.1061 - val_loss: 20.1169 - val_fingerprint_loss: 2.9076 - val_key_hat_loss: 0.0860\n",
      "Epoch 48/50\n",
      "43200/43200 [==============================] - 222s 5ms/step - loss: 21.7192 - fingerprint_loss: 0.2879 - key_hat_loss: 0.1072 - val_loss: 20.1441 - val_fingerprint_loss: 2.9764 - val_key_hat_loss: 0.0858\n",
      "Epoch 49/50\n",
      "43200/43200 [==============================] - 230s 5ms/step - loss: 21.4119 - fingerprint_loss: 0.3040 - key_hat_loss: 0.1055 - val_loss: 20.6028 - val_fingerprint_loss: 2.5249 - val_key_hat_loss: 0.0904\n",
      "Epoch 50/50\n",
      "43200/43200 [==============================] - 228s 5ms/step - loss: 21.4393 - fingerprint_loss: 0.2954 - key_hat_loss: 0.1057 - val_loss: 19.8244 - val_fingerprint_loss: 2.8890 - val_key_hat_loss: 0.0847\n",
      "Time to Fit the Model 12629.783421278\n"
     ]
    }
   ],
   "source": [
    "# n_true_train = 100000\n",
    "\n",
    "import keras.backend as K\n",
    "from keras import optimizers\n",
    "def mean_pred_loss(y_true, y_pred):\n",
    "    #when the coeficent is smaller, performance is better. When increasing, noise improves\n",
    "    sum_abs = K.abs(y_pred)\n",
    "    tmp =  K.mean(sum_abs) - 0.3 * K.mean(y_pred)# + K.epsilon()\n",
    "    return 100 * (K.abs(K.mean(y_pred)))# 1/tmp  keras.losses.mean_absolute_error(y_true, y_pred) + \n",
    "\n",
    "\n",
    "n_false_train = 0\n",
    "x_fing_w, key_hat_w, epoch, batch = 1, 200, 50, 64\n",
    "\n",
    "beg_time = time.time()\n",
    "t= n_true_train\n",
    "\n",
    "### first itme is the  key length 100 and 20000 trianing, second one: key length is 1000, 20000\n",
    "\n",
    "model = get_encoder_decoder_conv_dense_slice(sample_size=sample_size, key_length=key_length, chunk=chunk)\n",
    "#losses.mean_squared_error\n",
    "ad = optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0, amsgrad=False)\n",
    "\n",
    "model.compile(optimizer=ad, loss={'fingerprint':mean_pred_loss, 'key_hat': losses.categorical_crossentropy},\n",
    "                                loss_weights={'fingerprint': x_fing_w, 'key_hat': key_hat_w})\n",
    "\n",
    "\n",
    "# model.summary()\n",
    "print(\"Model %s is Built and Compiled in %f\" % (t, time.time() - beg_time))\n",
    "beg_time = time.time()\n",
    "\n",
    "model.fit([X_train[0:t], train_keys[0:t], array_mult_train[0:t], array_sub_train[0:t], noise_for_train[0:t]], [y_train[0:t], train_keys[0:t]], epochs=epoch, validation_split=0.1, batch_size=batch)#, validation_split=0.1,callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "print(\"Time to Fit the Model\", time.time() - beg_time)\n",
    "beg_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This is when we test encoder and decoder together using the same model: model_encoder_decoder\n",
    "x_test = np.array(X_test_all[0:n_test]).reshape((-1, sample_size, 1))\n",
    "key_options = selecting_valid_fingerprints(key_length = 100)# we use 100 keys.\n",
    "test_keys = np.array(get_keys_for_fingerprinting_data(size=n_test, key_options=key_options))\n",
    "noise_for_test = np.squeeze(noise_for_test[0:n_test])\n",
    "pred = model.predict([x_test, test_keys, array_mult_test, array_sub_test, noise_for_test])\n",
    "\n",
    "fingerprint_x2, keys_true = pred[0],  pred[1]\n",
    "ext_rate = compute_extract_rate(keys_true, true_keys = test_keys)\n",
    "\n",
    "print(\"Ext Rate:  \", ext_rate)\n",
    "compute_delay_on_each_packet(fingerprint_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_extract_rate(keys, true_keys):\n",
    "    correct = 0 \n",
    "    for i in range(len(keys)): \n",
    "        index = np.argmax(keys[i])\n",
    "        if index == np.argmax(true_keys[i]):\n",
    "            correct +=1\n",
    "    return correct/float(len(true_keys))\n",
    "def compute_delay_on_each_packet(fingerprint_x2):\n",
    "    number_of_train, sample_size = len(fingerprint_x2), len(fingerprint_x2[0])\n",
    "    for f in range(0, len(fingerprint_x2)):\n",
    "            delay, min_delay = 0, 0\n",
    "            for p in range(0, len(fingerprint_x2[f])):\n",
    "                    delay += fingerprint_x2[f][p][0]\n",
    "                    if delay < min_delay:\n",
    "                        min_delay = delay \n",
    "\n",
    "            fingerprint_x2[f][0][0] -= 1.001 * min_delay\n",
    "#######Compute the average delay on each packet.        \n",
    "    average_delay = []\n",
    "    max_d, pos = 0, 0\n",
    "    for fing in fingerprint_x2:\n",
    "        delay, neg = 0, 0\n",
    "        delays = []\n",
    "        for n in fing:\n",
    "            delay += n[0]\n",
    "            if delay > max_d:\n",
    "                max_d = delay\n",
    "            delays.append(delay)\n",
    "\n",
    "            if delay < 0:\n",
    "                neg += 1\n",
    "        if neg == 0:\n",
    "            pos += 1\n",
    "            average_delay.append(sum(delays)/sample_size)\n",
    "    print(sum(delays)/sample_size)\n",
    "    print(sample_size, number_of_train)\n",
    "    print(pos, sum(average_delay)/pos, sample_size, \"Max Delay: \", max_d)\n",
    "def decide_if_fingerprinted(keys, threshold):\n",
    "    fing = 0\n",
    "    for key in keys:\n",
    "        index = np.argmax(key)\n",
    "        if key[index] > threshold and index > 0:\n",
    "            fing += 1\n",
    "    return fing / float(len(keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ext Rate:   0.972\n",
      "66.0968705543677\n",
      "1500 2000\n",
      "2000 65.15001512413903 1500 Max Delay:  289.0710916519165\n"
     ]
    }
   ],
   "source": [
    "def compute_performance_key_length():\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading encoder takes too much time (hours), so we just use the model_encoder_decoder for encoding.\n",
    "model_decoder = load_decoder(key_length, sample_size)\n",
    "decoder_weights = []\n",
    "j = 0\n",
    "for i in range(0, 24):\n",
    "    if 'dec' in model.layers[-(24 - i)].name or 'key_hat' in model.layers[-(24 - i)].name:\n",
    "        model_decoder.layers[j].set_weights(model.layers[-(24 - i)].get_weights())\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.293 0.9925\n",
      "0.1925 0.98\n",
      "0.115 0.9685\n",
      "0.0655 0.941\n",
      "0.022 0.905\n",
      "0.007 0.8525\n",
      "0.0 0.734\n",
      "0.9695 Extraction Rate\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nkey = 100 training with 1/100 number of false data. sample size = 1800, number of training=20000\\n0.2395 0.9845\\n0.156 0.9695\\n0.093 0.9535\\n0.043 0.931\\n0.014 0.8975\\n0.0055 0.867\\n0.0005 0.766\\n0.9685 Extraction Rate\\nExt Rate:   0.969\\n1800 2000\\n2000 138.32507355565957 1800 Max Delay:  556.0820367336273\\n##############################################################################################\\n\\n\\nsample size 3300, training data = 45000, key = 1000\\n\\n0.327 0.9895\\n0.1985 0.9705\\n0.122 0.9525\\n0.0635 0.933\\n0.025 0.9\\n0.0095 0.861\\n0.0015 0.7415\\n0.9675 Extraction Rate\\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_for_test = noise_for_test.reshape((-1, sample_size, 1))\n",
    "\n",
    "output_fin = noise_for_test[0:n_test] + x_test\n",
    "keys_true_fp = model_decoder.predict([output_fin])\n",
    "\n",
    "###### True positve:\n",
    "output_fin = noise_for_test[0:n_test] + x_test + fingerprint_x2\n",
    "\n",
    "keys_true_tp = model_decoder.predict([output_fin])\n",
    "ext_rate = compute_extract_rate(keys_true_tp, test_keys)\n",
    "thresholds = [0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]\n",
    "\n",
    "\n",
    "for t in thresholds:\n",
    "    fp = decide_if_fingerprinted(keys_true_fp, t)\n",
    "    tp = decide_if_fingerprinted(keys_true_tp, t)\n",
    "    \n",
    "    print(fp, tp)\n",
    "print(ext_rate, 'Extraction Rate')\n",
    "'''\n",
    "\n",
    "key = 100 training with 1/100 number of false data. sample size = 1800, number of training=20000\n",
    "0.2395 0.9845\n",
    "0.156 0.9695\n",
    "0.093 0.9535\n",
    "0.043 0.931\n",
    "0.014 0.8975\n",
    "0.0055 0.867\n",
    "0.0005 0.766\n",
    "0.9685 Extraction Rate\n",
    "Ext Rate:   0.969\n",
    "1800 2000\n",
    "2000 138.32507355565957 1800 Max Delay:  556.0820367336273\n",
    "##############################################################################################\n",
    "\n",
    "\n",
    "sample size 3300, training data = 48000, key = 1000\n",
    "\n",
    "0.293 0.9925\n",
    "0.1925 0.98\n",
    "0.115 0.9685\n",
    "0.0655 0.941\n",
    "0.022 0.905\n",
    "0.007 0.8525\n",
    "0.0 0.734\n",
    "0.9695 Extraction Rate\n",
    "2000 80.0513701567251 3300 Max Delay:  369.6583148241043\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_false_train = 0\n",
    "x_fing_w, key_hat_w, epoch, batch = 1, 200, 100, 64\n",
    "model_name = str(sample_size) + \"_\" + str(key_length) + \"_\" + str(\n",
    "    n_true_train) + \"_\" + str(n_false_train) + \"_\" + str(epoch) + \"_\" + str(x_fing_w) + \"_\" + str(key_hat_w)\n",
    "\n",
    "beg_time = time.time()\n",
    "#models_key_length = []\n",
    "keys = [100]\n",
    "n_true_train = 30000\n",
    "for k in keys:\n",
    "\n",
    "    key_options = selecting_valid_fingerprints(key_length = k)# we use 100 keys.\n",
    "    X_train, y_train, train_keys = get_false_true_training(X_train_all[0:n_true_train], key_options)\n",
    "    train_keys = np.array(train_keys)\n",
    "    print(\"Finished radinf\")\n",
    "\n",
    "    model= get_encoder_decoder_conv_dense_slice(sample_size=sample_size, key_length=k)\n",
    "    #losses.mean_squared_error\n",
    "    ad = optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0, amsgrad=False)\n",
    "\n",
    "    model.compile(optimizer=ad, loss={'fingerprint':mean_pred_loss, 'key_hat': losses.categorical_crossentropy},\n",
    "                                    loss_weights={'fingerprint': x_fing_w, 'key_hat': key_hat_w})\n",
    "\n",
    "\n",
    "    # model.summary()\n",
    "    print(\"Model %s is Built and Compiled in %f\" % (model_name ,time.time() - beg_time))\n",
    "    beg_time = time.time()\n",
    "\n",
    "    model.fit([X_train, train_keys, array_mult_train[0:n_true_train], array_sub_train[0:n_true_train], noise_for_train[0:n_true_train]], [y_train, train_keys], epochs=epoch, validation_split=0.1, batch_size=batch)#, validation_split=0.1,callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "    print(\"Time to Fit the Model\", time.time() - beg_time)\n",
    "\n",
    "    \n",
    "    models_key_length.append(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_for_results = '/home/fatemeh/Dropbox/Fingerprint/Results/'\n",
    "def compute_ROC_data(n_train):\n",
    "    target_name = open(path_for_results + str(n_train)+\"_\" + str(sample_size)+\"_\"+str(key_length) + '.txt', 'w')\n",
    "    sample_size = 900\n",
    "    key_length = 100\n",
    "    n_test = 5000\n",
    "    thresholds = [0.6, 0.7, 0.8, 0.9]\n",
    "    X = create_sample_size_dataset(all_ipds, sample_size = sample_size)\n",
    "    key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    model_decoder, model_encoder = load_model_for_testing(key_length, sample_size, n_train)\n",
    "    X_test = np.expand_dims(X[n_train:n_train + n_test], axis=1)\n",
    "    test_keys = np.expand_dims(get_fingerprint_for_data(size = n_test, key_options = key_options), axis=1)\n",
    "    fingerprint_x = model_encoder.predict([test_keys])\n",
    "    false_poses, true_poses = [], []\n",
    "    \n",
    "     ########## True positve:\n",
    "    output_fin = add_gussian_noise_to_ipds_fingerprinted(fingerprint = fingerprint_x, x_test = X_test, std =10)\n",
    "    keys_true = model_decoder.predict([output_fin])\n",
    "\n",
    "    ########## False positve: \n",
    "    output_non = add_gussian_noise_to_ipds_non_fingerprinted(X_test, std =10)\n",
    "    keys_false = model_decoder.predict([output_non])\n",
    "    for t in thresholds:\n",
    "        true_pos = decide_if_fingerprinted(keys_true, threshold = t)\n",
    "        false_pos = decide_if_fingerprinted(keys_false, threshold = t)\n",
    "        false_poses.append(false_pos)\n",
    "        true_poses.append(true_pos)\n",
    "        \n",
    "    write_array_to_file(array = false_poses, target =target_name, delimiter =' ')\n",
    "    write_array_to_file(array = true_poses, target =target_name, delimiter =' ')\n",
    "    target_name.close()\n",
    "    \n",
    "compute_ROC_data(n_train=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_impact_of_jitter():\n",
    "    sample_size = 600\n",
    "    key_length = 10\n",
    "    n_train = 50000\n",
    "    n_test = 5000\n",
    "    X = create_sample_size_dataset(all_ipds, sample_size = sample_size)\n",
    "    key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "\n",
    "    target_name = open(path_for_results + str(n_train)+\"_\" + str(sample_size)+\"_\"+str(key_length) + '.txt', 'w')\n",
    "\n",
    "    jitters = [1, 10, 50, 100]\n",
    "   \n",
    "    model_decoder, model_encoder = load_model_for_testing(key_length, sample_size, n_train)\n",
    "    X_test = np.expand_dims(X[n_train:n_train + n_test], axis=1)\n",
    "    test_keys = np.expand_dims(get_fingerprint_for_data(size = n_test, key_options = key_options), axis=1)\n",
    "    fingerprint_x = model_encoder.predict([test_keys])\n",
    "    false_poses, true_poses, ext_rates = [], [], []\n",
    "\n",
    "    for std in jitters:      \n",
    "        ########## True positve:\n",
    "        output_fin = add_gussian_noise_to_ipds_fingerprinted(fingerprint = fingerprint_x, x_test = X_test, std =std)\n",
    "        keys_true = model_decoder.predict([output_fin])\n",
    "        true_pos = decide_if_fingerprinted(keys_true, threshold=4)\n",
    "        key_pred = extract_keys_from_key_hat(keys_true)\n",
    "        error_rate = compute_error_rate_flowwise(predict_key = key_pred, true_key = test_keys)\n",
    "\n",
    "        ########## False positve: \n",
    "        output_non = add_gussian_noise_to_ipds_non_fingerprinted(X_test, std =std)\n",
    "        keys_false = model_decoder.predict([output_non])\n",
    "        false_pos = decide_if_fingerprinted(keys_false, threshold=4)\n",
    "        false_poses.append(false_pos)\n",
    "        true_poses.append(true_pos)\n",
    "        ext_rates.append(1 - error_rate)\n",
    "    write_array_to_file(array = ext_rates, target =target_name, delimiter =' ')\n",
    "    write_array_to_file(array = false_poses, target =target_name, delimiter =' ')\n",
    "    write_array_to_file(array = true_poses, target =target_name, delimiter =' ')\n",
    "    target_name.close()\n",
    "# compute_impact_of_jitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_fing_w, key_hat_w, epoch = 1, 50, 100\n",
    "def call_fit_load_eval_Main():\n",
    "    n_all_true_trains =[5000]# [5000, 10000, 20000, 50000]#5000,, \n",
    "    sample_sizes = [600]#[400, 200, 600]\n",
    "    key_lengths = [10]#, 15, 20]\n",
    "\n",
    "    for sample_size in sample_sizes:\n",
    "        X = create_sample_size_dataset(all_ipds, sample_size = sample_size)\n",
    "        for key_length in key_lengths:\n",
    "            key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "            false_poses, true_poses, ext_rates = [], [], []\n",
    "            target_name = open(path_for_results + str(sample_size)+\"_\"+str(key_length) + '.txt', 'w')\n",
    "\n",
    "            for train_number in n_all_true_trains:\n",
    "                false_pos, true_pos, ext_rate = fit_model_load_evaulte(n_true_train =train_number, key_length=key_length, sample_size=sample_size,X=X,key_options=key_options)\n",
    "                false_poses.append(false_pos)\n",
    "                true_poses.append(true_pos)\n",
    "                ext_rates.append(ext_rate)\n",
    "                print(false_pos, true_pos, ext_rate)\n",
    "            write_array_to_file(array = ext_rates, target =target_name, delimiter =' ')\n",
    "            write_array_to_file(array = false_poses, target =target_name, delimiter =' ')\n",
    "            write_array_to_file(array = true_poses, target =target_name, delimiter =' ')\n",
    "            target_name.close()\n",
    "call_fit_load_eval_Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_model_for_more_epochs():\n",
    "    sample_size, key_length, n_true_train, epoch = 600, 10, 50000, 250\n",
    "    n_false_train = int(n_true_train/10)\n",
    "    x_fing_w, key_hat_w = 1, 50\n",
    "    model_name = \"march_10\" + str(sample_size) + \"_\" + str(key_length) + \"_\" + str(\n",
    "    n_true_train) + \"_\" + str(n_false_train) + \"_\" + str(epoch) + \"_\" + str(x_fing_w) + \"_\" + str(key_hat_w)\n",
    "\n",
    "    model = load_NN_model(path + model_name)\n",
    "    model_encoder_decoder.compile(optimizer='adam', \n",
    "                              loss=losses.mean_absolute_error,\n",
    "                                  loss_weights={'fingerprint':x_fing_w, 'key_hat':key_hat_w})\n",
    "\n",
    "    model_encoder_decoder.fit([X_train, training_keys], [y_train, training_keys],\n",
    "                        batch_size = 64, epochs = epoch + 250, verbose = 0)\n",
    "    #save_model_weights(model_encoder_decoder, name=model_name)\n",
    "# reload_model_for_more_epochs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_all_Main():\n",
    "    n_all_true_trains = [10000]# 5000, 10000, 20000, 50000]\n",
    "    sample_sizes = [600]\n",
    "    key_lengths = [10]\n",
    "    x_fing_w, key_hat_w, epoch = 1, 50, 100\n",
    "    n_test = 1000\n",
    "    for sample_size in sample_sizes:\n",
    "            \n",
    "            X = create_sample_size_dataset(all_ipds_for_test, sample_size = sample_size)\n",
    "            for key_length in key_lengths:\n",
    "                key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "                false_poses, true_poses, ext_rates = [], [], []\n",
    "                #target_name = open('/home/fatemeh/Dropbox/Fingerprint/Results/500_' + str(sample_size)+\"_\"+str(key_length) + '.txt', 'w')\n",
    "\n",
    "                for train_number in n_all_true_trains:\n",
    "                    model_decoder, model_encoder = load_model_for_testing(key_length, sample_size, train_number)\n",
    "                    false_pos, true_pos, ext_rate = evalute_encoder_decoder(model_decoder,model_encoder, X, sample_size, key_length,\n",
    "                                                                            key_options, train_number, int(train_number/10), n_test=n_test)\n",
    "                    false_poses.append(false_pos)\n",
    "                    true_poses.append(true_pos)\n",
    "                    ext_rates.append(ext_rate)\n",
    "                    print(sample_size, key_length, train_number, \"Result: \", false_pos, true_pos, ext_rate)\n",
    "#                 write_array_to_file(array = ext_rates, target =target_name, delimiter =' ')\n",
    "#                 write_array_to_file(array = false_poses, target =target_name, delimiter =' ')\n",
    "#                 write_array_to_file(array = true_poses, target =target_name, delimiter =' ')\n",
    "#                 target_name.close() \n",
    "load_test_all_Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_true_trains = [5000, 10000, 20000, 50000, 100000]\n",
    "sample_size, key_length = 600, 10\n",
    "epoch = 250\n",
    "for n_true_train in n_true_trains:\n",
    "    n_false_train = int(n_true_train/10)\n",
    "    key_hat_w, x_hat_w = 50, 1\n",
    "    model_name = \"march10_\" + str(sample_size) + \"_\" + str(key_length) + \"_\" + str(\n",
    "        n_true_train) + \"_\" + str(n_false_train) + \"_\" + str(epoch) + \"_\" + str(x_hat_w) + \"_\" + str(key_hat_w)\n",
    "\n",
    "    model = load_NN_model(path + model_name)\n",
    "\n",
    "    X_train, y_train, training_keys = get_false_true_training(n_true_train, n_false_train, key_length, X, key_options)\n",
    "    print(\"Finished reading dataset\")\n",
    "\n",
    "    model.compile(optimizer='adam', \n",
    "                                  loss=losses.mean_absolute_error,\n",
    "                                      loss_weights={'fingerprint':1, 'key_hat':50})\n",
    "    model.fit([X_train, training_keys], [y_train, training_keys],\n",
    "                            batch_size = 64, epochs = epoch, verbose = 0)\n",
    "    \n",
    "    model_name = \"march10_\" + str(sample_size) + \"_\" + str(key_length) + \"_\" + str(\n",
    "        n_true_train) + \"_\" + str(n_false_train) + \"_\" + str(500) + \"_\" + str(x_hat_w) + \"_\" + str(key_hat_w)\n",
    "\n",
    "    save_model_weights(model, name= model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_encoder(key_length, sample_size):\n",
    "    chunk, p = 10, 0\n",
    "    Input_ipd = Input(shape=(sample_size, 1), name='input1')  # this is needed just for the decoding\n",
    "    Input_key = Input(shape=(key_length,), name='input2')\n",
    "    fingerprint_mult = Input(shape=(chunk,), name='input3')\n",
    "    fingerprint_sub = Input(shape=(chunk,), name='input4')\n",
    "    \n",
    "    ipd = Flatten(name =\"ipd_flatten1\")(Input_ipd)\n",
    "    outputs = []\n",
    "    \n",
    "    quant = int(sample_size/chunk)\n",
    "    def slice(x):\n",
    "        return x[:, p * chunk:(1 + p) * chunk]\n",
    "    \n",
    "    key1 = Dense(32, name='key1')(Input_key)\n",
    "\n",
    "    sliced_ipd = Lambda(slice)(ipd)\n",
    "    x_fingerprint = sliced_ipd\n",
    "    for i in range(0, quant):\n",
    "        sliced_ipd = Lambda(slice)(ipd)\n",
    "        ss = Concatenate(name = 'concat'+ str(p))([x_fingerprint, sliced_ipd]) \n",
    "        ipd1 = Dense(32, name = 'dense'+ str(p))(ss)\n",
    "        batch_2 = BatchNormalization(name = 'batch'+ str(p))(ipd1)\n",
    "        relu_2 = Activation('relu', name = 'act'+ str(p))(batch_2)\n",
    "        \n",
    "        ipds_merged_all = Concatenate(name = 'concat_key_'+ str(p))([relu_2, key1])\n",
    "        dense_enc1 = Dense(64, name = 'dense_enc1' + str(p))(ipds_merged_all)\n",
    "        batch_2 = BatchNormalization(name = 'batch2_'+ str(p))(dense_enc1)\n",
    "        relu_2 = Activation('relu', name = 'act2_'+ str(p))(batch_2)\n",
    "        dense_drop_enc1 = Dropout(0.3, name = 'dense_drop_enc1' + str(p))(relu_2)\n",
    "        \n",
    "        x_fingerprint_sig = Dense(chunk, name = 'fingerprint_sig' + str(p), activation = 'sigmoid')(dense_drop_enc1)\n",
    "        x_fingerprint_mult = Multiply(name = 'fingerprint_mult' + str(p))([x_fingerprint_sig, fingerprint_mult])\n",
    "        x_fingerprint = Add(name = 'ipd_delay' + str(p))([x_fingerprint_mult, fingerprint_sub])\n",
    "        outputs.append(x_fingerprint)\n",
    "        p += 1\n",
    "    x_fingerprint = Concatenate(name = 'fingerprint2')(outputs)\n",
    "    x_fingerprint_output = Reshape((sample_size,1), name='fingerprint')(x_fingerprint)\n",
    "    model_encoder = Model(inputs=[Input_key,Input_ipd,  fingerprint_mult, fingerprint_sub], outputs=[x_fingerprint_output])\n",
    "    #model_encoder.load_weights(filepath=path + model_name + \".h5\", by_name=True)\n",
    "    return model_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
