{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Activation, Dropout, Dense, Input, Add, Multiply, Concatenate,Lambda\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D,  Flatten, Dot,Reshape\n",
    "from keras.models import Model\n",
    "import random, time, os\n",
    "import numpy as np\n",
    "from keras import losses\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.python import keras\n",
    "\n",
    "def create_sample_size_dataset(all_ipds, sample_size, n_sample):\n",
    "    #number_of_samples = int(len(all_ipds) / sample_size)\n",
    "    all_samples = []\n",
    "    for p in range(n_sample):\n",
    "        all_samples.append(all_ipds[p * sample_size:(p + 1) * sample_size])\n",
    "    return all_samples\n",
    "\n",
    "def write_array_to_file(array, target, delimiter):\n",
    "    for k in range(0, len(array)):\n",
    "        target.write(str(array[k]) + delimiter)\n",
    "    target.write(\"\\n\")\n",
    "\n",
    "def read_from_file(path):\n",
    "    with open(path, 'r') as content_file:\n",
    "        content = content_file.read()\n",
    "        return content\n",
    "\n",
    "\n",
    "def create_ipd_dataset(address):\n",
    "    files = os.listdir(address)\n",
    "    all_ipds = []\n",
    "    for f in files:\n",
    "            ipd = read_from_file(address + f).split(' ')\n",
    "            all_ipds.extend(convert_stringArrays_to_floatArray(ipd))\n",
    "    return all_ipds\n",
    "\n",
    "\n",
    "def isfloat(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def convert_stringArrays_to_floatArray(array):\n",
    "    intArray = []\n",
    "\n",
    "    for k in array:\n",
    "        if isfloat(k):\n",
    "            intArray.append(float(k))\n",
    "    return intArray\n",
    "\n",
    "\n",
    "def convert_stringArrays_to_intArray(array):\n",
    "    intArray = []\n",
    "\n",
    "    for k in array:\n",
    "        if isfloat(k):\n",
    "            intArray.append(int(k))\n",
    "    return intArray\n",
    "\n",
    "def get_fingerprints_for_ipds(n_train, sample_size, alpha):\n",
    "    \n",
    "    #Previous one was all + and the largest value was 50.\n",
    "   \n",
    "    fingerprint_output = []\n",
    "    while len(fingerprint_output) < n_train:\n",
    "        finger = [random.uniform(0, 250)]\n",
    "        neg_numbers = 0\n",
    "        for i in range(sample_size - 1):\n",
    "            #if random.randrange(0, 3) == 0:\n",
    "            if random.randrange(0, 2) == 1:\n",
    "                finger.append(random.uniform(0, alpha))#random.uniform(0, 10)\n",
    "            else:\n",
    "                finger.append(-1 * random.uniform(0, alpha))#\n",
    "            if sum(finger) < 0:\n",
    "                neg_numbers += 1\n",
    "#             else:\n",
    "#                 finger.append(0) \n",
    "        #if neg_numbers < 50:  ## this can be a hyperparameter\n",
    "        fingerprint_output.append(finger)\n",
    "    return fingerprint_output\n",
    "\n",
    "def get_keys_for_fingerprinting_data(size, key_options):\n",
    "    selected_keys = []\n",
    "    for i in range(size):\n",
    "        rnd = random.randrange(0, len(key_options))\n",
    "        selected_keys.append(key_options[rnd])\n",
    "\n",
    "    return selected_keys\n",
    "\n",
    "\n",
    "def get_false_true_data(X, key_options,alpha):\n",
    "    \n",
    "    training_keys_true = get_keys_for_fingerprinting_data(size=len(X), key_options=key_options)\n",
    "    #X_train_true =X# [x for x in X]\n",
    "    y_train_true = get_fingerprints_for_ipds(len(X), sample_size=len(X[0]),alpha=alpha)#,training_keys=training_keys_true)\n",
    "    \n",
    "    ####### changing true trianing to false\n",
    "    \n",
    "    X_train = np.expand_dims(X, axis=1).reshape((-1, len(X[0]), 1))\n",
    "    y_train = np.expand_dims(y_train_true, axis=1).reshape((-1, len(X[0]), 1))\n",
    "    training_keys = training_keys_true#,#np.expand_dims(training_keys_true, axis=1)\n",
    "    \n",
    "    return X_train, y_train, training_keys\n",
    "\n",
    "def selecting_valid_fingerprints(key_length):\n",
    "    all_keys = []\n",
    "    #address = '/home/fatemeh/MyProjects/Fingerprint/Synthetic dataset/keys/' + str(key_length) + \"/\"\n",
    "    key_i = np.zeros(key_length)\n",
    "    #keys = os.listdir(address)\n",
    "    for k in range(key_length):\n",
    "        key_i[k] = 1\n",
    "        #key_i = convert_stringArrays_to_intArray(read_from_file(address + k).split(\" \"))\n",
    "        #if key_i [0] == 1:\n",
    "         #   continue\n",
    "        all_keys.append(key_i)\n",
    "        key_i[k] = 0\n",
    "            \n",
    "    return all_keys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_decoder_conv_dense_slice(sample_size, key_length, chunk):\n",
    "    p = 0\n",
    "    Input_ipd = Input(shape=(sample_size, 1), name='input1')  # this is needed just for the decoding\n",
    "    Input_key = Input(shape=(key_length,), name='input2')\n",
    "    fingerprint_mult = Input(shape=(chunk,), name='input3')\n",
    "    fingerprint_sub = Input(shape=(chunk,), name='input4')\n",
    "    network_noise = Input(shape=(sample_size,), name='input5')\n",
    "    \n",
    "    ipd = Flatten(name =\"ipd_flatten1\")(Input_ipd)\n",
    "    outputs = []\n",
    "    \n",
    "    quant = int(sample_size/chunk)\n",
    "    def slice(x):\n",
    "        return x[:, p * chunk:(1 + p) * chunk]\n",
    "    \n",
    "    key1 = Dense(64, name='key1')(Input_key)\n",
    "\n",
    "    sliced_ipd = Lambda(slice)(ipd)\n",
    "    x_fingerprint = sliced_ipd\n",
    "    for i in range(0, quant):\n",
    "        sliced_ipd = Lambda(slice)(ipd)\n",
    "        ss = Concatenate(name = 'concat'+ str(p))([x_fingerprint, sliced_ipd]) \n",
    "        ipd1 = Dense(32, name = 'dense'+ str(p))(ss)\n",
    "        batch_2 = BatchNormalization(name = 'batch'+ str(p))(ipd1)\n",
    "        relu_2 = Activation('relu', name = 'act'+ str(p))(batch_2)\n",
    "        \n",
    "        ipds_key_merge = Concatenate(name = 'concat_key_'+ str(p))([relu_2, key1])\n",
    "        dense_enc1 = Dense(64, name = 'dense_enc1' + str(p))(ipds_key_merge)\n",
    "        batch_2 = BatchNormalization(name = 'batch2_'+ str(p))(dense_enc1)\n",
    "        relu_2 = Activation('relu', name = 'act2_'+ str(p))(batch_2)\n",
    "        dense_drop_enc1 = Dropout(0.3, name = 'dense_drop_enc1' + str(p))(relu_2)\n",
    "        \n",
    "        x_fingerprint_sig = Dense(chunk, name = 'fingerprint_sig' + str(p), activation = 'sigmoid')(dense_drop_enc1)\n",
    "        x_fingerprint_mult = Multiply(name = 'fingerprint_mult' + str(p))([x_fingerprint_sig, fingerprint_mult])\n",
    "        x_fingerprint = Add(name = 'ipd_delay' + str(p))([x_fingerprint_mult, fingerprint_sub])\n",
    "        outputs.append(x_fingerprint)\n",
    "        p += 1\n",
    "    x_fingerprint = Concatenate(name = 'fingerprint2')(outputs)\n",
    "    x_fingerprint_output = Reshape((sample_size, 1), name='fingerprint')(x_fingerprint)\n",
    "\n",
    "    x_ipd = Add(name = 'x_ipd')([x_fingerprint, ipd, network_noise])\n",
    "        \n",
    "    x_ipd_reshape = Reshape((sample_size, 1),name = 'reshape_dec')(x_ipd)\n",
    "    \n",
    "    conv_dec_2 = Conv1D(filters = 20, kernel_size=10, padding='same', name='conv_dec_2')(x_ipd_reshape)\n",
    "    conv_batch_2 = BatchNormalization(name='conv_batch_2_dec')(conv_dec_2)\n",
    "    conv_relu_2 = Activation('relu', name='conv_relu_2_dec')(conv_batch_2)\n",
    "    conv_drop_2 = Dropout(0.3, name='conv_drop_2_dec')(conv_relu_2)\n",
    "    max_pool_dec_2 = MaxPooling1D(pool_size=1, name=\"max_pool_dec_2\")(conv_drop_2)\n",
    "    \n",
    "    conv_dec_3 = Conv1D(filters = 10, kernel_size=10, padding='same', name='conv_dec_3')(max_pool_dec_2)\n",
    "    conv_batch_3 = BatchNormalization(name='conv_batch_3_dec')(conv_dec_3)\n",
    "    conv_relu_3 = Activation('relu', name='conv_relu_3_dec')(conv_batch_3)\n",
    "    conv_drop_2 = Dropout(0.3, name='conv_drop_3_dec')(conv_relu_3)\n",
    "    max_pool_dec_3 = MaxPooling1D(pool_size=1, name=\"max_pool_dec_3\")(conv_drop_2)\n",
    "    max_pool_dec_3_f = Flatten(name =\"flate_max3_dec\")(max_pool_dec_3)\n",
    "\n",
    "    dense_dec_1 = Dense(256, name='dense_dec_1')(max_pool_dec_3_f)\n",
    "    \n",
    "    dense_batch_dec1 = BatchNormalization(name='dense_batch_dec1')(dense_dec_1)\n",
    "    dense_relu_dec1 = Activation('relu', name='dense_relu_dec1')(dense_batch_dec1)\n",
    "    dense_drop_dec1 = Dropout(0.3, name='dense_drop_dec1')(dense_relu_dec1)    \n",
    "    \n",
    "    dense_dec_2 = Dense(64, name='dense_dec_2')(dense_drop_dec1)\n",
    "    dense_batch_dec2 = BatchNormalization(name='dense_batch_dec2')(dense_dec_2)\n",
    "    dense_relu_dec2 = Activation('relu', name='dense_relu_dec2')(dense_batch_dec2)\n",
    "    dense_drop_dec2 = Dropout(0.3, name='dense_drop_dec2')(dense_relu_dec2)\n",
    "    \n",
    "    key_hat = Dense(key_length, activation='softmax', name='key_hat')(dense_drop_dec2)\n",
    "\n",
    "    return Model(inputs=[Input_ipd, Input_key, fingerprint_mult, fingerprint_sub, network_noise], outputs=[x_fingerprint_output, key_hat])#, key_hat])\n",
    "\n",
    "\n",
    "def load_decoder(key_length, sample_size):\n",
    "    \n",
    "    Input_ipd = Input(shape=(sample_size, 1), name='reshape_dec') \n",
    "    #x_ipd_reshape = Reshape((sample_size, 1), name = 'reshape')(Input_ipd)\n",
    "    conv_dec_2 = Conv1D(filters = 20, kernel_size=10, padding='same', name='conv_dec_2')(Input_ipd)\n",
    "    conv_batch_2 = BatchNormalization(name='conv_batch_2_dec')(conv_dec_2)\n",
    "    conv_relu_2 = Activation('relu', name='conv_relu_2_dec')(conv_batch_2)\n",
    "    conv_drop_2 = Dropout(0.3, name='conv_drop_2_dec')(conv_relu_2)\n",
    "    max_pool_dec_2 = MaxPooling1D(pool_size=1, name=\"max_pool_dec_2\")(conv_drop_2)\n",
    "    \n",
    "    conv_dec_3 = Conv1D(filters = 10, kernel_size=10, padding='same', name='conv_dec_3')(max_pool_dec_2)\n",
    "    conv_batch_3 = BatchNormalization(name='conv_batch_3_dec')(conv_dec_3)\n",
    "    conv_relu_3 = Activation('relu', name='conv_relu_3_dec')(conv_batch_3)\n",
    "    conv_drop_2 = Dropout(0.3, name='conv_drop_3_dec')(conv_relu_3)\n",
    "    max_pool_dec_3 = MaxPooling1D(pool_size=1, name=\"max_pool_dec_3\")(conv_drop_2)\n",
    "    max_pool_dec_3_f = Flatten(name =\"flate_max3_dec\")(max_pool_dec_3)\n",
    "\n",
    "    dense_dec_1 = Dense(256, name='dense_dec_1')(max_pool_dec_3_f)\n",
    "    dense_batch_dec1 = BatchNormalization(name='dense_batch_dec1')(dense_dec_1)\n",
    "    dense_relu_dec1 = Activation('relu', name='dense_relu_dec1')(dense_batch_dec1)\n",
    "    dense_drop_dec1 = Dropout(0.3, name='dense_drop_dec1')(dense_relu_dec1)    \n",
    "    \n",
    "    dense_dec_2 = Dense(64, name='dense_dec_2')(dense_drop_dec1)\n",
    "    dense_batch_dec2 = BatchNormalization(name='dense_batch_dec2')(dense_dec_2)\n",
    "    dense_relu_dec2 = Activation('relu', name='dense_relu_dec2')(dense_batch_dec2)\n",
    "    dense_drop_dec2 = Dropout(0.3, name='dense_drop_dec2')(dense_relu_dec2)\n",
    "    \n",
    "    key_hat = Dense(key_length, activation='softmax', name='key_hat')(dense_drop_dec2)\n",
    "    \n",
    "    \n",
    "    model_decoder = Model(inputs=[Input_ipd], outputs=[key_hat])\n",
    "    #model_decoder.set_weights(model.get_weights())\n",
    "    #model_decoder.load_weights(filepath=path + model_name + \".h5\", by_name=True)\n",
    "\n",
    "    return model_decoder\n",
    "\n",
    "def compute_extract_rate(keys, true_keys):\n",
    "    correct = 0 \n",
    "    for i in range(len(keys)): \n",
    "        if np.argmax(keys[i]) == np.argmax(true_keys[i]):\n",
    "            correct +=1\n",
    "    return correct/float(len(keys))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/fatemeh/MyProjects/Fingerprint/models/\"\n",
    "rate = '10'\n",
    "all_ipds_for_test = create_ipd_dataset(\n",
    "    address='/home/fatemeh/MyProjects/Fingerprint/Synthetic dataset/in/' + rate + '/test/')\n",
    "all_ipds_for_train = create_ipd_dataset(\n",
    "    address='/home/fatemeh/MyProjects/Fingerprint/Synthetic dataset/in/' + rate + '/train/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 5000 Numbre of training and testing data\n"
     ]
    }
   ],
   "source": [
    "n_true_train, n_test = 100000, 5000\n",
    "sample_size, key_length = 300, 1024\n",
    "alpha = 25\n",
    "\n",
    "X_train_all = create_sample_size_dataset(all_ipds_for_train, sample_size = sample_size,n_sample=n_true_train)\n",
    "X_test_all = create_sample_size_dataset(all_ipds_for_test, sample_size = sample_size,n_sample=n_test)\n",
    "print(len(X_train_all), len(X_test_all), \"Numbre of training and testing data\")\n",
    "\n",
    "\n",
    "\n",
    "key_length = 100\n",
    "key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "X_train, y_train, train_keys = get_false_true_data(X_train_all, key_options, alpha=alpha)#get_only_true_data\n",
    "train_keys = np.array(train_keys)\n",
    "print(\"finished generating training data!...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9e859393a443>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mX_train_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_sample_size_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_ipds_for_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_true_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mX_test_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_sample_size_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_ipds_for_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Numbre of training and testing data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_size' is not defined"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras import optimizers\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "def mean_pred_loss(y_true, y_pred):\n",
    "    #when the coeficent is smaller, performance is better. When increasing, noise improves\n",
    "    sum_abs = K.abs(y_pred)\n",
    "    tmp =  K.mean(sum_abs) - 0.3 * K.mean(y_pred)# + K.epsilon()\n",
    "    return 100 * (K.abs(K.mean(y_pred)))# 1/tmp  keras.losses.mean_absolute_error(y_true, y_pred) + \n",
    "\n",
    "def get_mult_sub_for_fingerprinting(n_data, max_delay, chunk, sample_size):\n",
    "    array_mult, array_sub = [], []\n",
    "    for x in range(0, n_data):\n",
    "        array_mult.append([max_delay] * chunk)\n",
    "        array_sub.append([-max_delay/2] * chunk)\n",
    "    array_mult = np.array(array_mult)\n",
    "    array_sub = np.array(array_sub)\n",
    "    return array_mult, array_sub\n",
    "\n",
    "def get_noise_simulation_array(n_data, std, sample_size):\n",
    "    noise = []\n",
    "    for x in range(0, n_data):\n",
    "        #noise.append(np.random.normal(0, std, sample_size))\n",
    "        #noise.append(np.random.uniform(0, std, sample_size))\n",
    "        noise.append(np.random.laplace(0, std, sample_size));\n",
    "    noise = np.array(noise)\n",
    "    return noise\n",
    "n_trains = [200000]\n",
    "key_len = [512, 1024, 1024 *4, 1024*32]#2^(8, 10, 12, 15)\n",
    "models = []\n",
    "for key_length in key_len:\n",
    "        for n in n_trains:\n",
    "        \n",
    "            n_true_train, n_test = n, 5000\n",
    "            alpha = 25\n",
    "\n",
    "            X_train_all = create_sample_size_dataset(all_ipds_for_train, sample_size = sample_size,n_sample=n_true_train)\n",
    "            X_test_all = create_sample_size_dataset(all_ipds_for_test[500*300:], sample_size = sample_size,n_sample=n_test)\n",
    "            print(len(X_train_all), len(X_test_all), \"Numbre of training and testing data\")\n",
    "\n",
    "            key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "            X_train, y_train, train_keys = get_false_true_data(X_train_all, key_options, alpha=alpha)#get_only_true_data\n",
    "            train_keys = np.array(train_keys)\n",
    "            print(\"finished generating training data!...\")\n",
    "\n",
    "\n",
    "            beg_time = time.time()\n",
    "            x_fing_w, key_hat_w, epoch, batch = 1, 100, 50, 64\n",
    "            std, max_fing_delay = 1, alpha\n",
    "            chunk = 10\n",
    "\n",
    "            array_mult_test, array_sub_test =  get_mult_sub_for_fingerprinting(n_test, max_delay=max_fing_delay, chunk=chunk,\n",
    "                                                                                    sample_size=sample_size)\n",
    "            noise_for_test = get_noise_simulation_array(n_test, std=std, sample_size=sample_size)\n",
    "            array_mult_train, array_sub_train =  get_mult_sub_for_fingerprinting(n_true_train, max_delay=max_fing_delay, chunk=chunk,\n",
    "                                                                                    sample_size=sample_size)\n",
    "            noise_for_train = get_noise_simulation_array(n_true_train, std=std, sample_size=sample_size)\n",
    "            t = n_true_train\n",
    "\n",
    "            print(\"Date : \", datetime.datetime.now())\n",
    "\n",
    "            model_10 = get_encoder_decoder_conv_dense_slice(sample_size=sample_size, key_length=key_length, chunk=chunk)\n",
    "            # losses.mean_squared_error# 0.001\n",
    "            ad = optimizers.Adam(lr = 1e-3, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0, amsgrad=False)\n",
    "\n",
    "            model_10.compile(optimizer=ad, loss={'fingerprint': mean_pred_loss, 'key_hat': losses.categorical_crossentropy},\n",
    "                      loss_weights={'fingerprint': x_fing_w, 'key_hat': key_hat_w})\n",
    "\n",
    "            # model.summary()\n",
    "            print(\"Model %s is Built and Compiled in %f\" % (t, time.time() - beg_time))\n",
    "            beg_time = time.time()\n",
    "\n",
    "            model_10.fit([X_train[0:t], train_keys[0:t], array_mult_train[0:t], array_sub_train[0:t], noise_for_train[0:t]],\n",
    "                  [y_train[0:t], train_keys[0:t]], epochs=epoch, validation_split=0.1,\n",
    "                  batch_size=batch)  # callbacks=callbacks_list, verbose=0)\n",
    "            models.append(model_10)\n",
    "\n",
    "            print(\"Time to Fit the Model\", time.time() - beg_time)\n",
    "\n",
    "            #### This is when we test encoder and decoder together using the same model: model_encoder_decoder\n",
    "            x_test = np.array(X_test_all[0:n_test]).reshape((-1, sample_size, 1))\n",
    "            key_options = selecting_valid_fingerprints(key_length=key_length)  # we use 100 keys.\n",
    "            test_keys = np.array(get_keys_for_fingerprinting_data(size=n_test, key_options=key_options))\n",
    "            noise_for_test = np.squeeze(noise_for_test)\n",
    "            pred = model_10.predict([x_test, test_keys, array_mult_test, array_sub_test, noise_for_test])\n",
    "\n",
    "            fingerprint_x22, keys_true = pred[0], pred[1]\n",
    "            ext_rate = compute_extract_rate(keys_true, true_keys=test_keys)\n",
    "\n",
    "            print(\"Ext Rate:  \", ext_rate)\n",
    "# avg_d, max_d = compute_delay_on_each_packet(fingerprint_x2)\n",
    "\n",
    "#fingerprint_x2 = adjust_fingerprint_delays(fingerprint_x2)\n",
    "#avg_d, max_d = compute_delay_on_packets(fingerprint_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time to Fit the Model 22027.91614151001, is ~6 hours.\n",
    "Ext Rate:   1.0 when 1000000 traiing wiht key_length = 500\n",
    "\n",
    "Same thing with key_length = 1000 is : \n",
    "    \n",
    "#### try the sample_size = 600...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha =25\n",
    "#pkt = 10,  ext_rate = 0.701, key_length = 20, epoch = 100, n_train = 5000\n",
    "#usiing uniform (0,alpha) for fingerprint same parameters as above line: ext_rate: 0.54\n",
    "\n",
    "Time to Fit the Model 2291.7441816329956 = 2291/3600 = 36 minutes?\n",
    "Ext Rate:   0.978 with 10000 training and 20 keys, with 20,000 it goes to ext_rate = 1\n",
    "    10000 training and 200 key  ext_rate =0.019\n",
    "    with 20000 and 200 key ext_rate =0.53\n",
    "    and with 40, 000 it goes to ext_rate = 1\n",
    ".\n",
    "\n",
    "#n_train = 24000 with key_length = 200, epoch = 50, pkt  =100 , ext_rate = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adjust_fingerprint_delays' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ee207a577aa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fingerprints_for_ipds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madjust_fingerprint_delays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mavg_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_delay_on_packets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'adjust_fingerprint_delays' is not defined"
     ]
    }
   ],
   "source": [
    "def get_fingerprints_for_ipds(n_train, sample_size, alpha):\n",
    "    \n",
    "    #Previous one was all + and the largest value was 50.\n",
    "   \n",
    "    fingerprint_output = []\n",
    "    thrshold = 100\n",
    "    while len(fingerprint_output) < n_train:\n",
    "        finger = [0]#[random.uniform(0, 250)]\n",
    "        neg_numbers = 0\n",
    "        delay = 0\n",
    "        for i in range(sample_size - 1):\n",
    "            #if random.randrange(0, 3) == 0:\n",
    "            rnd = random.randrange(0, 2)\n",
    "            if rnd == 1 and delay < thrshold:\n",
    "                finger.append(alpha)\n",
    "            elif rnd ==1 and delay>= thrshold:\n",
    "                finger.append(-alpha)\n",
    "                \n",
    "            elif rnd == 0 and delay <= 0 and  delay >-thrshold: # random.randrange(0, 2) == 0:# and np.abs(delay) <50:\n",
    "                finger.append(-1 * alpha)#\n",
    "            elif rnd ==0 and delay <=0 and delay <=-thrshold:\n",
    "                finger.append(alpha)\n",
    "            elif rnd == 0 and delay > 0:\n",
    "                finger.append(-1 * alpha)\n",
    "            delay += finger[-1]\n",
    "            #print(delay)\n",
    "            if sum(finger) < 0:\n",
    "                neg_numbers += 1\n",
    "#             else:\n",
    "#                 finger.append(0) \n",
    "        #if neg_numbers < 50:  ## this can be a hyperparameter\n",
    "        fingerprint_output.append(finger)\n",
    "    return fingerprint_output\n",
    "\n",
    "\n",
    "\n",
    "y = get_fingerprints_for_ipds(1, 1500, alpha=25) \n",
    "y = np.expand_dims(y, axis=1).reshape((-1, 1500, 1))\n",
    "y = adjust_fingerprint_delays(y)\n",
    "avg_d, max_d = compute_delay_on_packets(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative delay:  4000 Average delay for them: 421.7638539961676 Max Delay:  2091.979405403137\n"
     ]
    }
   ],
   "source": [
    "def adjust_fingerprint_delays(fingerprint_x2):\n",
    "    number_of_train, sample_size = len(fingerprint_x2), len(fingerprint_x2[0])\n",
    "    for f in range(0, len(fingerprint_x2)):\n",
    "            delay, min_delay = 0, 0\n",
    "            for p in range(0, len(fingerprint_x2[f])):\n",
    "                    delay += fingerprint_x2[f][p][0]\n",
    "                    #print(delay)\n",
    "                    if delay < min_delay:\n",
    "                        min_delay = delay \n",
    "            fingerprint_x2[f][0][0] -= 1.001 * min_delay\n",
    "           # print(\"min\",min_delay)\n",
    "           # break\n",
    "    return fingerprint_x2\n",
    "\n",
    "def compute_delay_on_packets(fingerprint_x2):\n",
    "#######Compute the average delay on each packet.        \n",
    "    average_delay = []\n",
    "    max_d, pos = 0, 0\n",
    "    for fing in fingerprint_x2:\n",
    "        delay, neg = 0, 0\n",
    "        delays = []\n",
    "        for n in fing:\n",
    "            delay += n[0]\n",
    "            if delay > max_d:\n",
    "                max_d = delay\n",
    "            delays.append(delay)\n",
    "\n",
    "            if delay < 0:\n",
    "                neg += 1\n",
    "        if neg == 0:\n",
    "            pos += 1\n",
    "            average_delay.append(sum(delays)/sample_size)\n",
    "   # print(sum(delays)/sample_size,\" Average Delay\")\n",
    "   # print(sample_size, number_of_train)\n",
    "    print(\"Negative delay: \",pos, \"Average delay for them:\",sum(average_delay)/pos, \"Max Delay: \", max_d)\n",
    "    return sum(average_delay)/pos, max_d\n",
    "fingerprint_x2 = adjust_fingerprint_delays(fingerprint_x2)\n",
    "avg_d, max_d = compute_delay_on_packets(fingerprint_x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    target = open(\"/home/fatemeh/MyProjects/Fingerprint/encoder/fing_ipds/\" + str(i) + \".txt\", 'w')\n",
    "    array = x_test[i] + fingerprint_x2[i]\n",
    "    array = np.squeeze(array)\n",
    "    write_array_to_file(array, target, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape_dec 1960\n",
      "conv_dec_2 1961\n",
      "conv_batch_2_dec 1962\n",
      "conv_relu_2_dec 1963\n",
      "conv_drop_2_dec 1964\n",
      "max_pool_dec_2 1965\n",
      "conv_dec_3 1966\n",
      "conv_batch_3_dec 1967\n",
      "conv_relu_3_dec 1968\n",
      "conv_drop_3_dec 1969\n",
      "max_pool_dec_3 1970\n",
      "flate_max3_dec 1971\n",
      "dense_dec_1 1972\n",
      "dense_batch_dec1 1973\n",
      "dense_relu_dec1 1974\n",
      "dense_drop_dec1 1975\n",
      "dense_dec_2 1976\n",
      "dense_batch_dec2 1977\n",
      "dense_relu_dec2 1978\n",
      "dense_drop_dec2 1979\n",
      "key_hat 1981\n",
      "Time it takes to load the decoder:  126.30563545227051\n"
     ]
    }
   ],
   "source": [
    "# TODO: I want to check to see if I 0 the negative fingerprints, I can have the same extraction rate.\n",
    "model_decoder = load_decoder(key_length, sample_size)\n",
    "#model_decoder.set_weights(model.get_weights())\n",
    "beg_time = time.time()\n",
    "i = 0\n",
    "for layer in model_10.layers:\n",
    "  #  w_target=layer.get_weights()\n",
    "    for dec_layer in model_decoder.layers:\n",
    "        if layer.name ==dec_layer.name:\n",
    "            print(layer.name, i)\n",
    "            dec_layer.set_weights(layer.get_weights())\n",
    "    i += 1\n",
    "print(\"Time it takes to load the decoder: \", time.time() - beg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fingerprinted_ipds():\n",
    "    path = '/home/fatemeh/MyProjects/Fingerprint/encoder/ext_ipds/'\n",
    "    all_ipds = []\n",
    "    for i in range(11):\n",
    "        string_ipds = read_from_file(path + str(i) + \".txt\").split(\" \")\n",
    "        ipds = convert_stringArrays_to_floatArray(string_ipds)\n",
    "        all_ipds.append(ipds)\n",
    "    return all_ipds\n",
    "all_ipds = read_fingerprinted_ipds()\n",
    "fingerprinted_ipds=np.array(all_ipds).reshape((-1, sample_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "noise_for_test = np.squeeze(noise_for_test)\n",
    "noise_for_test = get_noise_simulation_array(n_test, std=1, sample_size=sample_size)\n",
    "noise_for_test=np.array(noise_for_test[0:n_test]).reshape((-1, sample_size, 1))\n",
    "fingerprinted_ipds = fingerprint_x2 + x_test + noise_for_test\n",
    "'''\n",
    "\n",
    "\n",
    "key_hat = model_decoder.predict(fingerprinted_ipds)\n",
    "ext_rate = compute_extract_rate(key_hat, true_keys=test_keys)\n",
    "print(ext_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# avg_d, max_d = compute_delay_on_each_packet(fingerprint_x2)\n",
    "rate=100\n",
    "with open('sep_results.csv', mode='a') as csv_file:\n",
    "    fieldnames = ['sample_size', 'key_length', \"number_training\", 'ext_rate', 'average_delay', 'max_delay',\n",
    "                  'packet_rate', 'std', 'max_train_noise','date']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "    #writer.writeheader()\n",
    "    writer.writerow(\n",
    "        {'sample_size': sample_size, 'key_length': key_length, \"number_training\": n_true_train, 'ext_rate': ext_rate,\n",
    "         'average_delay': avg_d, 'max_delay': max_d, 'packet_rate': rate, 'std': std, 'max_train_noise': max_fing_delay,'date': datetime.datetime.now()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.993533930040474\n",
      "4.925919764672603\n",
      "4.863956283428593\n",
      "4.892161564091119\n",
      "5.108175177722096\n",
      "4.905545496833845\n",
      "4.938426689987192\n",
      "4.921194612130275\n",
      "4.9018388531082415\n",
      "4.888976889759503\n"
     ]
    }
   ],
   "source": [
    "#print(X_train[0][0:100])\n",
    "# print(fingerprint_x2[0][0:100])\n",
    "array_mult_test2, array_sub_test2, noise_for_test2 = get_arrays_mult_noise_sub(10,max_delay=5,chunk=10,std=5,sample_size=sample_size)\n",
    "\n",
    "d = 0\n",
    "import numpy as np\n",
    "for f in noise_for_test2:\n",
    "    dd = []\n",
    "    for p in f:\n",
    "        dd.append(p)\n",
    "    std = np.std(dd, axis=0)\n",
    "    print(std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ext Rate:   0.0195\n",
      "98.70847329990069\n",
      "1500 4000\n",
      "4000 49.32459005435246 1500 Max Delay:  223.82663989067078\n"
     ]
    }
   ],
   "source": [
    "ext_rate = compute_extract_rate(keys_true, true_keys = test_keys)\n",
    "\n",
    "print(\"Ext Rate:  \", ext_rate)\n",
    "compute_delay_on_each_packet(fingerprint_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81200 5333 Numbre of training and testing data\n",
      "100 ratio\n",
      "finished getting true data\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "54000/54000 [==============================] - 525s 10ms/step - loss: 940.4309 - fingerprint_loss: 0.5370 - key_hat_loss: 4.6995 - val_loss: 919.3937 - val_fingerprint_loss: 0.7712 - val_key_hat_loss: 4.5931\n",
      "Epoch 2/50\n",
      "54000/54000 [==============================] - 260s 5ms/step - loss: 900.0707 - fingerprint_loss: 0.5327 - key_hat_loss: 4.4977 - val_loss: 886.4092 - val_fingerprint_loss: 0.4663 - val_key_hat_loss: 4.4297\n",
      "Epoch 3/50\n",
      "54000/54000 [==============================] - 259s 5ms/step - loss: 758.0978 - fingerprint_loss: 0.4542 - key_hat_loss: 3.7882 - val_loss: 900.9127 - val_fingerprint_loss: 0.4163 - val_key_hat_loss: 4.5025\n",
      "Epoch 4/50\n",
      "54000/54000 [==============================] - 269s 5ms/step - loss: 310.8727 - fingerprint_loss: 0.3480 - key_hat_loss: 1.5526 - val_loss: 175.3338 - val_fingerprint_loss: 1.0500 - val_key_hat_loss: 0.8714\n",
      "Epoch 5/50\n",
      "54000/54000 [==============================] - 267s 5ms/step - loss: 188.6905 - fingerprint_loss: 0.3636 - key_hat_loss: 0.9416 - val_loss: 103.5668 - val_fingerprint_loss: 1.1307 - val_key_hat_loss: 0.5122\n",
      "Epoch 6/50\n",
      "54000/54000 [==============================] - 266s 5ms/step - loss: 139.7549 - fingerprint_loss: 0.3638 - key_hat_loss: 0.6970 - val_loss: 71.8514 - val_fingerprint_loss: 1.4163 - val_key_hat_loss: 0.3522\n",
      "Epoch 7/50\n",
      "54000/54000 [==============================] - 265s 5ms/step - loss: 113.9955 - fingerprint_loss: 0.3555 - key_hat_loss: 0.5682 - val_loss: 66.9623 - val_fingerprint_loss: 1.2522 - val_key_hat_loss: 0.3286\n",
      "Epoch 8/50\n",
      "54000/54000 [==============================] - 266s 5ms/step - loss: 98.6902 - fingerprint_loss: 0.3828 - key_hat_loss: 0.4915 - val_loss: 41.3542 - val_fingerprint_loss: 1.7267 - val_key_hat_loss: 0.1981\n",
      "Epoch 9/50\n",
      "54000/54000 [==============================] - 265s 5ms/step - loss: 87.1638 - fingerprint_loss: 0.3794 - key_hat_loss: 0.4339 - val_loss: 75.7226 - val_fingerprint_loss: 1.4328 - val_key_hat_loss: 0.3714\n",
      "Epoch 10/50\n",
      "54000/54000 [==============================] - 265s 5ms/step - loss: 77.9767 - fingerprint_loss: 0.3624 - key_hat_loss: 0.3881 - val_loss: 35.7555 - val_fingerprint_loss: 1.7122 - val_key_hat_loss: 0.1702\n",
      "Epoch 11/50\n",
      "54000/54000 [==============================] - 265s 5ms/step - loss: 70.3313 - fingerprint_loss: 0.3737 - key_hat_loss: 0.3498 - val_loss: 37.0456 - val_fingerprint_loss: 1.7630 - val_key_hat_loss: 0.1764\n",
      "Epoch 12/50\n",
      "54000/54000 [==============================] - 266s 5ms/step - loss: 63.8551 - fingerprint_loss: 0.3800 - key_hat_loss: 0.3174 - val_loss: 34.3690 - val_fingerprint_loss: 1.5597 - val_key_hat_loss: 0.1640\n",
      "Epoch 13/50\n",
      "54000/54000 [==============================] - 267s 5ms/step - loss: 61.4691 - fingerprint_loss: 0.3728 - key_hat_loss: 0.3055 - val_loss: 42.9701 - val_fingerprint_loss: 1.8628 - val_key_hat_loss: 0.2055\n",
      "Epoch 14/50\n",
      "54000/54000 [==============================] - 275s 5ms/step - loss: 56.0929 - fingerprint_loss: 0.3856 - key_hat_loss: 0.2785 - val_loss: 44.2234 - val_fingerprint_loss: 1.8270 - val_key_hat_loss: 0.2120\n",
      "Epoch 15/50\n",
      "54000/54000 [==============================] - 267s 5ms/step - loss: 53.0511 - fingerprint_loss: 0.3824 - key_hat_loss: 0.2633 - val_loss: 47.5773 - val_fingerprint_loss: 1.9143 - val_key_hat_loss: 0.2283\n",
      "Epoch 16/50\n",
      "54000/54000 [==============================] - 277s 5ms/step - loss: 50.6544 - fingerprint_loss: 0.3643 - key_hat_loss: 0.2515 - val_loss: 33.0840 - val_fingerprint_loss: 1.4494 - val_key_hat_loss: 0.1582\n",
      "Epoch 17/50\n",
      "54000/54000 [==============================] - 275s 5ms/step - loss: 48.8602 - fingerprint_loss: 0.3621 - key_hat_loss: 0.2425 - val_loss: 59.9817 - val_fingerprint_loss: 1.5511 - val_key_hat_loss: 0.2922\n",
      "Epoch 18/50\n",
      "54000/54000 [==============================] - 263s 5ms/step - loss: 46.4441 - fingerprint_loss: 0.3823 - key_hat_loss: 0.2303 - val_loss: 39.7536 - val_fingerprint_loss: 1.7385 - val_key_hat_loss: 0.1901\n",
      "Epoch 19/50\n",
      "54000/54000 [==============================] - 262s 5ms/step - loss: 44.1063 - fingerprint_loss: 0.3819 - key_hat_loss: 0.2186 - val_loss: 89.7794 - val_fingerprint_loss: 1.7296 - val_key_hat_loss: 0.4402\n",
      "Epoch 20/50\n",
      "54000/54000 [==============================] - 270s 5ms/step - loss: 42.7524 - fingerprint_loss: 0.3973 - key_hat_loss: 0.2118 - val_loss: 50.6303 - val_fingerprint_loss: 1.9302 - val_key_hat_loss: 0.2435\n",
      "Epoch 21/50\n",
      "54000/54000 [==============================] - 280s 5ms/step - loss: 40.7061 - fingerprint_loss: 0.3709 - key_hat_loss: 0.2017 - val_loss: 26.9912 - val_fingerprint_loss: 1.7822 - val_key_hat_loss: 0.1260\n",
      "Epoch 22/50\n",
      "54000/54000 [==============================] - 276s 5ms/step - loss: 38.6745 - fingerprint_loss: 0.3782 - key_hat_loss: 0.1915 - val_loss: 31.4162 - val_fingerprint_loss: 1.8851 - val_key_hat_loss: 0.1477\n",
      "Epoch 23/50\n",
      "54000/54000 [==============================] - 263s 5ms/step - loss: 38.4965 - fingerprint_loss: 0.3582 - key_hat_loss: 0.1907 - val_loss: 19.7632 - val_fingerprint_loss: 1.8583 - val_key_hat_loss: 0.0895\n",
      "Epoch 24/50\n",
      "54000/54000 [==============================] - 264s 5ms/step - loss: 38.2520 - fingerprint_loss: 0.3844 - key_hat_loss: 0.1893 - val_loss: 21.2462 - val_fingerprint_loss: 1.9975 - val_key_hat_loss: 0.0962\n",
      "Epoch 25/50\n",
      "54000/54000 [==============================] - 277s 5ms/step - loss: 36.5415 - fingerprint_loss: 0.3658 - key_hat_loss: 0.1809 - val_loss: 20.1977 - val_fingerprint_loss: 2.0566 - val_key_hat_loss: 0.0907\n",
      "Epoch 26/50\n",
      "54000/54000 [==============================] - 280s 5ms/step - loss: 35.3318 - fingerprint_loss: 0.3689 - key_hat_loss: 0.1748 - val_loss: 20.6696 - val_fingerprint_loss: 2.3141 - val_key_hat_loss: 0.0918\n",
      "Epoch 27/50\n",
      "54000/54000 [==============================] - 265s 5ms/step - loss: 34.6664 - fingerprint_loss: 0.3633 - key_hat_loss: 0.1715 - val_loss: 20.9651 - val_fingerprint_loss: 1.9726 - val_key_hat_loss: 0.0950\n",
      "Epoch 28/50\n",
      "54000/54000 [==============================] - 266s 5ms/step - loss: 34.2002 - fingerprint_loss: 0.3603 - key_hat_loss: 0.1692 - val_loss: 30.5230 - val_fingerprint_loss: 2.0309 - val_key_hat_loss: 0.1425\n",
      "Epoch 29/50\n",
      "54000/54000 [==============================] - 280s 5ms/step - loss: 33.6566 - fingerprint_loss: 0.3697 - key_hat_loss: 0.1664 - val_loss: 21.6281 - val_fingerprint_loss: 1.9576 - val_key_hat_loss: 0.0984\n",
      "Epoch 30/50\n",
      "54000/54000 [==============================] - 273s 5ms/step - loss: 30.5151 - fingerprint_loss: 0.3538 - key_hat_loss: 0.1508 - val_loss: 44.6891 - val_fingerprint_loss: 1.9758 - val_key_hat_loss: 0.2136\n",
      "Epoch 31/50\n",
      "54000/54000 [==============================] - 265s 5ms/step - loss: 31.2489 - fingerprint_loss: 0.3757 - key_hat_loss: 0.1544 - val_loss: 51.8373 - val_fingerprint_loss: 2.2921 - val_key_hat_loss: 0.2477\n",
      "Epoch 32/50\n",
      "54000/54000 [==============================] - 265s 5ms/step - loss: 30.0372 - fingerprint_loss: 0.3662 - key_hat_loss: 0.1484 - val_loss: 30.0118 - val_fingerprint_loss: 2.0148 - val_key_hat_loss: 0.1400\n",
      "Epoch 33/50\n",
      "54000/54000 [==============================] - 270s 5ms/step - loss: 29.9032 - fingerprint_loss: 0.3656 - key_hat_loss: 0.1477 - val_loss: 18.8054 - val_fingerprint_loss: 1.8890 - val_key_hat_loss: 0.0846\n",
      "Epoch 34/50\n",
      "54000/54000 [==============================] - 276s 5ms/step - loss: 28.9566 - fingerprint_loss: 0.3654 - key_hat_loss: 0.1430 - val_loss: 27.2345 - val_fingerprint_loss: 2.1201 - val_key_hat_loss: 0.1256\n",
      "Epoch 35/50\n",
      "54000/54000 [==============================] - 263s 5ms/step - loss: 29.1643 - fingerprint_loss: 0.3669 - key_hat_loss: 0.1440 - val_loss: 31.0092 - val_fingerprint_loss: 2.0747 - val_key_hat_loss: 0.1447\n",
      "Epoch 36/50\n",
      "54000/54000 [==============================] - 269s 5ms/step - loss: 28.1734 - fingerprint_loss: 0.3685 - key_hat_loss: 0.1390 - val_loss: 17.8831 - val_fingerprint_loss: 1.8602 - val_key_hat_loss: 0.0801\n",
      "Epoch 37/50\n",
      "54000/54000 [==============================] - 281s 5ms/step - loss: 28.0937 - fingerprint_loss: 0.3799 - key_hat_loss: 0.1386 - val_loss: 21.6546 - val_fingerprint_loss: 1.7581 - val_key_hat_loss: 0.0995\n",
      "Epoch 38/50\n",
      "54000/54000 [==============================] - 263s 5ms/step - loss: 28.2460 - fingerprint_loss: 0.3912 - key_hat_loss: 0.1393 - val_loss: 26.6925 - val_fingerprint_loss: 2.0097 - val_key_hat_loss: 0.1234\n",
      "Epoch 39/50\n",
      "54000/54000 [==============================] - 267s 5ms/step - loss: 26.4085 - fingerprint_loss: 0.3838 - key_hat_loss: 0.1301 - val_loss: 21.9945 - val_fingerprint_loss: 2.1165 - val_key_hat_loss: 0.0994\n",
      "Epoch 40/50\n",
      "54000/54000 [==============================] - 285s 5ms/step - loss: 25.3904 - fingerprint_loss: 0.3721 - key_hat_loss: 0.1251 - val_loss: 21.5589 - val_fingerprint_loss: 2.2579 - val_key_hat_loss: 0.0965\n",
      "Epoch 41/50\n",
      "54000/54000 [==============================] - 276s 5ms/step - loss: 25.8353 - fingerprint_loss: 0.3598 - key_hat_loss: 0.1274 - val_loss: 38.2633 - val_fingerprint_loss: 1.9553 - val_key_hat_loss: 0.1815\n",
      "Epoch 42/50\n",
      "54000/54000 [==============================] - 273s 5ms/step - loss: 24.3449 - fingerprint_loss: 0.3471 - key_hat_loss: 0.1200 - val_loss: 32.8452 - val_fingerprint_loss: 2.1188 - val_key_hat_loss: 0.1536\n",
      "Epoch 43/50\n",
      "54000/54000 [==============================] - 285s 5ms/step - loss: 24.6413 - fingerprint_loss: 0.3554 - key_hat_loss: 0.1214 - val_loss: 21.9575 - val_fingerprint_loss: 2.2647 - val_key_hat_loss: 0.0985\n",
      "Epoch 44/50\n",
      "54000/54000 [==============================] - 272s 5ms/step - loss: 25.2880 - fingerprint_loss: 0.3571 - key_hat_loss: 0.1247 - val_loss: 20.4467 - val_fingerprint_loss: 2.1387 - val_key_hat_loss: 0.0915\n",
      "Epoch 45/50\n",
      "54000/54000 [==============================] - 271s 5ms/step - loss: 24.9361 - fingerprint_loss: 0.3425 - key_hat_loss: 0.1230 - val_loss: 45.7472 - val_fingerprint_loss: 2.0677 - val_key_hat_loss: 0.2184\n",
      "Epoch 46/50\n",
      "54000/54000 [==============================] - 285s 5ms/step - loss: 25.5192 - fingerprint_loss: 0.3340 - key_hat_loss: 0.1259 - val_loss: 17.9692 - val_fingerprint_loss: 2.2815 - val_key_hat_loss: 0.0784\n",
      "Epoch 47/50\n",
      "54000/54000 [==============================] - 279s 5ms/step - loss: 24.7465 - fingerprint_loss: 0.3453 - key_hat_loss: 0.1220 - val_loss: 17.0963 - val_fingerprint_loss: 1.8923 - val_key_hat_loss: 0.0760\n",
      "Epoch 48/50\n",
      "54000/54000 [==============================] - 271s 5ms/step - loss: 24.2901 - fingerprint_loss: 0.3467 - key_hat_loss: 0.1197 - val_loss: 18.5384 - val_fingerprint_loss: 2.0394 - val_key_hat_loss: 0.0825\n",
      "Epoch 49/50\n",
      "54000/54000 [==============================] - 283s 5ms/step - loss: 24.4617 - fingerprint_loss: 0.3614 - key_hat_loss: 0.1205 - val_loss: 17.9934 - val_fingerprint_loss: 2.4276 - val_key_hat_loss: 0.0778\n",
      "Epoch 50/50\n",
      "54000/54000 [==============================] - 271s 5ms/step - loss: 24.5841 - fingerprint_loss: 0.3607 - key_hat_loss: 0.1211 - val_loss: 17.0112 - val_fingerprint_loss: 2.3009 - val_key_hat_loss: 0.0736\n",
      "Time to Fit the Model 13866.692671775818\n",
      "Ext Rate:   0.981\n",
      "47.74679842678706\n",
      "1500 4000\n",
      "4000 99.59597391273797 1500 Max Delay:  560.3011629581451\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 23.3023 - fingerprint_loss: 0.3883 - key_hat_loss: 0.1146 - val_loss: 2.3554 - val_fingerprint_loss: 2.2696 - val_key_hat_loss: 4.2925e-04\n",
      "Epoch 2/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 21.2457 - fingerprint_loss: 0.3262 - key_hat_loss: 0.1046 - val_loss: 2.6465 - val_fingerprint_loss: 2.5398 - val_key_hat_loss: 5.3363e-04\n",
      "Epoch 3/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 17.9058 - fingerprint_loss: 0.3832 - key_hat_loss: 0.0876 - val_loss: 2.4169 - val_fingerprint_loss: 2.3279 - val_key_hat_loss: 4.4512e-04\n",
      "Epoch 4/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 18.9434 - fingerprint_loss: 0.3579 - key_hat_loss: 0.0929 - val_loss: 2.6124 - val_fingerprint_loss: 2.3329 - val_key_hat_loss: 0.0014\n",
      "Epoch 5/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 16.2670 - fingerprint_loss: 0.3416 - key_hat_loss: 0.0796 - val_loss: 2.2730 - val_fingerprint_loss: 2.1629 - val_key_hat_loss: 5.5056e-04\n",
      "Epoch 6/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 14.6923 - fingerprint_loss: 0.3628 - key_hat_loss: 0.0716 - val_loss: 2.3349 - val_fingerprint_loss: 2.2524 - val_key_hat_loss: 4.1271e-04\n",
      "Epoch 7/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 15.8678 - fingerprint_loss: 0.3547 - key_hat_loss: 0.0776 - val_loss: 3.0001 - val_fingerprint_loss: 2.4874 - val_key_hat_loss: 0.0026\n",
      "Epoch 8/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 16.6087 - fingerprint_loss: 0.3477 - key_hat_loss: 0.0813 - val_loss: 2.9992 - val_fingerprint_loss: 2.1275 - val_key_hat_loss: 0.0044\n",
      "Epoch 9/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 15.5725 - fingerprint_loss: 0.3868 - key_hat_loss: 0.0759 - val_loss: 3.5027 - val_fingerprint_loss: 2.4733 - val_key_hat_loss: 0.0051\n",
      "Epoch 10/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 16.2981 - fingerprint_loss: 0.3635 - key_hat_loss: 0.0797 - val_loss: 4.5830 - val_fingerprint_loss: 2.2365 - val_key_hat_loss: 0.0117\n",
      "Epoch 11/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 12.9874 - fingerprint_loss: 0.3750 - key_hat_loss: 0.0631 - val_loss: 2.2394 - val_fingerprint_loss: 1.9890 - val_key_hat_loss: 0.0013\n",
      "Epoch 12/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 12.8173 - fingerprint_loss: 0.3165 - key_hat_loss: 0.0625 - val_loss: 2.3236 - val_fingerprint_loss: 2.1868 - val_key_hat_loss: 6.8396e-04\n",
      "Epoch 13/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 13.4149 - fingerprint_loss: 0.3856 - key_hat_loss: 0.0651 - val_loss: 2.3744 - val_fingerprint_loss: 2.2065 - val_key_hat_loss: 8.3945e-04\n",
      "Epoch 14/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 13.2353 - fingerprint_loss: 0.3562 - key_hat_loss: 0.0644 - val_loss: 3.4475 - val_fingerprint_loss: 2.2328 - val_key_hat_loss: 0.0061\n",
      "Epoch 15/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 14.0110 - fingerprint_loss: 0.3656 - key_hat_loss: 0.0682 - val_loss: 2.3701 - val_fingerprint_loss: 2.1295 - val_key_hat_loss: 0.0012\n",
      "Epoch 16/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 13.3206 - fingerprint_loss: 0.3188 - key_hat_loss: 0.0650 - val_loss: 2.8257 - val_fingerprint_loss: 2.6781 - val_key_hat_loss: 7.3806e-04\n",
      "Epoch 17/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 11.7543 - fingerprint_loss: 0.3248 - key_hat_loss: 0.0571 - val_loss: 2.3692 - val_fingerprint_loss: 2.0987 - val_key_hat_loss: 0.0014\n",
      "Epoch 18/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 10.9376 - fingerprint_loss: 0.3714 - key_hat_loss: 0.0528 - val_loss: 2.7491 - val_fingerprint_loss: 2.4096 - val_key_hat_loss: 0.0017\n",
      "Epoch 19/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 11.3936 - fingerprint_loss: 0.3115 - key_hat_loss: 0.0554 - val_loss: 2.2534 - val_fingerprint_loss: 2.1636 - val_key_hat_loss: 4.4943e-04\n",
      "Epoch 20/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 11.3137 - fingerprint_loss: 0.3369 - key_hat_loss: 0.0549 - val_loss: 2.3610 - val_fingerprint_loss: 2.1195 - val_key_hat_loss: 0.0012\n",
      "Epoch 21/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 13.1869 - fingerprint_loss: 0.3016 - key_hat_loss: 0.0644 - val_loss: 2.3107 - val_fingerprint_loss: 2.1421 - val_key_hat_loss: 8.4310e-04\n",
      "Epoch 22/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 11.2771 - fingerprint_loss: 0.3796 - key_hat_loss: 0.0545 - val_loss: 3.8851 - val_fingerprint_loss: 2.0594 - val_key_hat_loss: 0.0091\n",
      "Epoch 23/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 10.6218 - fingerprint_loss: 0.3144 - key_hat_loss: 0.0515 - val_loss: 2.4689 - val_fingerprint_loss: 2.2279 - val_key_hat_loss: 0.0012\n",
      "Epoch 24/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 12.0764 - fingerprint_loss: 0.3672 - key_hat_loss: 0.0585 - val_loss: 1.9989 - val_fingerprint_loss: 1.8824 - val_key_hat_loss: 5.8241e-04\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 44s 5ms/step - loss: 11.4743 - fingerprint_loss: 0.3522 - key_hat_loss: 0.0556 - val_loss: 3.2145 - val_fingerprint_loss: 2.1029 - val_key_hat_loss: 0.0056\n",
      "Epoch 26/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 11.8253 - fingerprint_loss: 0.3625 - key_hat_loss: 0.0573 - val_loss: 2.6989 - val_fingerprint_loss: 2.2489 - val_key_hat_loss: 0.0023\n",
      "Epoch 27/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 10.0726 - fingerprint_loss: 0.3354 - key_hat_loss: 0.0487 - val_loss: 2.4329 - val_fingerprint_loss: 2.1389 - val_key_hat_loss: 0.0015\n",
      "Epoch 28/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 10.8778 - fingerprint_loss: 0.3097 - key_hat_loss: 0.0528 - val_loss: 4.8458 - val_fingerprint_loss: 2.4681 - val_key_hat_loss: 0.0119\n",
      "Epoch 29/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 10.1352 - fingerprint_loss: 0.3204 - key_hat_loss: 0.0491 - val_loss: 3.2815 - val_fingerprint_loss: 2.3402 - val_key_hat_loss: 0.0047\n",
      "Epoch 30/50\n",
      "9000/9000 [==============================] - 45s 5ms/step - loss: 9.9959 - fingerprint_loss: 0.3376 - key_hat_loss: 0.0483 - val_loss: 2.8815 - val_fingerprint_loss: 2.3096 - val_key_hat_loss: 0.0029\n",
      "Epoch 31/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 9.3748 - fingerprint_loss: 0.3491 - key_hat_loss: 0.0451 - val_loss: 4.6202 - val_fingerprint_loss: 2.1472 - val_key_hat_loss: 0.0124\n",
      "Epoch 32/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 12.6129 - fingerprint_loss: 0.3500 - key_hat_loss: 0.0613 - val_loss: 5.2024 - val_fingerprint_loss: 2.3185 - val_key_hat_loss: 0.0144\n",
      "Epoch 33/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 10.3355 - fingerprint_loss: 0.2908 - key_hat_loss: 0.0502 - val_loss: 3.0031 - val_fingerprint_loss: 2.1581 - val_key_hat_loss: 0.0042\n",
      "Epoch 34/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 9.2507 - fingerprint_loss: 0.3331 - key_hat_loss: 0.0446 - val_loss: 3.0496 - val_fingerprint_loss: 2.2215 - val_key_hat_loss: 0.0041\n",
      "Epoch 35/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 9.8748 - fingerprint_loss: 0.3016 - key_hat_loss: 0.0479 - val_loss: 2.7183 - val_fingerprint_loss: 2.1593 - val_key_hat_loss: 0.0028\n",
      "Epoch 36/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 10.0643 - fingerprint_loss: 0.3522 - key_hat_loss: 0.0486 - val_loss: 3.9902 - val_fingerprint_loss: 2.3126 - val_key_hat_loss: 0.0084\n",
      "Epoch 37/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 10.2662 - fingerprint_loss: 0.3341 - key_hat_loss: 0.0497 - val_loss: 4.8333 - val_fingerprint_loss: 1.9158 - val_key_hat_loss: 0.0146\n",
      "Epoch 38/50\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 8.3123 - fingerprint_loss: 0.3296 - key_hat_loss: 0.0399 - val_loss: 2.6796 - val_fingerprint_loss: 2.0879 - val_key_hat_loss: 0.0030\n",
      "Epoch 39/50\n",
      "9000/9000 [==============================] - 47s 5ms/step - loss: 8.2668 - fingerprint_loss: 0.3101 - key_hat_loss: 0.0398 - val_loss: 2.4667 - val_fingerprint_loss: 2.0112 - val_key_hat_loss: 0.0023\n",
      "Epoch 40/50\n",
      "9000/9000 [==============================] - 47s 5ms/step - loss: 9.5460 - fingerprint_loss: 0.3449 - key_hat_loss: 0.0460 - val_loss: 3.8086 - val_fingerprint_loss: 1.8995 - val_key_hat_loss: 0.0095\n",
      "Epoch 41/50\n",
      "9000/9000 [==============================] - 47s 5ms/step - loss: 8.8725 - fingerprint_loss: 0.3577 - key_hat_loss: 0.0426 - val_loss: 6.2785 - val_fingerprint_loss: 2.1054 - val_key_hat_loss: 0.0209\n",
      "Epoch 42/50\n",
      "9000/9000 [==============================] - 48s 5ms/step - loss: 8.9605 - fingerprint_loss: 0.3250 - key_hat_loss: 0.0432 - val_loss: 3.1266 - val_fingerprint_loss: 2.2693 - val_key_hat_loss: 0.0043\n",
      "Epoch 43/50\n",
      "9000/9000 [==============================] - 49s 5ms/step - loss: 8.4915 - fingerprint_loss: 0.3135 - key_hat_loss: 0.0409 - val_loss: 4.2943 - val_fingerprint_loss: 2.1390 - val_key_hat_loss: 0.0108\n",
      "Epoch 44/50\n",
      "9000/9000 [==============================] - 49s 5ms/step - loss: 8.3123 - fingerprint_loss: 0.2854 - key_hat_loss: 0.0401 - val_loss: 3.2390 - val_fingerprint_loss: 2.1802 - val_key_hat_loss: 0.0053\n",
      "Epoch 45/50\n",
      "9000/9000 [==============================] - 49s 5ms/step - loss: 7.5886 - fingerprint_loss: 0.3313 - key_hat_loss: 0.0363 - val_loss: 2.8536 - val_fingerprint_loss: 1.9621 - val_key_hat_loss: 0.0045\n",
      "Epoch 46/50\n",
      "9000/9000 [==============================] - 47s 5ms/step - loss: 8.8100 - fingerprint_loss: 0.3344 - key_hat_loss: 0.0424 - val_loss: 3.5918 - val_fingerprint_loss: 1.8019 - val_key_hat_loss: 0.0089\n",
      "Epoch 47/50\n",
      "9000/9000 [==============================] - 46s 5ms/step - loss: 8.5292 - fingerprint_loss: 0.3557 - key_hat_loss: 0.0409 - val_loss: 2.5607 - val_fingerprint_loss: 2.0469 - val_key_hat_loss: 0.0026\n",
      "Epoch 48/50\n",
      "9000/9000 [==============================] - 46s 5ms/step - loss: 9.1828 - fingerprint_loss: 0.2904 - key_hat_loss: 0.0445 - val_loss: 2.8220 - val_fingerprint_loss: 2.1052 - val_key_hat_loss: 0.0036\n",
      "Epoch 49/50\n",
      "9000/9000 [==============================] - 46s 5ms/step - loss: 9.1871 - fingerprint_loss: 0.3251 - key_hat_loss: 0.0443 - val_loss: 2.4809 - val_fingerprint_loss: 1.9446 - val_key_hat_loss: 0.0027\n",
      "Epoch 50/50\n",
      "9000/9000 [==============================] - 46s 5ms/step - loss: 9.4392 - fingerprint_loss: 0.3265 - key_hat_loss: 0.0456 - val_loss: 2.5219 - val_fingerprint_loss: 1.8376 - val_key_hat_loss: 0.0034\n",
      "Time to Fit the Model 2245.255168914795\n",
      "Ext Rate:   0.974\n",
      "76.50762864216169\n",
      "1500 4000\n",
      "4000 95.41991751324593 1500 Max Delay:  497.87255859375\n",
      "67666 4444 Numbre of training and testing data\n",
      "100 ratio\n",
      "finished getting true data\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "54000/54000 [==============================] - 1063s 20ms/step - loss: 940.0254 - fingerprint_loss: 0.5500 - key_hat_loss: 4.6974 - val_loss: 918.0393 - val_fingerprint_loss: 1.0412 - val_key_hat_loss: 4.5850\n",
      "Epoch 2/50\n",
      "54000/54000 [==============================] - 336s 6ms/step - loss: 890.2158 - fingerprint_loss: 0.4255 - key_hat_loss: 4.4490 - val_loss: 843.6592 - val_fingerprint_loss: 0.5113 - val_key_hat_loss: 4.2157\n",
      "Epoch 3/50\n",
      "54000/54000 [==============================] - 334s 6ms/step - loss: 593.4494 - fingerprint_loss: 0.3959 - key_hat_loss: 2.9653 - val_loss: 399.7729 - val_fingerprint_loss: 1.1601 - val_key_hat_loss: 1.9931\n",
      "Epoch 4/50\n",
      "54000/54000 [==============================] - 336s 6ms/step - loss: 199.0920 - fingerprint_loss: 0.3540 - key_hat_loss: 0.9937 - val_loss: 67.0052 - val_fingerprint_loss: 1.2688 - val_key_hat_loss: 0.3287\n",
      "Epoch 5/50\n",
      "54000/54000 [==============================] - 336s 6ms/step - loss: 115.0199 - fingerprint_loss: 0.3189 - key_hat_loss: 0.5735 - val_loss: 49.0406 - val_fingerprint_loss: 1.3710 - val_key_hat_loss: 0.2383\n",
      "Epoch 6/50\n",
      "54000/54000 [==============================] - 337s 6ms/step - loss: 83.3377 - fingerprint_loss: 0.3134 - key_hat_loss: 0.4151 - val_loss: 55.8893 - val_fingerprint_loss: 1.4996 - val_key_hat_loss: 0.2719\n",
      "Epoch 7/50\n",
      "54000/54000 [==============================] - 336s 6ms/step - loss: 67.1493 - fingerprint_loss: 0.3153 - key_hat_loss: 0.3342 - val_loss: 26.3046 - val_fingerprint_loss: 1.5557 - val_key_hat_loss: 0.1237\n",
      "Epoch 8/50\n",
      "54000/54000 [==============================] - 337s 6ms/step - loss: 55.9393 - fingerprint_loss: 0.3038 - key_hat_loss: 0.2782 - val_loss: 35.2437 - val_fingerprint_loss: 2.0221 - val_key_hat_loss: 0.1661\n",
      "Epoch 9/50\n",
      "54000/54000 [==============================] - 339s 6ms/step - loss: 48.3111 - fingerprint_loss: 0.3060 - key_hat_loss: 0.2400 - val_loss: 14.1121 - val_fingerprint_loss: 1.5883 - val_key_hat_loss: 0.0626\n",
      "Epoch 10/50\n",
      "54000/54000 [==============================] - 335s 6ms/step - loss: 44.0344 - fingerprint_loss: 0.3126 - key_hat_loss: 0.2186 - val_loss: 17.5798 - val_fingerprint_loss: 1.7941 - val_key_hat_loss: 0.0789\n",
      "Epoch 11/50\n",
      "54000/54000 [==============================] - 336s 6ms/step - loss: 39.6400 - fingerprint_loss: 0.2995 - key_hat_loss: 0.1967 - val_loss: 18.6911 - val_fingerprint_loss: 1.6390 - val_key_hat_loss: 0.0853\n",
      "Epoch 12/50\n",
      "54000/54000 [==============================] - 335s 6ms/step - loss: 36.4941 - fingerprint_loss: 0.3030 - key_hat_loss: 0.1810 - val_loss: 15.6583 - val_fingerprint_loss: 1.4283 - val_key_hat_loss: 0.0711\n",
      "Epoch 13/50\n",
      "54000/54000 [==============================] - 335s 6ms/step - loss: 34.5126 - fingerprint_loss: 0.2997 - key_hat_loss: 0.1711 - val_loss: 13.5667 - val_fingerprint_loss: 1.8142 - val_key_hat_loss: 0.0588\n",
      "Epoch 14/50\n",
      "54000/54000 [==============================] - 335s 6ms/step - loss: 31.3091 - fingerprint_loss: 0.3075 - key_hat_loss: 0.1550 - val_loss: 29.6944 - val_fingerprint_loss: 1.5241 - val_key_hat_loss: 0.1409\n",
      "Epoch 15/50\n",
      "54000/54000 [==============================] - 335s 6ms/step - loss: 29.9615 - fingerprint_loss: 0.2989 - key_hat_loss: 0.1483 - val_loss: 12.1407 - val_fingerprint_loss: 1.5516 - val_key_hat_loss: 0.0529\n",
      "Epoch 16/50\n",
      "54000/54000 [==============================] - 335s 6ms/step - loss: 28.4106 - fingerprint_loss: 0.2972 - key_hat_loss: 0.1406 - val_loss: 15.3271 - val_fingerprint_loss: 1.8665 - val_key_hat_loss: 0.0673\n",
      "Epoch 17/50\n",
      "54000/54000 [==============================] - 338s 6ms/step - loss: 27.8149 - fingerprint_loss: 0.2807 - key_hat_loss: 0.1377 - val_loss: 10.6370 - val_fingerprint_loss: 1.9091 - val_key_hat_loss: 0.0436\n",
      "Epoch 18/50\n",
      "54000/54000 [==============================] - 346s 6ms/step - loss: 24.5802 - fingerprint_loss: 0.2864 - key_hat_loss: 0.1215 - val_loss: 9.6275 - val_fingerprint_loss: 1.6661 - val_key_hat_loss: 0.0398\n",
      "Epoch 19/50\n",
      "54000/54000 [==============================] - 360s 7ms/step - loss: 24.7704 - fingerprint_loss: 0.2933 - key_hat_loss: 0.1224 - val_loss: 9.2417 - val_fingerprint_loss: 2.0419 - val_key_hat_loss: 0.0360\n",
      "Epoch 20/50\n",
      "54000/54000 [==============================] - 334s 6ms/step - loss: 22.9716 - fingerprint_loss: 0.2760 - key_hat_loss: 0.1135 - val_loss: 11.4737 - val_fingerprint_loss: 1.9369 - val_key_hat_loss: 0.0477\n",
      "Epoch 21/50\n",
      "54000/54000 [==============================] - 332s 6ms/step - loss: 22.2077 - fingerprint_loss: 0.2781 - key_hat_loss: 0.1096 - val_loss: 9.4321 - val_fingerprint_loss: 1.8664 - val_key_hat_loss: 0.0378\n",
      "Epoch 22/50\n",
      "54000/54000 [==============================] - 332s 6ms/step - loss: 22.8203 - fingerprint_loss: 0.2843 - key_hat_loss: 0.1127 - val_loss: 9.4380 - val_fingerprint_loss: 1.6978 - val_key_hat_loss: 0.0387\n",
      "Epoch 23/50\n",
      "54000/54000 [==============================] - 332s 6ms/step - loss: 21.4879 - fingerprint_loss: 0.2827 - key_hat_loss: 0.1060 - val_loss: 9.3874 - val_fingerprint_loss: 1.8782 - val_key_hat_loss: 0.0375\n",
      "Epoch 24/50\n",
      "54000/54000 [==============================] - 337s 6ms/step - loss: 20.6765 - fingerprint_loss: 0.2816 - key_hat_loss: 0.1020 - val_loss: 7.8804 - val_fingerprint_loss: 1.8104 - val_key_hat_loss: 0.0303\n",
      "Epoch 25/50\n",
      "54000/54000 [==============================] - 337s 6ms/step - loss: 19.8389 - fingerprint_loss: 0.2850 - key_hat_loss: 0.0978 - val_loss: 8.3916 - val_fingerprint_loss: 1.9801 - val_key_hat_loss: 0.0321\n",
      "Epoch 26/50\n",
      "54000/54000 [==============================] - 340s 6ms/step - loss: 19.6832 - fingerprint_loss: 0.2873 - key_hat_loss: 0.0970 - val_loss: 9.7318 - val_fingerprint_loss: 2.1464 - val_key_hat_loss: 0.0379\n",
      "Epoch 27/50\n",
      "54000/54000 [==============================] - 341s 6ms/step - loss: 17.7679 - fingerprint_loss: 0.2778 - key_hat_loss: 0.0875 - val_loss: 8.1476 - val_fingerprint_loss: 2.2641 - val_key_hat_loss: 0.0294\n",
      "Epoch 28/50\n",
      "54000/54000 [==============================] - 347s 6ms/step - loss: 18.3445 - fingerprint_loss: 0.2720 - key_hat_loss: 0.0904 - val_loss: 8.5794 - val_fingerprint_loss: 1.8558 - val_key_hat_loss: 0.0336\n",
      "Epoch 29/50\n",
      "54000/54000 [==============================] - 341s 6ms/step - loss: 17.8740 - fingerprint_loss: 0.2651 - key_hat_loss: 0.0880 - val_loss: 9.0981 - val_fingerprint_loss: 1.9765 - val_key_hat_loss: 0.0356\n",
      "Epoch 30/50\n",
      "54000/54000 [==============================] - 344s 6ms/step - loss: 17.9253 - fingerprint_loss: 0.2734 - key_hat_loss: 0.0883 - val_loss: 8.8044 - val_fingerprint_loss: 1.8384 - val_key_hat_loss: 0.0348\n",
      "Epoch 31/50\n",
      "54000/54000 [==============================] - 346s 6ms/step - loss: 17.3214 - fingerprint_loss: 0.2734 - key_hat_loss: 0.0852 - val_loss: 7.2574 - val_fingerprint_loss: 1.8820 - val_key_hat_loss: 0.0269\n",
      "Epoch 32/50\n",
      "54000/54000 [==============================] - 340s 6ms/step - loss: 17.4564 - fingerprint_loss: 0.2658 - key_hat_loss: 0.0860 - val_loss: 8.1478 - val_fingerprint_loss: 1.7730 - val_key_hat_loss: 0.0319\n",
      "Epoch 33/50\n",
      "54000/54000 [==============================] - 333s 6ms/step - loss: 16.1467 - fingerprint_loss: 0.2637 - key_hat_loss: 0.0794 - val_loss: 7.7603 - val_fingerprint_loss: 2.0815 - val_key_hat_loss: 0.0284\n",
      "Epoch 34/50\n",
      "54000/54000 [==============================] - 338s 6ms/step - loss: 16.1064 - fingerprint_loss: 0.2753 - key_hat_loss: 0.0792 - val_loss: 8.1468 - val_fingerprint_loss: 2.1830 - val_key_hat_loss: 0.0298\n",
      "Epoch 35/50\n",
      "54000/54000 [==============================] - 343s 6ms/step - loss: 15.7455 - fingerprint_loss: 0.2775 - key_hat_loss: 0.0773 - val_loss: 7.6886 - val_fingerprint_loss: 1.7891 - val_key_hat_loss: 0.0295\n",
      "Epoch 36/50\n",
      "54000/54000 [==============================] - 343s 6ms/step - loss: 14.9558 - fingerprint_loss: 0.2760 - key_hat_loss: 0.0734 - val_loss: 8.2309 - val_fingerprint_loss: 1.9913 - val_key_hat_loss: 0.0312\n",
      "Epoch 37/50\n",
      "54000/54000 [==============================] - 335s 6ms/step - loss: 15.4850 - fingerprint_loss: 0.2656 - key_hat_loss: 0.0761 - val_loss: 8.7244 - val_fingerprint_loss: 2.0033 - val_key_hat_loss: 0.0336\n",
      "Epoch 38/50\n",
      "54000/54000 [==============================] - 344s 6ms/step - loss: 15.6849 - fingerprint_loss: 0.2591 - key_hat_loss: 0.0771 - val_loss: 8.9944 - val_fingerprint_loss: 2.0634 - val_key_hat_loss: 0.0347\n",
      "Epoch 39/50\n",
      "54000/54000 [==============================] - 340s 6ms/step - loss: 15.4454 - fingerprint_loss: 0.2828 - key_hat_loss: 0.0758 - val_loss: 9.0585 - val_fingerprint_loss: 1.6069 - val_key_hat_loss: 0.0373\n",
      "Epoch 40/50\n",
      "54000/54000 [==============================] - 342s 6ms/step - loss: 14.0009 - fingerprint_loss: 0.2743 - key_hat_loss: 0.0686 - val_loss: 7.4396 - val_fingerprint_loss: 1.9675 - val_key_hat_loss: 0.0274\n",
      "Epoch 41/50\n",
      "54000/54000 [==============================] - 343s 6ms/step - loss: 14.0196 - fingerprint_loss: 0.2662 - key_hat_loss: 0.0688 - val_loss: 6.8189 - val_fingerprint_loss: 1.8293 - val_key_hat_loss: 0.0249\n",
      "Epoch 42/50\n",
      "54000/54000 [==============================] - 342s 6ms/step - loss: 14.3657 - fingerprint_loss: 0.2877 - key_hat_loss: 0.0704 - val_loss: 8.3080 - val_fingerprint_loss: 1.8940 - val_key_hat_loss: 0.0321\n",
      "Epoch 43/50\n",
      "54000/54000 [==============================] - 343s 6ms/step - loss: 14.5011 - fingerprint_loss: 0.2636 - key_hat_loss: 0.0712 - val_loss: 8.2244 - val_fingerprint_loss: 1.7652 - val_key_hat_loss: 0.0323\n",
      "Epoch 44/50\n",
      "54000/54000 [==============================] - 341s 6ms/step - loss: 14.1396 - fingerprint_loss: 0.2745 - key_hat_loss: 0.0693 - val_loss: 12.3083 - val_fingerprint_loss: 1.7000 - val_key_hat_loss: 0.0530\n",
      "Epoch 45/50\n",
      "54000/54000 [==============================] - 346s 6ms/step - loss: 13.8613 - fingerprint_loss: 0.2630 - key_hat_loss: 0.0680 - val_loss: 8.2522 - val_fingerprint_loss: 1.9956 - val_key_hat_loss: 0.0313\n",
      "Epoch 46/50\n",
      "54000/54000 [==============================] - 348s 6ms/step - loss: 13.7353 - fingerprint_loss: 0.2775 - key_hat_loss: 0.0673 - val_loss: 8.0695 - val_fingerprint_loss: 1.8632 - val_key_hat_loss: 0.0310\n",
      "Epoch 47/50\n",
      "54000/54000 [==============================] - 346s 6ms/step - loss: 13.0597 - fingerprint_loss: 0.2611 - key_hat_loss: 0.0640 - val_loss: 8.4134 - val_fingerprint_loss: 1.8854 - val_key_hat_loss: 0.0326\n",
      "Epoch 48/50\n",
      "54000/54000 [==============================] - 342s 6ms/step - loss: 13.6147 - fingerprint_loss: 0.2730 - key_hat_loss: 0.0667 - val_loss: 7.4727 - val_fingerprint_loss: 1.9891 - val_key_hat_loss: 0.0274\n",
      "Epoch 49/50\n",
      "54000/54000 [==============================] - 339s 6ms/step - loss: 12.9663 - fingerprint_loss: 0.2756 - key_hat_loss: 0.0635 - val_loss: 7.1987 - val_fingerprint_loss: 2.0010 - val_key_hat_loss: 0.0260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50\n",
      "54000/54000 [==============================] - 340s 6ms/step - loss: 13.2627 - fingerprint_loss: 0.2737 - key_hat_loss: 0.0649 - val_loss: 7.4658 - val_fingerprint_loss: 1.8670 - val_key_hat_loss: 0.0280\n",
      "Time to Fit the Model 18055.78747653961\n",
      "Ext Rate:   0.99225\n",
      "90.50788768331209\n",
      "1800 4000\n",
      "4000 84.35799002459503 1800 Max Delay:  371.3043042421341\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "9000/9000 [==============================] - 56s 6ms/step - loss: 14.3306 - fingerprint_loss: 0.2273 - key_hat_loss: 0.0705 - val_loss: 1.8057 - val_fingerprint_loss: 1.7722 - val_key_hat_loss: 1.6755e-04\n",
      "Epoch 2/50\n",
      "9000/9000 [==============================] - 54s 6ms/step - loss: 10.9617 - fingerprint_loss: 0.2458 - key_hat_loss: 0.0536 - val_loss: 1.5877 - val_fingerprint_loss: 1.5775 - val_key_hat_loss: 5.0716e-05\n",
      "Epoch 3/50\n",
      "9000/9000 [==============================] - 54s 6ms/step - loss: 9.3417 - fingerprint_loss: 0.2633 - key_hat_loss: 0.0454 - val_loss: 1.7548 - val_fingerprint_loss: 1.7462 - val_key_hat_loss: 4.2745e-05\n",
      "Epoch 4/50\n",
      "9000/9000 [==============================] - 54s 6ms/step - loss: 9.3998 - fingerprint_loss: 0.2855 - key_hat_loss: 0.0456 - val_loss: 1.8489 - val_fingerprint_loss: 1.8275 - val_key_hat_loss: 1.0695e-04\n",
      "Epoch 5/50\n",
      "9000/9000 [==============================] - 54s 6ms/step - loss: 10.5573 - fingerprint_loss: 0.2641 - key_hat_loss: 0.0515 - val_loss: 1.6438 - val_fingerprint_loss: 1.6244 - val_key_hat_loss: 9.7149e-05\n",
      "Epoch 6/50\n",
      "9000/9000 [==============================] - 54s 6ms/step - loss: 9.2957 - fingerprint_loss: 0.2702 - key_hat_loss: 0.0451 - val_loss: 1.5723 - val_fingerprint_loss: 1.5672 - val_key_hat_loss: 2.5317e-05\n",
      "Epoch 7/50\n",
      "9000/9000 [==============================] - 54s 6ms/step - loss: 9.6021 - fingerprint_loss: 0.2338 - key_hat_loss: 0.0468 - val_loss: 1.9560 - val_fingerprint_loss: 1.9440 - val_key_hat_loss: 6.0014e-05\n",
      "Epoch 8/50\n",
      "9000/9000 [==============================] - 54s 6ms/step - loss: 10.4131 - fingerprint_loss: 0.2677 - key_hat_loss: 0.0507 - val_loss: 1.8978 - val_fingerprint_loss: 1.8907 - val_key_hat_loss: 3.5413e-05\n",
      "Epoch 9/50\n",
      "9000/9000 [==============================] - 55s 6ms/step - loss: 8.9055 - fingerprint_loss: 0.2458 - key_hat_loss: 0.0433 - val_loss: 1.6761 - val_fingerprint_loss: 1.6616 - val_key_hat_loss: 7.2336e-05\n",
      "Epoch 10/50\n",
      "9000/9000 [==============================] - 56s 6ms/step - loss: 8.7519 - fingerprint_loss: 0.2507 - key_hat_loss: 0.0425 - val_loss: 1.8218 - val_fingerprint_loss: 1.7837 - val_key_hat_loss: 1.9050e-04\n",
      "Epoch 11/50\n",
      "9000/9000 [==============================] - 56s 6ms/step - loss: 8.2248 - fingerprint_loss: 0.2877 - key_hat_loss: 0.0397 - val_loss: 1.9291 - val_fingerprint_loss: 1.8922 - val_key_hat_loss: 1.8452e-04\n",
      "Epoch 12/50\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 7.4944 - fingerprint_loss: 0.2772 - key_hat_loss: 0.0361 - val_loss: 2.0395 - val_fingerprint_loss: 2.0054 - val_key_hat_loss: 1.7065e-04\n",
      "Epoch 13/50\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 8.6002 - fingerprint_loss: 0.3094 - key_hat_loss: 0.0415 - val_loss: 1.7777 - val_fingerprint_loss: 1.7710 - val_key_hat_loss: 3.3596e-05\n",
      "Epoch 14/50\n",
      "9000/9000 [==============================] - 61s 7ms/step - loss: 8.3035 - fingerprint_loss: 0.2755 - key_hat_loss: 0.0401 - val_loss: 1.7387 - val_fingerprint_loss: 1.7295 - val_key_hat_loss: 4.5925e-05\n",
      "Epoch 15/50\n",
      "9000/9000 [==============================] - 61s 7ms/step - loss: 7.6604 - fingerprint_loss: 0.2422 - key_hat_loss: 0.0371 - val_loss: 1.8719 - val_fingerprint_loss: 1.8530 - val_key_hat_loss: 9.4506e-05\n",
      "Epoch 16/50\n",
      "9000/9000 [==============================] - 55s 6ms/step - loss: 8.9182 - fingerprint_loss: 0.2393 - key_hat_loss: 0.0434 - val_loss: 1.8172 - val_fingerprint_loss: 1.7878 - val_key_hat_loss: 1.4733e-04\n",
      "Epoch 17/50\n",
      "9000/9000 [==============================] - 54s 6ms/step - loss: 7.4034 - fingerprint_loss: 0.2865 - key_hat_loss: 0.0356 - val_loss: 1.7539 - val_fingerprint_loss: 1.7405 - val_key_hat_loss: 6.7098e-05\n",
      "Epoch 18/50\n",
      "9000/9000 [==============================] - 54s 6ms/step - loss: 7.0022 - fingerprint_loss: 0.2849 - key_hat_loss: 0.0336 - val_loss: 1.5502 - val_fingerprint_loss: 1.5313 - val_key_hat_loss: 9.4482e-05\n",
      "Epoch 19/50\n",
      "9000/9000 [==============================] - 54s 6ms/step - loss: 8.3448 - fingerprint_loss: 0.2908 - key_hat_loss: 0.0403 - val_loss: 1.9235 - val_fingerprint_loss: 1.9178 - val_key_hat_loss: 2.8560e-05\n",
      "Epoch 20/50\n",
      "9000/9000 [==============================] - 54s 6ms/step - loss: 7.3488 - fingerprint_loss: 0.2564 - key_hat_loss: 0.0355 - val_loss: 1.6989 - val_fingerprint_loss: 1.6799 - val_key_hat_loss: 9.4984e-05\n",
      "Epoch 21/50\n",
      "9000/9000 [==============================] - 54s 6ms/step - loss: 6.4103 - fingerprint_loss: 0.2389 - key_hat_loss: 0.0309 - val_loss: 1.6977 - val_fingerprint_loss: 1.6813 - val_key_hat_loss: 8.1826e-05\n",
      "Epoch 22/50\n",
      "9000/9000 [==============================] - 54s 6ms/step - loss: 7.2459 - fingerprint_loss: 0.2642 - key_hat_loss: 0.0349 - val_loss: 1.8468 - val_fingerprint_loss: 1.7703 - val_key_hat_loss: 3.8246e-04\n",
      "Epoch 23/50\n",
      "9000/9000 [==============================] - 54s 6ms/step - loss: 7.3466 - fingerprint_loss: 0.2582 - key_hat_loss: 0.0354 - val_loss: 1.8461 - val_fingerprint_loss: 1.8320 - val_key_hat_loss: 7.0549e-05\n",
      "Epoch 24/50\n",
      "9000/9000 [==============================] - 56s 6ms/step - loss: 7.0373 - fingerprint_loss: 0.2499 - key_hat_loss: 0.0339 - val_loss: 1.9094 - val_fingerprint_loss: 1.8995 - val_key_hat_loss: 4.9793e-05\n",
      "Epoch 25/50\n",
      "9000/9000 [==============================] - 56s 6ms/step - loss: 7.1949 - fingerprint_loss: 0.2643 - key_hat_loss: 0.0347 - val_loss: 1.8645 - val_fingerprint_loss: 1.8472 - val_key_hat_loss: 8.6396e-05\n",
      "Epoch 26/50\n",
      "9000/9000 [==============================] - 54s 6ms/step - loss: 6.6871 - fingerprint_loss: 0.2691 - key_hat_loss: 0.0321 - val_loss: 1.6758 - val_fingerprint_loss: 1.6522 - val_key_hat_loss: 1.1785e-04\n",
      "Epoch 27/50\n",
      "9000/9000 [==============================] - 55s 6ms/step - loss: 6.4821 - fingerprint_loss: 0.2802 - key_hat_loss: 0.0310 - val_loss: 1.4856 - val_fingerprint_loss: 1.4790 - val_key_hat_loss: 3.3244e-05\n",
      "Epoch 28/50\n",
      "9000/9000 [==============================] - 56s 6ms/step - loss: 7.1828 - fingerprint_loss: 0.2748 - key_hat_loss: 0.0345 - val_loss: 1.6905 - val_fingerprint_loss: 1.6698 - val_key_hat_loss: 1.0328e-04\n",
      "Epoch 29/50\n",
      "9000/9000 [==============================] - 56s 6ms/step - loss: 6.0423 - fingerprint_loss: 0.2836 - key_hat_loss: 0.0288 - val_loss: 1.9176 - val_fingerprint_loss: 1.8952 - val_key_hat_loss: 1.1228e-04\n",
      "Epoch 30/50\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 6.6310 - fingerprint_loss: 0.2454 - key_hat_loss: 0.0319 - val_loss: 1.7695 - val_fingerprint_loss: 1.7175 - val_key_hat_loss: 2.6032e-04\n",
      "Epoch 31/50\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 6.1253 - fingerprint_loss: 0.2600 - key_hat_loss: 0.0293 - val_loss: 1.7103 - val_fingerprint_loss: 1.6842 - val_key_hat_loss: 1.3055e-04\n",
      "Epoch 32/50\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 6.3976 - fingerprint_loss: 0.2821 - key_hat_loss: 0.0306 - val_loss: 1.4958 - val_fingerprint_loss: 1.4579 - val_key_hat_loss: 1.8946e-04\n",
      "Epoch 33/50\n",
      "9000/9000 [==============================] - 62s 7ms/step - loss: 7.0450 - fingerprint_loss: 0.2967 - key_hat_loss: 0.0337 - val_loss: 1.9759 - val_fingerprint_loss: 1.9493 - val_key_hat_loss: 1.3297e-04\n",
      "Epoch 34/50\n",
      "9000/9000 [==============================] - 61s 7ms/step - loss: 6.2480 - fingerprint_loss: 0.2730 - key_hat_loss: 0.0299 - val_loss: 1.8819 - val_fingerprint_loss: 1.8718 - val_key_hat_loss: 5.0544e-05\n",
      "Epoch 35/50\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 5.6690 - fingerprint_loss: 0.2374 - key_hat_loss: 0.0272 - val_loss: 1.5661 - val_fingerprint_loss: 1.5374 - val_key_hat_loss: 1.4319e-04\n",
      "Epoch 36/50\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 5.7595 - fingerprint_loss: 0.2703 - key_hat_loss: 0.0274 - val_loss: 1.6827 - val_fingerprint_loss: 1.6398 - val_key_hat_loss: 2.1434e-04\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 60s 7ms/step - loss: 6.7460 - fingerprint_loss: 0.2804 - key_hat_loss: 0.0323 - val_loss: 2.1369 - val_fingerprint_loss: 2.1080 - val_key_hat_loss: 1.4490e-04\n",
      "Epoch 38/50\n",
      "9000/9000 [==============================] - 62s 7ms/step - loss: 5.9046 - fingerprint_loss: 0.2718 - key_hat_loss: 0.0282 - val_loss: 2.0500 - val_fingerprint_loss: 1.9125 - val_key_hat_loss: 6.8789e-04\n",
      "Epoch 39/50\n",
      "9000/9000 [==============================] - 62s 7ms/step - loss: 6.2920 - fingerprint_loss: 0.2684 - key_hat_loss: 0.0301 - val_loss: 2.0410 - val_fingerprint_loss: 2.0018 - val_key_hat_loss: 1.9621e-04\n",
      "Epoch 40/50\n",
      "9000/9000 [==============================] - 62s 7ms/step - loss: 6.0196 - fingerprint_loss: 0.2531 - key_hat_loss: 0.0288 - val_loss: 1.7909 - val_fingerprint_loss: 1.7572 - val_key_hat_loss: 1.6834e-04\n",
      "Epoch 41/50\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 6.0426 - fingerprint_loss: 0.2643 - key_hat_loss: 0.0289 - val_loss: 1.6414 - val_fingerprint_loss: 1.6200 - val_key_hat_loss: 1.0701e-04\n",
      "Epoch 42/50\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 5.4162 - fingerprint_loss: 0.2581 - key_hat_loss: 0.0258 - val_loss: 1.8480 - val_fingerprint_loss: 1.8358 - val_key_hat_loss: 6.1012e-05\n",
      "Epoch 43/50\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 5.6609 - fingerprint_loss: 0.3114 - key_hat_loss: 0.0267 - val_loss: 1.7556 - val_fingerprint_loss: 1.7434 - val_key_hat_loss: 6.0638e-05\n",
      "Epoch 44/50\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 5.6360 - fingerprint_loss: 0.2787 - key_hat_loss: 0.0268 - val_loss: 1.4249 - val_fingerprint_loss: 1.4152 - val_key_hat_loss: 4.8491e-05\n",
      "Epoch 45/50\n",
      "9000/9000 [==============================] - 61s 7ms/step - loss: 5.8848 - fingerprint_loss: 0.2491 - key_hat_loss: 0.0282 - val_loss: 1.7762 - val_fingerprint_loss: 1.7494 - val_key_hat_loss: 1.3447e-04\n",
      "Epoch 46/50\n",
      "9000/9000 [==============================] - 61s 7ms/step - loss: 5.9556 - fingerprint_loss: 0.2827 - key_hat_loss: 0.0284 - val_loss: 1.5723 - val_fingerprint_loss: 1.5256 - val_key_hat_loss: 2.3332e-04\n",
      "Epoch 47/50\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 4.8119 - fingerprint_loss: 0.2532 - key_hat_loss: 0.0228 - val_loss: 1.8268 - val_fingerprint_loss: 1.7256 - val_key_hat_loss: 5.0633e-04\n",
      "Epoch 48/50\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 5.0991 - fingerprint_loss: 0.2372 - key_hat_loss: 0.0243 - val_loss: 1.6530 - val_fingerprint_loss: 1.5630 - val_key_hat_loss: 4.5006e-04\n",
      "Epoch 49/50\n",
      "9000/9000 [==============================] - 61s 7ms/step - loss: 5.6262 - fingerprint_loss: 0.2617 - key_hat_loss: 0.0268 - val_loss: 1.3894 - val_fingerprint_loss: 1.3220 - val_key_hat_loss: 3.3694e-04\n",
      "Epoch 50/50\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 6.2392 - fingerprint_loss: 0.2618 - key_hat_loss: 0.0299 - val_loss: 1.7766 - val_fingerprint_loss: 1.5060 - val_key_hat_loss: 0.0014\n",
      "Time to Fit the Model 2878.2609119415283\n",
      "Ext Rate:   0.9875\n",
      "55.358419994976785\n",
      "1800 4000\n",
      "4000 83.80761962356794 1800 Max Delay:  366.6225972175598\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n    size = 1800\\n    Ext Rate:   0.992\\n    4000 95.84566262555747 1800 Max Delay:  415.03377401828766\\n    Ext Rate:   0.9765\\n    4000 93.27346397158448 1800 Max Delay:  380.78301668167114\\n    \\n    \\n    size 1200\\n    \\n    Ext Rate:   0.948\\n    4000 72.26921833757295 1200 Max Delay:  357.55537247657776\\n\\n    Ext Rate:   0.943\\n    4000 72.2022527100891 1200 Max Delay:  356.89986884593964\\n    \\n    size = 600\\n    Ext Rate: 0.788\\n    4000 38.018    max delay: 235.928\\n    \\n    size 600, and 10000 training:\\n    Ext Rate:   0.731\\n    4000 37.918288900979874 600 Max Delay:  233.056\\n    \\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras import optimizers\n",
    "n_false_train = 0\n",
    "x_fing_w, key_hat_w, epoch, batch = 1, 200, 50, 64\n",
    "\n",
    "beg_time = time.time()\n",
    "key_length = 100\n",
    "key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "sample_sizes = [1500, 1800]#, 600, 1200]\n",
    "#models = []\n",
    "trains = [60000, 10000]\n",
    "for sam in sample_sizes:\n",
    "    sample_size = sam\n",
    "    X_train_all = create_sample_size_dataset(all_ipds_for_train, sample_size = sample_size)\n",
    "    X_test_all = create_sample_size_dataset(all_ipds_for_test, sample_size = sample_size)\n",
    "    print(len(X_train_all),len(X_test_all), \"Numbre of training and testing data\")\n",
    "    model = get_encoder_decoder_conv_dense_slice(sample_size=sample_size, key_length=key_length, chunk=10)\n",
    "    ad = optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0, amsgrad=False)\n",
    "\n",
    "    model.compile(optimizer=ad, loss={'fingerprint':mean_pred_loss, 'key_hat': losses.categorical_crossentropy},\n",
    "                                    loss_weights={'fingerprint': x_fing_w, 'key_hat': key_hat_w})\n",
    "    X_train, y_train, train_keys = get_false_true_data(X_train_all, key_options)#get_only_true_data\n",
    "    train_keys = np.array(train_keys)\n",
    "    n_test = 4000\n",
    "    for t in trains:\n",
    "        # model.summary()\n",
    "        beg_time = time.time()\n",
    "        array_mult_train, array_sub_train, noise_for_train = get_arrays_mult_noise_sub(t,max_delay=5,chunk=10,std=1,sample_size=sample_size)\n",
    "        model.fit([X_train[0:t], train_keys[0:t], array_mult_train[0:t], array_sub_train[0:t], \n",
    "                   noise_for_train[0:t]], [y_train[0:t], train_keys[0:t]], epochs=epoch,\n",
    "                  validation_split=0.1, batch_size=batch,verbose =1)#, validation_split=0.1,callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "        print(\"Time to Fit the Model\", time.time() - beg_time)\n",
    "        models.append(model) \n",
    "        array_mult_test, array_sub_test, noise_for_test = get_arrays_mult_noise_sub(n_test,max_delay=5,chunk=10,std=1,sample_size=sample_size)\n",
    "\n",
    "        #### This is when we test encoder and decoder together using the same model: model_encoder_decoder\n",
    "        x_test = np.array(X_test_all[0:n_test]).reshape((-1, sample_size, 1))\n",
    "        key_options = selecting_valid_fingerprints(key_length = key_length)# we use 100 keys.\n",
    "        test_keys = np.array(get_keys_for_fingerprinting_data(size=n_test, key_options=key_options))\n",
    "        noise_for_test = np.squeeze(noise_for_test[0:n_test])\n",
    "        pred = model.predict([x_test, test_keys, array_mult_test, array_sub_test, noise_for_test])\n",
    "\n",
    "        fingerprint_x2, keys_true = pred[0],  pred[1]\n",
    "        ext_rate = compute_extract_rate(keys_true, true_keys = test_keys)\n",
    "\n",
    "        print(\"Ext Rate:  \", ext_rate)\n",
    "        compute_delay_on_each_packet(fingerprint_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "Ext Rate:   0.8862\n",
      "50.484288698991136\n",
      "1500 5000\n",
      "5000 77.23775366936363 1500 Max Delay:  401.2521390914917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nsample size = 1800, n_train = 10000, and key = 100:\\n1800\\nExt Rate:   0.0842\\n52.90532826509741\\n1800 4444\\n4444 47.894631279740935 1800 Max Delay:  182.74153697490692\\nwhen we only change n_train = 60000:\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_test = 5000\n",
    "print(sample_size)\n",
    "array_mult_test, array_sub_test, noise_for_test = get_arrays_mult_noise_sub(n_test,max_delay=5,chunk=10,std=1,sample_size=sample_size)\n",
    "\n",
    "x_test = np.array(X_test_all[0:n_test]).reshape((-1, sample_size, 1))\n",
    "key_options = selecting_valid_fingerprints(key_length = 200)# we use 100 keys.\n",
    "test_keys = np.array(get_keys_for_fingerprinting_data(size=n_test, key_options=key_options))\n",
    "noise_for_test = np.squeeze(noise_for_test[0:n_test])\n",
    "pred = model.predict([x_test, test_keys, array_mult_test, array_sub_test, noise_for_test])\n",
    "\n",
    "fingerprint_x2, keys_true = pred[0],  pred[1]\n",
    "ext_rate = compute_extract_rate(keys_true, true_keys = test_keys)\n",
    "\n",
    "print(\"Ext Rate:  \", ext_rate)\n",
    "compute_delay_on_each_packet(fingerprint_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decide_if_fingerprinted(keys, threshold):\n",
    "    fing = 0\n",
    "    for key in keys:\n",
    "        index = np.argmax(key)\n",
    "        if key[index] > threshold and index > 0:\n",
    "            fing += 1\n",
    "    return fing / float(len(keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading encoder takes too much time (hours), so we just use the model_encoder_decoder for encoding.\n",
    "model_decoder = load_decoder(key_length, sample_size)\n",
    "decoder_weights = []\n",
    "j = 0\n",
    "for i in range(0, 24):\n",
    "    if 'dec' in model.layers[-(24 - i)].name or 'key_hat' in model.layers[-(24 - i)].name:\n",
    "        model_decoder.layers[j].set_weights(model.layers[-(24 - i)].get_weights())\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.293 0.9925\n",
      "0.1925 0.98\n",
      "0.115 0.9685\n",
      "0.0655 0.941\n",
      "0.022 0.905\n",
      "0.007 0.8525\n",
      "0.0 0.734\n",
      "0.9695 Extraction Rate\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nkey = 100 training with 1/100 number of false data. sample size = 1800, number of training=20000\\n0.2395 0.9845\\n0.156 0.9695\\n0.093 0.9535\\n0.043 0.931\\n0.014 0.8975\\n0.0055 0.867\\n0.0005 0.766\\n0.9685 Extraction Rate\\nExt Rate:   0.969\\n1800 2000\\n2000 138.32507355565957 1800 Max Delay:  556.0820367336273\\n##############################################################################################\\n\\n\\nsample size 3300, training data = 45000, key = 1000\\n\\n0.327 0.9895\\n0.1985 0.9705\\n0.122 0.9525\\n0.0635 0.933\\n0.025 0.9\\n0.0095 0.861\\n0.0015 0.7415\\n0.9675 Extraction Rate\\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_for_test = noise_for_test.reshape((-1, sample_size, 1))\n",
    "\n",
    "output_fin = noise_for_test[0:n_test] + x_test\n",
    "keys_true_fp = model_decoder.predict([output_fin])\n",
    "\n",
    "###### True positve:\n",
    "output_fin = noise_for_test[0:n_test] + x_test + fingerprint_x2\n",
    "\n",
    "keys_true_tp = model_decoder.predict([output_fin])\n",
    "ext_rate = compute_extract_rate(keys_true_tp, test_keys)\n",
    "thresholds = [0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]\n",
    "\n",
    "\n",
    "for t in thresholds:\n",
    "    fp = decide_if_fingerprinted(keys_true_fp, t)\n",
    "    tp = decide_if_fingerprinted(keys_true_tp, t)\n",
    "    \n",
    "    print(fp, tp)\n",
    "print(ext_rate, 'Extraction Rate')\n",
    "'''\n",
    "\n",
    "key = 100 training with 1/100 number of false data. sample size = 1800, number of training=20000\n",
    "0.2395 0.9845\n",
    "0.156 0.9695\n",
    "0.093 0.9535\n",
    "0.043 0.931\n",
    "0.014 0.8975\n",
    "0.0055 0.867\n",
    "0.0005 0.766\n",
    "0.9685 Extraction Rate\n",
    "Ext Rate:   0.969\n",
    "1800 2000\n",
    "2000 138.32507355565957 1800 Max Delay:  556.0820367336273\n",
    "##############################################################################################\n",
    "\n",
    "\n",
    "sample size 3300, training data = 48000, key = 1000\n",
    "\n",
    "0.293 0.9925\n",
    "0.1925 0.98\n",
    "0.115 0.9685\n",
    "0.0655 0.941\n",
    "0.022 0.905\n",
    "0.007 0.8525\n",
    "0.0 0.734\n",
    "0.9695 Extraction Rate\n",
    "2000 80.0513701567251 3300 Max Delay:  369.6583148241043\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_false_train = 0\n",
    "x_fing_w, key_hat_w, epoch, batch = 1, 200, 100, 64\n",
    "model_name = str(sample_size) + \"_\" + str(key_length) + \"_\" + str(\n",
    "    n_true_train) + \"_\" + str(n_false_train) + \"_\" + str(epoch) + \"_\" + str(x_fing_w) + \"_\" + str(key_hat_w)\n",
    "\n",
    "beg_time = time.time()\n",
    "#models_key_length = []\n",
    "keys = [100]\n",
    "n_true_train = 30000\n",
    "for k in keys:\n",
    "\n",
    "    key_options = selecting_valid_fingerprints(key_length = k)# we use 100 keys.\n",
    "    X_train, y_train, train_keys = get_false_true_training(X_train_all[0:n_true_train], key_options)\n",
    "    train_keys = np.array(train_keys)\n",
    "    print(\"Finished radinf\")\n",
    "\n",
    "    model= get_encoder_decoder_conv_dense_slice(sample_size=sample_size, key_length=k)\n",
    "    #losses.mean_squared_error\n",
    "    ad = optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0, amsgrad=False)\n",
    "\n",
    "    model.compile(optimizer=ad, loss={'fingerprint':mean_pred_loss, 'key_hat': losses.categorical_crossentropy},\n",
    "                                    loss_weights={'fingerprint': x_fing_w, 'key_hat': key_hat_w})\n",
    "\n",
    "\n",
    "    # model.summary()\n",
    "    print(\"Model %s is Built and Compiled in %f\" % (model_name ,time.time() - beg_time))\n",
    "    beg_time = time.time()\n",
    "\n",
    "    model.fit([X_train, train_keys, array_mult_train[0:n_true_train], array_sub_train[0:n_true_train], noise_for_train[0:n_true_train]], [y_train, train_keys], epochs=epoch, validation_split=0.1, batch_size=batch)#, validation_split=0.1,callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "    print(\"Time to Fit the Model\", time.time() - beg_time)\n",
    "\n",
    "    \n",
    "    models_key_length.append(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_for_results = '/home/fatemeh/Dropbox/Fingerprint/Results/'\n",
    "def compute_ROC_data(n_train):\n",
    "    target_name = open(path_for_results + str(n_train)+\"_\" + str(sample_size)+\"_\"+str(key_length) + '.txt', 'w')\n",
    "    sample_size = 900\n",
    "    key_length = 100\n",
    "    n_test = 5000\n",
    "    thresholds = [0.6, 0.7, 0.8, 0.9]\n",
    "    X = create_sample_size_dataset(all_ipds, sample_size = sample_size)\n",
    "    key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    model_decoder, model_encoder = load_model_for_testing(key_length, sample_size, n_train)\n",
    "    X_test = np.expand_dims(X[n_train:n_train + n_test], axis=1)\n",
    "    test_keys = np.expand_dims(get_fingerprint_for_data(size = n_test, key_options = key_options), axis=1)\n",
    "    fingerprint_x = model_encoder.predict([test_keys])\n",
    "    false_poses, true_poses = [], []\n",
    "    \n",
    "     ########## True positve:\n",
    "    output_fin = add_gussian_noise_to_ipds_fingerprinted(fingerprint = fingerprint_x, x_test = X_test, std =10)\n",
    "    keys_true = model_decoder.predict([output_fin])\n",
    "\n",
    "    ########## False positve: \n",
    "    output_non = add_gussian_noise_to_ipds_non_fingerprinted(X_test, std =10)\n",
    "    keys_false = model_decoder.predict([output_non])\n",
    "    for t in thresholds:\n",
    "        true_pos = decide_if_fingerprinted(keys_true, threshold = t)\n",
    "        false_pos = decide_if_fingerprinted(keys_false, threshold = t)\n",
    "        false_poses.append(false_pos)\n",
    "        true_poses.append(true_pos)\n",
    "        \n",
    "    write_array_to_file(array = false_poses, target =target_name, delimiter =' ')\n",
    "    write_array_to_file(array = true_poses, target =target_name, delimiter =' ')\n",
    "    target_name.close()\n",
    "    \n",
    "compute_ROC_data(n_train=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_impact_of_jitter():\n",
    "    sample_size = 600\n",
    "    key_length = 10\n",
    "    n_train = 50000\n",
    "    n_test = 5000\n",
    "    X = create_sample_size_dataset(all_ipds, sample_size = sample_size)\n",
    "    key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "\n",
    "    target_name = open(path_for_results + str(n_train)+\"_\" + str(sample_size)+\"_\"+str(key_length) + '.txt', 'w')\n",
    "\n",
    "    jitters = [1, 10, 50, 100]\n",
    "   \n",
    "    model_decoder, model_encoder = load_model_for_testing(key_length, sample_size, n_train)\n",
    "    X_test = np.expand_dims(X[n_train:n_train + n_test], axis=1)\n",
    "    test_keys = np.expand_dims(get_fingerprint_for_data(size = n_test, key_options = key_options), axis=1)\n",
    "    fingerprint_x = model_encoder.predict([test_keys])\n",
    "    false_poses, true_poses, ext_rates = [], [], []\n",
    "\n",
    "    for std in jitters:      \n",
    "        ########## True positve:\n",
    "        output_fin = add_gussian_noise_to_ipds_fingerprinted(fingerprint = fingerprint_x, x_test = X_test, std =std)\n",
    "        keys_true = model_decoder.predict([output_fin])\n",
    "        true_pos = decide_if_fingerprinted(keys_true, threshold=4)\n",
    "        key_pred = extract_keys_from_key_hat(keys_true)\n",
    "        error_rate = compute_error_rate_flowwise(predict_key = key_pred, true_key = test_keys)\n",
    "\n",
    "        ########## False positve: \n",
    "        output_non = add_gussian_noise_to_ipds_non_fingerprinted(X_test, std =std)\n",
    "        keys_false = model_decoder.predict([output_non])\n",
    "        false_pos = decide_if_fingerprinted(keys_false, threshold=4)\n",
    "        false_poses.append(false_pos)\n",
    "        true_poses.append(true_pos)\n",
    "        ext_rates.append(1 - error_rate)\n",
    "    write_array_to_file(array = ext_rates, target =target_name, delimiter =' ')\n",
    "    write_array_to_file(array = false_poses, target =target_name, delimiter =' ')\n",
    "    write_array_to_file(array = true_poses, target =target_name, delimiter =' ')\n",
    "    target_name.close()\n",
    "# compute_impact_of_jitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_fing_w, key_hat_w, epoch = 1, 50, 100\n",
    "def call_fit_load_eval_Main():\n",
    "    n_all_true_trains =[5000]# [5000, 10000, 20000, 50000]#5000,, \n",
    "    sample_sizes = [600]#[400, 200, 600]\n",
    "    key_lengths = [10]#, 15, 20]\n",
    "\n",
    "    for sample_size in sample_sizes:\n",
    "        X = create_sample_size_dataset(all_ipds, sample_size = sample_size)\n",
    "        for key_length in key_lengths:\n",
    "            key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "            false_poses, true_poses, ext_rates = [], [], []\n",
    "            target_name = open(path_for_results + str(sample_size)+\"_\"+str(key_length) + '.txt', 'w')\n",
    "\n",
    "            for train_number in n_all_true_trains:\n",
    "                false_pos, true_pos, ext_rate = fit_model_load_evaulte(n_true_train =train_number, key_length=key_length, sample_size=sample_size,X=X,key_options=key_options)\n",
    "                false_poses.append(false_pos)\n",
    "                true_poses.append(true_pos)\n",
    "                ext_rates.append(ext_rate)\n",
    "                print(false_pos, true_pos, ext_rate)\n",
    "            write_array_to_file(array = ext_rates, target =target_name, delimiter =' ')\n",
    "            write_array_to_file(array = false_poses, target =target_name, delimiter =' ')\n",
    "            write_array_to_file(array = true_poses, target =target_name, delimiter =' ')\n",
    "            target_name.close()\n",
    "call_fit_load_eval_Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_model_for_more_epochs():\n",
    "    sample_size, key_length, n_true_train, epoch = 600, 10, 50000, 250\n",
    "    n_false_train = int(n_true_train/10)\n",
    "    x_fing_w, key_hat_w = 1, 50\n",
    "    model_name = \"march_10\" + str(sample_size) + \"_\" + str(key_length) + \"_\" + str(\n",
    "    n_true_train) + \"_\" + str(n_false_train) + \"_\" + str(epoch) + \"_\" + str(x_fing_w) + \"_\" + str(key_hat_w)\n",
    "\n",
    "    model = load_NN_model(path + model_name)\n",
    "    model_encoder_decoder.compile(optimizer='adam', \n",
    "                              loss=losses.mean_absolute_error,\n",
    "                                  loss_weights={'fingerprint':x_fing_w, 'key_hat':key_hat_w})\n",
    "\n",
    "    model_encoder_decoder.fit([X_train, training_keys], [y_train, training_keys],\n",
    "                        batch_size = 64, epochs = epoch + 250, verbose = 0)\n",
    "    #save_model_weights(model_encoder_decoder, name=model_name)\n",
    "# reload_model_for_more_epochs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_all_Main():\n",
    "    n_all_true_trains = [10000]# 5000, 10000, 20000, 50000]\n",
    "    sample_sizes = [600]\n",
    "    key_lengths = [10]\n",
    "    x_fing_w, key_hat_w, epoch = 1, 50, 100\n",
    "    n_test = 1000\n",
    "    for sample_size in sample_sizes:\n",
    "            \n",
    "            X = create_sample_size_dataset(all_ipds_for_test, sample_size = sample_size)\n",
    "            for key_length in key_lengths:\n",
    "                key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "                false_poses, true_poses, ext_rates = [], [], []\n",
    "                #target_name = open('/home/fatemeh/Dropbox/Fingerprint/Results/500_' + str(sample_size)+\"_\"+str(key_length) + '.txt', 'w')\n",
    "\n",
    "                for train_number in n_all_true_trains:\n",
    "                    model_decoder, model_encoder = load_model_for_testing(key_length, sample_size, train_number)\n",
    "                    false_pos, true_pos, ext_rate = evalute_encoder_decoder(model_decoder,model_encoder, X, sample_size, key_length,\n",
    "                                                                            key_options, train_number, int(train_number/10), n_test=n_test)\n",
    "                    false_poses.append(false_pos)\n",
    "                    true_poses.append(true_pos)\n",
    "                    ext_rates.append(ext_rate)\n",
    "                    print(sample_size, key_length, train_number, \"Result: \", false_pos, true_pos, ext_rate)\n",
    "#                 write_array_to_file(array = ext_rates, target =target_name, delimiter =' ')\n",
    "#                 write_array_to_file(array = false_poses, target =target_name, delimiter =' ')\n",
    "#                 write_array_to_file(array = true_poses, target =target_name, delimiter =' ')\n",
    "#                 target_name.close() \n",
    "load_test_all_Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_true_trains = [5000, 10000, 20000, 50000, 100000]\n",
    "sample_size, key_length = 600, 10\n",
    "epoch = 250\n",
    "for n_true_train in n_true_trains:\n",
    "    n_false_train = int(n_true_train/10)\n",
    "    key_hat_w, x_hat_w = 50, 1\n",
    "    model_name = \"march10_\" + str(sample_size) + \"_\" + str(key_length) + \"_\" + str(\n",
    "        n_true_train) + \"_\" + str(n_false_train) + \"_\" + str(epoch) + \"_\" + str(x_hat_w) + \"_\" + str(key_hat_w)\n",
    "\n",
    "    model = load_NN_model(path + model_name)\n",
    "\n",
    "    X_train, y_train, training_keys = get_false_true_training(n_true_train, n_false_train, key_length, X, key_options)\n",
    "    print(\"Finished reading dataset\")\n",
    "\n",
    "    model.compile(optimizer='adam', \n",
    "                                  loss=losses.mean_absolute_error,\n",
    "                                      loss_weights={'fingerprint':1, 'key_hat':50})\n",
    "    model.fit([X_train, training_keys], [y_train, training_keys],\n",
    "                            batch_size = 64, epochs = epoch, verbose = 0)\n",
    "    \n",
    "    model_name = \"march10_\" + str(sample_size) + \"_\" + str(key_length) + \"_\" + str(\n",
    "        n_true_train) + \"_\" + str(n_false_train) + \"_\" + str(500) + \"_\" + str(x_hat_w) + \"_\" + str(key_hat_w)\n",
    "\n",
    "    save_model_weights(model, name= model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_encoder(key_length, sample_size):\n",
    "    chunk, p = 10, 0\n",
    "    Input_ipd = Input(shape=(sample_size, 1), name='input1')  # this is needed just for the decoding\n",
    "    Input_key = Input(shape=(key_length,), name='input2')\n",
    "    fingerprint_mult = Input(shape=(chunk,), name='input3')\n",
    "    fingerprint_sub = Input(shape=(chunk,), name='input4')\n",
    "    \n",
    "    ipd = Flatten(name =\"ipd_flatten1\")(Input_ipd)\n",
    "    outputs = []\n",
    "    \n",
    "    quant = int(sample_size/chunk)\n",
    "    def slice(x):\n",
    "        return x[:, p * chunk:(1 + p) * chunk]\n",
    "    \n",
    "    key1 = Dense(32, name='key1')(Input_key)\n",
    "\n",
    "    sliced_ipd = Lambda(slice)(ipd)\n",
    "    x_fingerprint = sliced_ipd\n",
    "    for i in range(0, quant):\n",
    "        sliced_ipd = Lambda(slice)(ipd)\n",
    "        ss = Concatenate(name = 'concat'+ str(p))([x_fingerprint, sliced_ipd]) \n",
    "        ipd1 = Dense(32, name = 'dense'+ str(p))(ss)\n",
    "        batch_2 = BatchNormalization(name = 'batch'+ str(p))(ipd1)\n",
    "        relu_2 = Activation('relu', name = 'act'+ str(p))(batch_2)\n",
    "        \n",
    "        ipds_merged_all = Concatenate(name = 'concat_key_'+ str(p))([relu_2, key1])\n",
    "        dense_enc1 = Dense(64, name = 'dense_enc1' + str(p))(ipds_merged_all)\n",
    "        batch_2 = BatchNormalization(name = 'batch2_'+ str(p))(dense_enc1)\n",
    "        relu_2 = Activation('relu', name = 'act2_'+ str(p))(batch_2)\n",
    "        dense_drop_enc1 = Dropout(0.3, name = 'dense_drop_enc1' + str(p))(relu_2)\n",
    "        \n",
    "        x_fingerprint_sig = Dense(chunk, name = 'fingerprint_sig' + str(p), activation = 'sigmoid')(dense_drop_enc1)\n",
    "        x_fingerprint_mult = Multiply(name = 'fingerprint_mult' + str(p))([x_fingerprint_sig, fingerprint_mult])\n",
    "        x_fingerprint = Add(name = 'ipd_delay' + str(p))([x_fingerprint_mult, fingerprint_sub])\n",
    "        outputs.append(x_fingerprint)\n",
    "        p += 1\n",
    "    x_fingerprint = Concatenate(name = 'fingerprint2')(outputs)\n",
    "    x_fingerprint_output = Reshape((sample_size,1), name='fingerprint')(x_fingerprint)\n",
    "    model_encoder = Model(inputs=[Input_key,Input_ipd,  fingerprint_mult, fingerprint_sub], outputs=[x_fingerprint_output])\n",
    "    #model_encoder.load_weights(filepath=path + model_name + \".h5\", by_name=True)\n",
    "    return model_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
