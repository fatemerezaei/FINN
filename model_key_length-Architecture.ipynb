{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\ndef get_only_true_data(X, key_options):\\n    \\n    training_keys_true = get_keys_for_fingerprinting_data(size=len(X), key_options=key_options)\\n    X_train_true = [x for x in X]\\n    y_train_true = get_fingerprints_for_ipds(len(X), sample_size=len(X[0]))#,training_keys=training_keys_true)\\n    \\n    \\n    X_train = np.expand_dims(X_train_true, axis=1).reshape((-1,900,1))\\n    y_train = np.expand_dims(y_train_true, axis=1).reshape((-1,900,1))\\n    training_keys = training_keys_true#,#np.expand_dims(training_keys_true, axis=1)\\n    \\n    return X_train, y_train, training_keys\\ndef get_false_training(key_length, X):\\n    false_value = 0\\n    k = [1]\\n    for i in range(key_length-1):\\n        k.append(0)\\n    k = np.array(k)\\n    y_train_non =  [np.zeros(len(x)) for x in X]#np.zeros(len(x))\\n    training_keys = [k for x in range(len(y_train_non))]\\n    \\n    return X, y_train_non, training_keys\\ndef get_false_true_training(X, key_options):\\n    n_false_train = int(len(X) / 50)\\n    n_true_train = len(X) - n_false_train\\n    \\n    training_keys_true = get_keys_for_fingerprinting_data(size=n_true_train, key_options=key_options)\\n    X_train_true = [x for x in X[0:n_true_train]]\\n    y_train_true = add_fignerprinting_to_ipds(n_true_train,sample_size=len(X[0]))#,training_keys=training_keys_true)\\n    \\n    X_train_false, y_train_false, training_keys_false = get_false_training(len(key_options[0]), X[-n_false_train:])\\n    \\n    print(len(X_train_false),len(X_train_true))\\n    X_train, y_train, training_keys = [], [], []\\n    f, t = 0, 0\\n    for i in range(n_false_train + n_true_train):\\n        rnd = random.randrange(0, 50)\\n        if rnd == 1 and f < len(X_train_false) :\\n            X_train.append(X_train_false[f])\\n            y_train.append(y_train_false[f])\\n            training_keys.append(training_keys_false[f])\\n            f += 1\\n        elif t < len(X_train_true):\\n            X_train.append(X_train_true[t])\\n            y_train.append(y_train_true[t])\\n            training_keys.append(training_keys_true[t])\\n            t += 1 \\n    \\n\\n    \\n    X_train = np.expand_dims(X_train, axis=1).reshape((-1, 900, 1))#np.array(X_train).reshape((-1, 900, 1))\\n    y_train = np.expand_dims(y_train, axis=1).reshape((-1, 900, 1))\\n    training_keys = np.array(training_keys)\\n    \\n    \\n    return X_train, y_train, training_keys\\ndef save_model_weights(model, name):\\n    model_json = model.to_json()\\n    with open(path + str(name) + \".json\", \"w\") as json_file:\\n        json_file.write(model_json)\\n    model.save_weights(path + str(name) + \".h5\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Activation, Dropout, Dense, Input, Add, Multiply, Concatenate,Lambda\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D,  Flatten, Dot,Reshape\n",
    "from keras.models import Model\n",
    "import random, time, os\n",
    "import numpy as np\n",
    "from keras import losses\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.python import keras\n",
    "\n",
    "def create_sample_size_dataset(all_ipds, sample_size, n_sample):\n",
    "    #number_of_samples = int(len(all_ipds) / sample_size)\n",
    "    all_samples = []\n",
    "    for p in range(n_sample):\n",
    "        all_samples.append(all_ipds[p * sample_size:(p + 1) * sample_size])\n",
    "    return all_samples\n",
    "\n",
    "def write_array_to_file(array, target, delimiter):\n",
    "    for k in range(0, len(array)):\n",
    "        target.write(str(array[k]) + delimiter)\n",
    "    target.write(\"\\n\")\n",
    "\n",
    "def read_from_file(path):\n",
    "    with open(path, 'r') as content_file:\n",
    "        content = content_file.read()\n",
    "        return content\n",
    "\n",
    "\n",
    "def create_ipd_dataset(address):\n",
    "    files = os.listdir(address)\n",
    "    all_ipds = []\n",
    "    for f in files:\n",
    "            ipd = read_from_file(address + f).split(' ')\n",
    "            all_ipds.extend(convert_stringArrays_to_floatArray(ipd))\n",
    "    return all_ipds\n",
    "\n",
    "\n",
    "def isfloat(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def convert_stringArrays_to_floatArray(array):\n",
    "    intArray = []\n",
    "\n",
    "    for k in array:\n",
    "        if isfloat(k):\n",
    "            intArray.append(float(k))\n",
    "    return intArray\n",
    "\n",
    "\n",
    "def convert_stringArrays_to_intArray(array):\n",
    "    intArray = []\n",
    "\n",
    "    for k in array:\n",
    "        if isfloat(k):\n",
    "            intArray.append(int(k))\n",
    "    return intArray\n",
    "\n",
    "def get_fingerprints_for_ipds(n_train, sample_size, alpha):\n",
    "    \n",
    "    #Previous one was all + and the largest value was 50.\n",
    "   \n",
    "    fingerprint_output = []\n",
    "    while len(fingerprint_output) < n_train:\n",
    "        finger = [random.uniform(0, 250)]\n",
    "        neg_numbers = 0\n",
    "        #np.random.laplace(0, std, sample_size)\n",
    "        finger = np.random.laplace(0, 1, sample_size)\n",
    "        for i in range(sample_size - 1):\n",
    "            #if random.randrange(0, 3) == 0:\n",
    "            '''if random.randrange(0, 2) == 1:\n",
    "                finger.append(random.uniform(0, alpha))#random.uniform(0, 10)\n",
    "            else:\n",
    "                finger.append(-1 * random.uniform(0, alpha))#\n",
    "            if sum(finger) < 0:\n",
    "                neg_numbers += 1'''\n",
    "#             else:\n",
    "#                 finger.append(0) \n",
    "        #if neg_numbers < 50:  ## this can be a hyperparameter\n",
    "        fingerprint_output.append(finger)\n",
    "    return fingerprint_output\n",
    "\n",
    "def get_keys_for_fingerprinting_data(size, key_options):\n",
    "    selected_keys = []#np.array([key_options[0]])\n",
    "#     print(selected_keys,type(selected_keys))\n",
    "    for i in range(size ):\n",
    "        rnd = random.randrange(0, len(key_options))\n",
    "#         print([key_options[rnd]],type(key_options[0]))\n",
    "        #selected_keys = np.concatenate((selected_keys, np.array([key_options[rnd]])))\n",
    "        selected_keys.append(key_options[rnd])\n",
    "\n",
    "    return selected_keys\n",
    "\n",
    "\n",
    "def get_false_true_data(X, key_options,alpha):\n",
    "    \n",
    "    training_keys_true = get_keys_for_fingerprinting_data(size=len(X), key_options=key_options)\n",
    "    #X_train_true =X# [x for x in X]\n",
    "    y_train_true = get_fingerprints_for_ipds(len(X), sample_size=len(X[0]),alpha=alpha)#,training_keys=training_keys_true)\n",
    "    \n",
    "    ####### changing true trianing to false\n",
    "    \n",
    "    '''\n",
    "    false_key = np.zeros(len(key_options[0]))\n",
    "    false_key[0] = 1\n",
    "    ratio = len(key_options[0])\n",
    "    print(ratio, \"ratio\")\n",
    "    key_length = len(key_options[0])\n",
    "    num_false_train = int(len(X)/key_length )\n",
    "    print(\"finished getting true data\")\n",
    "    for i in range(num_false_train):\n",
    "        y_train_true[i * ratio] = np.zeros(len(X[0]))\n",
    "        training_keys_true[i * ratio] = false_key\n",
    "    \n",
    "    '''\n",
    "    X_train = np.expand_dims(X, axis=1).reshape((-1, len(X[0]), 1))\n",
    "    y_train = np.expand_dims(y_train_true, axis=1).reshape((-1, len(X[0]), 1))\n",
    "    #training_keys = training_keys_true#,#np.expand_dims(training_keys_true, axis=1)\n",
    "#     training_keys_true = np.array(training_keys_true)\n",
    "    \n",
    "    return X_train, y_train, training_keys_true\n",
    "\n",
    "def selecting_valid_fingerprints(key_length):\n",
    "    all_keys = []\n",
    "    #address = '/home/fatemeh/MyProjects/Fingerprint/Synthetic dataset/keys/' + str(key_length) + \"/\"\n",
    "    key_i =np.zeros(key_length)\n",
    "    #keys = os.listdir(address)\n",
    "    for k in range(key_length):\n",
    "        key_i =np.zeros(key_length)\n",
    "        key_i[k] = 1\n",
    "        #key_i = convert_stringArrays_to_intArray(read_from_file(address + k).split(\" \"))\n",
    "        #if key_i [0] == 1:\n",
    "         #   continue\n",
    "        all_keys.append(key_i)\n",
    "       # key_i[k]=0\n",
    "            \n",
    "    return all_keys\n",
    "\n",
    "'''\n",
    "\n",
    "def get_only_true_data(X, key_options):\n",
    "    \n",
    "    training_keys_true = get_keys_for_fingerprinting_data(size=len(X), key_options=key_options)\n",
    "    X_train_true = [x for x in X]\n",
    "    y_train_true = get_fingerprints_for_ipds(len(X), sample_size=len(X[0]))#,training_keys=training_keys_true)\n",
    "    \n",
    "    \n",
    "    X_train = np.expand_dims(X_train_true, axis=1).reshape((-1,900,1))\n",
    "    y_train = np.expand_dims(y_train_true, axis=1).reshape((-1,900,1))\n",
    "    training_keys = training_keys_true#,#np.expand_dims(training_keys_true, axis=1)\n",
    "    \n",
    "    return X_train, y_train, training_keys\n",
    "def get_false_training(key_length, X):\n",
    "    false_value = 0\n",
    "    k = [1]\n",
    "    for i in range(key_length-1):\n",
    "        k.append(0)\n",
    "    k = np.array(k)\n",
    "    y_train_non =  [np.zeros(len(x)) for x in X]#np.zeros(len(x))\n",
    "    training_keys = [k for x in range(len(y_train_non))]\n",
    "    \n",
    "    return X, y_train_non, training_keys\n",
    "def get_false_true_training(X, key_options):\n",
    "    n_false_train = int(len(X) / 50)\n",
    "    n_true_train = len(X) - n_false_train\n",
    "    \n",
    "    training_keys_true = get_keys_for_fingerprinting_data(size=n_true_train, key_options=key_options)\n",
    "    X_train_true = [x for x in X[0:n_true_train]]\n",
    "    y_train_true = add_fignerprinting_to_ipds(n_true_train,sample_size=len(X[0]))#,training_keys=training_keys_true)\n",
    "    \n",
    "    X_train_false, y_train_false, training_keys_false = get_false_training(len(key_options[0]), X[-n_false_train:])\n",
    "    \n",
    "    print(len(X_train_false),len(X_train_true))\n",
    "    X_train, y_train, training_keys = [], [], []\n",
    "    f, t = 0, 0\n",
    "    for i in range(n_false_train + n_true_train):\n",
    "        rnd = random.randrange(0, 50)\n",
    "        if rnd == 1 and f < len(X_train_false) :\n",
    "            X_train.append(X_train_false[f])\n",
    "            y_train.append(y_train_false[f])\n",
    "            training_keys.append(training_keys_false[f])\n",
    "            f += 1\n",
    "        elif t < len(X_train_true):\n",
    "            X_train.append(X_train_true[t])\n",
    "            y_train.append(y_train_true[t])\n",
    "            training_keys.append(training_keys_true[t])\n",
    "            t += 1 \n",
    "    \n",
    "\n",
    "    \n",
    "    X_train = np.expand_dims(X_train, axis=1).reshape((-1, 900, 1))#np.array(X_train).reshape((-1, 900, 1))\n",
    "    y_train = np.expand_dims(y_train, axis=1).reshape((-1, 900, 1))\n",
    "    training_keys = np.array(training_keys)\n",
    "    \n",
    "    \n",
    "    return X_train, y_train, training_keys\n",
    "def save_model_weights(model, name):\n",
    "    model_json = model.to_json()\n",
    "    with open(path + str(name) + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(path + str(name) + \".h5\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_encoder_dense_slice(sample_size, key_length, chunk):\n",
    "    p = 0\n",
    "    Input_ipd = Input(shape=(sample_size, 1), name='input1')  # this is needed just for the decoding\n",
    "    Input_key = Input(shape=(key_length,), name='input2')\n",
    "    fingerprint_mult = Input(shape=(chunk,), name='input3')\n",
    "    fingerprint_sub = Input(shape=(chunk,), name='input4')\n",
    "    network_noise = Input(shape=(sample_size,), name='input5')\n",
    "    \n",
    "    ipd = Flatten(name =\"ipd_flatten1\")(Input_ipd)\n",
    "    outputs = []\n",
    "    \n",
    "    quant = int(sample_size/chunk)\n",
    "    def slice(x):\n",
    "        return x[:, p * chunk:(1 + p) * chunk]\n",
    "    \n",
    "    key1 = Dense(64, name='key1')(Input_key)\n",
    "\n",
    "    sliced_ipd = Lambda(slice)(ipd)\n",
    "    x_fingerprint = sliced_ipd\n",
    "    for i in range(0, quant):\n",
    "        sliced_ipd = Lambda(slice)(ipd)\n",
    "        print(sliced_ipd, x_fingerprint)\n",
    "        ss = Concatenate(name = 'concat'+ str(p))([x_fingerprint, sliced_ipd]) \n",
    "        ipd1 = Dense(32, name = 'dense'+ str(p))(ss)\n",
    "        batch_2 = BatchNormalization(name = 'batch'+ str(p))(ipd1)\n",
    "        relu_2 = Activation('relu', name = 'act'+ str(p))(batch_2)\n",
    "        \n",
    "        ipds_key_merge = Concatenate(name = 'concat_key_'+ str(p))([relu_2, key1])\n",
    "        dense_enc1 = Dense(64, name = 'dense_enc1' + str(p))(ipds_key_merge)\n",
    "        batch_2 = BatchNormalization(name = 'batch2_'+ str(p))(dense_enc1)\n",
    "        relu_2 = Activation('relu', name = 'act2_'+ str(p))(batch_2)\n",
    "        dense_drop_enc1 = Dropout(0.3, name = 'dense_drop_enc1' + str(p))(relu_2)\n",
    "        \n",
    "        x_fingerprint_sig = Dense(chunk, name = 'fingerprint_sig' + str(p), activation = 'sigmoid')(dense_drop_enc1)\n",
    "        x_fingerprint_mult = Multiply(name = 'fingerprint_mult' + str(p))([x_fingerprint_sig, fingerprint_mult])\n",
    "        x_fingerprint = Add(name = 'ipd_delay' + str(p))([x_fingerprint_mult, fingerprint_sub])\n",
    "        outputs.append(x_fingerprint)\n",
    "        p += 1\n",
    "    x_fingerprint = Concatenate(name = 'fingerprint2')(outputs)\n",
    "    x_fingerprint_output = Reshape((sample_size, 1), name='fingerprint')(x_fingerprint)\n",
    "\n",
    "def get_encoder_slice(key_length, chunk, p):\n",
    "    Input_ipd = Input(shape=(chunk, 1), name='input1')  # this is needed just for the decoding\n",
    "    Input_prev_fing = Input(shape=(chunk, 1), name='input2')\n",
    "    Input_key = Input(shape=(key_length,), name='input3')\n",
    "    fingerprint_mult = Input(shape=(chunk,), name='input4')\n",
    "    fingerprint_sub = Input(shape=(chunk,), name='input5')\n",
    "    \n",
    "    ipd = Flatten(name =\"ipd_flatten\")(Input_ipd)\n",
    "    fing = Flatten(name =\"fing_flatten\")(Input_prev_fing)\n",
    "    \n",
    "    key1 = Dense(64, name='key1')(Input_key)\n",
    "    ss = Concatenate(name = 'concat'+ str(p))([fing, ipd]) \n",
    "    ipd1 = Dense(32, name = 'dense'+ str(p))(ss)\n",
    "    \n",
    "    batch_2 = BatchNormalization(name = 'batch'+ str(p))(ipd1)\n",
    "    relu_2 = Activation('relu', name = 'act'+ str(p))(batch_2)\n",
    "\n",
    "    ipds_key_merge = Concatenate(name = 'concat_key_'+ str(p))([relu_2, key1])\n",
    "    dense_enc1 = Dense(64, name = 'dense_enc1' + str(p))(ipds_key_merge)\n",
    "    batch_2 = BatchNormalization(name = 'batch2_'+ str(p))(dense_enc1)\n",
    "    relu_2 = Activation('relu', name = 'act2_'+ str(p))(batch_2)\n",
    "    dense_drop_enc1 = Dropout(0.3, name = 'dense_drop_enc1' + str(p))(relu_2)\n",
    "\n",
    "    x_fingerprint_sig = Dense(chunk, name = 'fingerprint_sig' + str(p), activation = 'sigmoid')(dense_drop_enc1)\n",
    "    x_fingerprint_mult = Multiply(name = 'fingerprint_mult' + str(p))([x_fingerprint_sig, fingerprint_mult])\n",
    "    x_fingerprint = Add(name = 'ipd_delay' + str(p))([x_fingerprint_mult, fingerprint_sub])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_encoder_decoder_conv_dense_slice(sample_size, key_length, chunk,reg):\n",
    "    p = 0\n",
    "    Input_ipd = Input(shape=(sample_size, 1), name='input1')  # this is needed just for the decoding\n",
    "    Input_key = Input(shape=(key_length,), name='input2')\n",
    "    fingerprint_mult = Input(shape=(chunk,), name='input3')\n",
    "    fingerprint_sub = Input(shape=(chunk,), name='input4')\n",
    "    network_noise = Input(shape=(sample_size,), name='input5')\n",
    "    \n",
    "    ipd = Flatten(name =\"ipd_flatten1\")(Input_ipd)\n",
    "    outputs = []\n",
    "    \n",
    "    quant = int(sample_size/chunk)\n",
    "    def slice(x):\n",
    "        return x[:, p * chunk:(1 + p) * chunk]\n",
    "    \n",
    "    key1 = Dense(64, name='key1')(Input_key)\n",
    "\n",
    "    sliced_ipd = Lambda(slice)(ipd)\n",
    "    x_fingerprint = sliced_ipd\n",
    "    for i in range(0, quant):\n",
    "        sliced_ipd = Lambda(slice)(ipd)\n",
    "        #print(sliced_ipd, x_fingerprint)\n",
    "        ss = Concatenate(name = 'concat'+ str(p))([x_fingerprint, sliced_ipd]) \n",
    "        ipd1 = Dense(32,kernel_regularizer=l2(reg), name = 'dense'+ str(p))(ss)\n",
    "        batch_2 = BatchNormalization(name = 'batch'+ str(p))(ipd1)\n",
    "        relu_2 = Activation('relu', name = 'act'+ str(p))(batch_2)\n",
    "        \n",
    "        ipds_key_merge = Concatenate(name = 'concat_key_'+ str(p))([relu_2, key1])\n",
    "        dense_enc1 = Dense(64, kernel_regularizer=l2(reg), name = 'dense_enc1' + str(p))(ipds_key_merge)\n",
    "        batch_2 = BatchNormalization(name = 'batch2_'+ str(p))(dense_enc1)\n",
    "        relu_2 = Activation('relu', name = 'act2_'+ str(p))(batch_2)\n",
    "        dense_drop_enc1 = Dropout(0.3, name = 'dense_drop_enc1' + str(p))(relu_2)\n",
    "        \n",
    "        x_fingerprint_sig = Dense(chunk,kernel_regularizer=l2(reg), name = 'fingerprint_sig' + str(p), activation = 'sigmoid')(dense_drop_enc1)\n",
    "        x_fingerprint_mult = Multiply(name = 'fingerprint_mult' + str(p))([x_fingerprint_sig, fingerprint_mult])\n",
    "        x_fingerprint = Add(name = 'ipd_delay' + str(p))([x_fingerprint_mult, fingerprint_sub])\n",
    "        outputs.append(x_fingerprint)\n",
    "        p += 1\n",
    "    x_fingerprint = Concatenate(name = 'fingerprint2')(outputs)\n",
    "    x_fingerprint_output = Reshape((sample_size, 1), name='fingerprint')(x_fingerprint)\n",
    "\n",
    "    x_ipd = Add(name = 'x_ipd')([x_fingerprint, ipd, network_noise])\n",
    "        \n",
    "    x_ipd_reshape = Reshape((sample_size, 1),name = 'reshape_dec')(x_ipd)\n",
    "    \n",
    "    \n",
    "    conv_dec_2 = Conv1D(filters = 10, kernel_size=10,kernel_regularizer=l2(reg), padding='same', name='conv_dec_2')(x_ipd_reshape)\n",
    "    conv_batch_2 = BatchNormalization(name='conv_batch_2_dec')(conv_dec_2)\n",
    "    conv_relu_2 = Activation('relu', name='conv_relu_2_dec')(conv_batch_2)\n",
    "    conv_drop_2 = Dropout(0.3, name='conv_drop_2_dec')(conv_relu_2)\n",
    "    max_pool_dec_2 = MaxPooling1D(pool_size=1, name=\"max_pool_dec_2\")(conv_drop_2)\n",
    "    \n",
    "    \n",
    "    conv_dec_3 = Conv1D(filters = 5, kernel_size=10,kernel_regularizer=l2(reg), padding='same', name='conv_dec_3')(max_pool_dec_2)\n",
    "    conv_batch_3 = BatchNormalization(name='conv_batch_3_dec')(conv_dec_3)\n",
    "    conv_relu_3 = Activation('relu', name='conv_relu_3_dec')(conv_batch_3)\n",
    "    conv_drop_2 = Dropout(0.3, name='conv_drop_3_dec')(conv_relu_3)\n",
    "    max_pool_dec_3 = MaxPooling1D(pool_size=1, name=\"max_pool_dec_3\")(conv_drop_2)\n",
    "    max_pool_dec_3_f = Flatten(name =\"flate_max3_dec\")(max_pool_dec_3)\n",
    "\n",
    "    dense_dec_1 = Dense(256, kernel_regularizer=l2(reg),name='dense_dec_1')(max_pool_dec_3_f)\n",
    "    \n",
    "    dense_batch_dec1 = BatchNormalization(name='dense_batch_dec1')(dense_dec_1)\n",
    "    dense_relu_dec1 = Activation('relu', name='dense_relu_dec1')(dense_batch_dec1)\n",
    "    dense_drop_dec1 = Dropout(0.3, name='dense_drop_dec1')(dense_relu_dec1)    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    dense_dec_2 = Dense(512, kernel_regularizer=l2(reg), name='dense_dec_2')(dense_drop_dec1)\n",
    "    dense_batch_dec2 = BatchNormalization(name='dense_batch_dec2')(dense_dec_2)\n",
    "    dense_relu_dec2 = Activation('relu', name='dense_relu_dec2')(dense_batch_dec2)\n",
    "    dense_drop_dec2 = Dropout(0.3, name='dense_drop_dec2')(dense_relu_dec2)\n",
    "\n",
    "    \n",
    "    \n",
    "    key_hat = Dense(key_length, activation='softmax', name='key_hat')(dense_drop_dec2)\n",
    "\n",
    "    return Model(inputs=[Input_ipd, Input_key, fingerprint_mult, fingerprint_sub, network_noise], outputs=[x_fingerprint_output, key_hat])#, key_hat])\n",
    "\n",
    "\n",
    "def load_decoder(key_length, sample_size):\n",
    "    \n",
    "    Input_ipd = Input(shape=(sample_size, 1), name='reshape_dec') \n",
    "    #x_ipd_reshape = Reshape((sample_size, 1), name = 'reshape')(Input_ipd)\n",
    "    conv_dec_2 = Conv1D(filters = 20, kernel_size=10, padding='same', name='conv_dec_2')(Input_ipd)\n",
    "    conv_batch_2 = BatchNormalization(name='conv_batch_2_dec')(conv_dec_2)\n",
    "    conv_relu_2 = Activation('relu', name='conv_relu_2_dec')(conv_batch_2)\n",
    "    conv_drop_2 = Dropout(0.3, name='conv_drop_2_dec')(conv_relu_2)\n",
    "    max_pool_dec_2 = MaxPooling1D(pool_size=1, name=\"max_pool_dec_2\")(conv_drop_2)\n",
    "    \n",
    "    conv_dec_3 = Conv1D(filters = 10, kernel_size=10, padding='same', name='conv_dec_3')(max_pool_dec_2)\n",
    "    conv_batch_3 = BatchNormalization(name='conv_batch_3_dec')(conv_dec_3)\n",
    "    conv_relu_3 = Activation('relu', name='conv_relu_3_dec')(conv_batch_3)\n",
    "    conv_drop_2 = Dropout(0.3, name='conv_drop_3_dec')(conv_relu_3)\n",
    "    max_pool_dec_3 = MaxPooling1D(pool_size=1, name=\"max_pool_dec_3\")(conv_drop_2)\n",
    "    max_pool_dec_3_f = Flatten(name =\"flate_max3_dec\")(max_pool_dec_3)\n",
    "\n",
    "    dense_dec_1 = Dense(256, name='dense_dec_1')(max_pool_dec_3_f)\n",
    "    dense_batch_dec1 = BatchNormalization(name='dense_batch_dec1')(dense_dec_1)\n",
    "    dense_relu_dec1 = Activation('relu', name='dense_relu_dec1')(dense_batch_dec1)\n",
    "    dense_drop_dec1 = Dropout(0.3, name='dense_drop_dec1')(dense_relu_dec1)    \n",
    "    \n",
    "    dense_dec_2 = Dense(64, name='dense_dec_2')(dense_drop_dec1)\n",
    "    dense_batch_dec2 = BatchNormalization(name='dense_batch_dec2')(dense_dec_2)\n",
    "    dense_relu_dec2 = Activation('relu', name='dense_relu_dec2')(dense_batch_dec2)\n",
    "    dense_drop_dec2 = Dropout(0.3, name='dense_drop_dec2')(dense_relu_dec2)\n",
    "    \n",
    "    key_hat = Dense(key_length, activation='softmax', name='key_hat')(dense_drop_dec2)\n",
    "    \n",
    "    \n",
    "    model_decoder = Model(inputs=[Input_ipd], outputs=[key_hat])\n",
    "    #model_decoder.set_weights(model.get_weights())\n",
    "    #model_decoder.load_weights(filepath=path + model_name + \".h5\", by_name=True)\n",
    "\n",
    "    return model_decoder\n",
    "\n",
    "def compute_extract_rate(keys, true_keys):\n",
    "    correct = 0 \n",
    "    for i in range(len(keys)): \n",
    "        if np.argmax(keys[i]) == np.argmax(true_keys[i]):\n",
    "            correct +=1\n",
    "    return correct/float(len(keys))\n",
    "\n",
    "\n",
    "def mean_pred_loss(y_true, y_pred):\n",
    "    #when the coeficent is smaller, performance is better. When increasing, noise improves\n",
    "    sum_abs = K.abs(y_pred)\n",
    "    tmp =  K.mean(sum_abs) - 0.3 * K.mean(y_pred)# + K.epsilon()\n",
    "    return 100 * (K.abs(K.mean(y_pred)))# 1/tmp  keras.losses.mean_absolute_error(y_true, y_pred) + \n",
    "\n",
    "def get_mult_sub_for_fingerprinting(n_data, max_delay, chunk, sample_size):\n",
    "    array_mult, array_sub = [], []\n",
    "    for x in range(0, n_data):\n",
    "        array_mult.append([max_delay] * chunk)\n",
    "        array_sub.append([-max_delay/2] * chunk)\n",
    "    array_mult = np.array(array_mult)\n",
    "    array_sub = np.array(array_sub)\n",
    "    return array_mult, array_sub\n",
    "\n",
    "def get_noise_simulation_array(n_data, std, sample_size):\n",
    "    noise = []\n",
    "    for x in range(0, n_data):\n",
    "        #noise.append(np.random.normal(0, std, sample_size))\n",
    "        #noise.append(np.random.uniform(0, std, sample_size))\n",
    "        noise.append(np.random.laplace(0, std, sample_size));\n",
    "    noise = np.array(noise)\n",
    "    return noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/fatemeh/MyProjects/Fingerprint/models/\"\n",
    "rate = '10'\n",
    "all_ipds_for_test = create_ipd_dataset(\n",
    "    address='/home/fatemeh/MyProjects/Fingerprint/Synthetic dataset/in/' + rate + '/test/')\n",
    "all_ipds_for_train = create_ipd_dataset(\n",
    "    address='/home/fatemeh/MyProjects/Fingerprint/Synthetic dataset/in/' + rate + '/train/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 5000 Numbre of training and testing data\n"
     ]
    }
   ],
   "source": [
    "n_true_train, n_test = 400000, 5000\n",
    "sample_size, key_length = 300, 1024# * 32\n",
    "\n",
    "X_train_all = create_sample_size_dataset(all_ipds_for_train, sample_size = sample_size,n_sample=n_true_train)\n",
    "X_test_all = create_sample_size_dataset(all_ipds_for_test, sample_size = sample_size,n_sample=n_test)\n",
    "print(len(X_train_all), len(X_test_all), \"Numbre of training and testing data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.27709889411926\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "alpha = 30\n",
    "beg = time.time()\n",
    "key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "X_train, y_train, train_keys = get_false_true_data(X_train_all, key_options, alpha=alpha)#get_only_true_data\n",
    "print(time.time() - beg)\n",
    "train_keys = np.array(train_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std, max_fing_delay = 10, alpha\n",
    "chunk = 10\n",
    "array_mult_test, array_sub_test = get_mult_sub_for_fingerprinting(n_test, max_delay=max_fing_delay, chunk=chunk,\n",
    "                                                                  sample_size=sample_size)\n",
    "noise_for_test = get_noise_simulation_array(n_test, std=std, sample_size=sample_size)\n",
    "array_mult_train, array_sub_train = get_mult_sub_for_fingerprinting(n_true_train, max_delay=max_fing_delay, chunk=chunk,\n",
    "                                                                    sample_size=sample_size)\n",
    "noise_for_train = get_noise_simulation_array(n_true_train, std=std, sample_size=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date :  2019-11-23 14:44:16.763304\n",
      "Model is Built and Compiled in 12.258148\n",
      "Train on 180000 samples, validate on 20000 samples\n",
      "Epoch 1/50\n",
      "180000/180000 [==============================] - 216s 1ms/step - loss: 1420.8660 - fingerprint_loss: 4.9186 - key_hat_loss: 7.0797 - val_loss: 1390.2592 - val_fingerprint_loss: 3.3570 - val_key_hat_loss: 6.9345\n",
      "Epoch 2/50\n",
      "180000/180000 [==============================] - 181s 1ms/step - loss: 1390.4584 - fingerprint_loss: 4.2565 - key_hat_loss: 6.9310 - val_loss: 1389.6198 - val_fingerprint_loss: 2.7194 - val_key_hat_loss: 6.9345\n",
      "Epoch 3/50\n",
      "180000/180000 [==============================] - 181s 1ms/step - loss: 1388.9406 - fingerprint_loss: 3.9903 - key_hat_loss: 6.9247 - val_loss: 1392.4084 - val_fingerprint_loss: 5.3171 - val_key_hat_loss: 6.9354\n",
      "Epoch 4/50\n",
      "180000/180000 [==============================] - 181s 1ms/step - loss: 1376.4475 - fingerprint_loss: 4.2383 - key_hat_loss: 6.8610 - val_loss: 1354.4590 - val_fingerprint_loss: 50.4721 - val_key_hat_loss: 6.5199\n",
      "Epoch 5/50\n",
      "180000/180000 [==============================] - 181s 1ms/step - loss: 921.3700 - fingerprint_loss: 13.4455 - key_hat_loss: 4.5396 - val_loss: 585.1320 - val_fingerprint_loss: 51.7097 - val_key_hat_loss: 2.6670\n",
      "Epoch 6/50\n",
      "180000/180000 [==============================] - 182s 1ms/step - loss: 528.8042 - fingerprint_loss: 18.1751 - key_hat_loss: 2.5531 - val_loss: 338.5141 - val_fingerprint_loss: 47.2371 - val_key_hat_loss: 1.4563\n",
      "Epoch 7/50\n",
      "180000/180000 [==============================] - 182s 1ms/step - loss: 371.5787 - fingerprint_loss: 19.2063 - key_hat_loss: 1.7618 - val_loss: 255.6184 - val_fingerprint_loss: 46.1800 - val_key_hat_loss: 1.0471\n",
      "Epoch 8/50\n",
      "180000/180000 [==============================] - 182s 1ms/step - loss: 300.9139 - fingerprint_loss: 18.8700 - key_hat_loss: 1.4101 - val_loss: 210.6604 - val_fingerprint_loss: 42.3270 - val_key_hat_loss: 0.8415\n",
      "Epoch 9/50\n",
      "180000/180000 [==============================] - 182s 1ms/step - loss: 261.7278 - fingerprint_loss: 17.7874 - key_hat_loss: 1.2196 - val_loss: 191.6653 - val_fingerprint_loss: 42.4152 - val_key_hat_loss: 0.7461\n",
      "Epoch 10/50\n",
      "180000/180000 [==============================] - 182s 1ms/step - loss: 237.0283 - fingerprint_loss: 16.2227 - key_hat_loss: 1.1039 - val_loss: 179.0051 - val_fingerprint_loss: 41.6019 - val_key_hat_loss: 0.6869\n",
      "Epoch 11/50\n",
      "180000/180000 [==============================] - 182s 1ms/step - loss: 218.1843 - fingerprint_loss: 14.7503 - key_hat_loss: 1.0170 - val_loss: 172.8920 - val_fingerprint_loss: 42.3096 - val_key_hat_loss: 0.6527\n",
      "Epoch 12/50\n",
      "180000/180000 [==============================] - 182s 1ms/step - loss: 203.7056 - fingerprint_loss: 13.2903 - key_hat_loss: 0.9519 - val_loss: 164.2841 - val_fingerprint_loss: 41.4791 - val_key_hat_loss: 0.6138\n",
      "Epoch 13/50\n",
      "180000/180000 [==============================] - 182s 1ms/step - loss: 192.0570 - fingerprint_loss: 12.4886 - key_hat_loss: 0.8977 - val_loss: 154.8525 - val_fingerprint_loss: 40.9862 - val_key_hat_loss: 0.5691\n",
      "Epoch 14/50\n",
      "180000/180000 [==============================] - 182s 1ms/step - loss: 183.2761 - fingerprint_loss: 11.7535 - key_hat_loss: 0.8574 - val_loss: 151.4856 - val_fingerprint_loss: 40.6900 - val_key_hat_loss: 0.5538\n",
      "Epoch 15/50\n",
      "180000/180000 [==============================] - 182s 1ms/step - loss: 175.6091 - fingerprint_loss: 11.1373 - key_hat_loss: 0.8221 - val_loss: 146.4578 - val_fingerprint_loss: 40.2562 - val_key_hat_loss: 0.5308\n",
      "Epoch 16/50\n",
      "180000/180000 [==============================] - 182s 1ms/step - loss: 169.0359 - fingerprint_loss: 10.4593 - key_hat_loss: 0.7927 - val_loss: 144.1196 - val_fingerprint_loss: 40.8618 - val_key_hat_loss: 0.5161\n",
      "Epoch 17/50\n",
      "180000/180000 [==============================] - 182s 1ms/step - loss: 163.3386 - fingerprint_loss: 9.9766 - key_hat_loss: 0.7666 - val_loss: 149.6321 - val_fingerprint_loss: 40.8318 - val_key_hat_loss: 0.5438\n",
      "Epoch 18/50\n",
      "180000/180000 [==============================] - 182s 1ms/step - loss: 156.6562 - fingerprint_loss: 9.3057 - key_hat_loss: 0.7365 - val_loss: 139.6868 - val_fingerprint_loss: 40.2706 - val_key_hat_loss: 0.4968\n",
      "Epoch 19/50\n",
      "180000/180000 [==============================] - 184s 1ms/step - loss: 153.1865 - fingerprint_loss: 9.0709 - key_hat_loss: 0.7203 - val_loss: 136.6652 - val_fingerprint_loss: 39.6870 - val_key_hat_loss: 0.4846\n",
      "Epoch 20/50\n",
      "180000/180000 [==============================] - 179s 996us/step - loss: 150.0377 - fingerprint_loss: 8.8074 - key_hat_loss: 0.7059 - val_loss: 137.2284 - val_fingerprint_loss: 39.4789 - val_key_hat_loss: 0.4885\n",
      "Epoch 21/50\n",
      "180000/180000 [==============================] - 178s 991us/step - loss: 145.1425 - fingerprint_loss: 8.6107 - key_hat_loss: 0.6824 - val_loss: 135.3759 - val_fingerprint_loss: 40.7041 - val_key_hat_loss: 0.4731\n",
      "Epoch 22/50\n",
      "180000/180000 [==============================] - 178s 990us/step - loss: 142.7252 - fingerprint_loss: 8.1946 - key_hat_loss: 0.6724 - val_loss: 132.4449 - val_fingerprint_loss: 39.2092 - val_key_hat_loss: 0.4659\n",
      "Epoch 23/50\n",
      "180000/180000 [==============================] - 178s 991us/step - loss: 139.5973 - fingerprint_loss: 8.0239 - key_hat_loss: 0.6576 - val_loss: 131.6686 - val_fingerprint_loss: 39.4342 - val_key_hat_loss: 0.4608\n",
      "Epoch 24/50\n",
      "180000/180000 [==============================] - 179s 993us/step - loss: 136.4178 - fingerprint_loss: 7.8783 - key_hat_loss: 0.6424 - val_loss: 129.9305 - val_fingerprint_loss: 38.9869 - val_key_hat_loss: 0.4544\n",
      "Epoch 25/50\n",
      "180000/180000 [==============================] - 184s 1ms/step - loss: 134.5039 - fingerprint_loss: 7.7601 - key_hat_loss: 0.6334 - val_loss: 133.7289 - val_fingerprint_loss: 40.8929 - val_key_hat_loss: 0.4638\n",
      "Epoch 26/50\n",
      "180000/180000 [==============================] - 182s 1ms/step - loss: 132.3507 - fingerprint_loss: 7.8458 - key_hat_loss: 0.6222 - val_loss: 130.5994 - val_fingerprint_loss: 40.4714 - val_key_hat_loss: 0.4503\n",
      "Epoch 27/50\n",
      "180000/180000 [==============================] - 190s 1ms/step - loss: 130.2903 - fingerprint_loss: 7.6734 - key_hat_loss: 0.6127 - val_loss: 127.5355 - val_fingerprint_loss: 39.5170 - val_key_hat_loss: 0.4397\n",
      "Epoch 28/50\n",
      "180000/180000 [==============================] - 187s 1ms/step - loss: 128.3731 - fingerprint_loss: 7.4817 - key_hat_loss: 0.6041 - val_loss: 127.3753 - val_fingerprint_loss: 39.2267 - val_key_hat_loss: 0.4404\n",
      "Epoch 29/50\n",
      "180000/180000 [==============================] - 183s 1ms/step - loss: 126.1923 - fingerprint_loss: 7.4235 - key_hat_loss: 0.5934 - val_loss: 129.0085 - val_fingerprint_loss: 39.1520 - val_key_hat_loss: 0.4489\n",
      "Epoch 30/50\n",
      "180000/180000 [==============================] - 184s 1ms/step - loss: 123.6937 - fingerprint_loss: 7.2061 - key_hat_loss: 0.5820 - val_loss: 126.8008 - val_fingerprint_loss: 40.8651 - val_key_hat_loss: 0.4293\n",
      "Epoch 31/50\n",
      "180000/180000 [==============================] - 183s 1ms/step - loss: 123.2364 - fingerprint_loss: 7.3149 - key_hat_loss: 0.5792 - val_loss: 131.2078 - val_fingerprint_loss: 38.9260 - val_key_hat_loss: 0.4610\n",
      "Epoch 32/50\n",
      "180000/180000 [==============================] - 180s 1ms/step - loss: 120.8917 - fingerprint_loss: 7.1491 - key_hat_loss: 0.5683 - val_loss: 121.9976 - val_fingerprint_loss: 38.9921 - val_key_hat_loss: 0.4146\n",
      "Epoch 33/50\n",
      "180000/180000 [==============================] - 183s 1ms/step - loss: 119.6297 - fingerprint_loss: 7.0667 - key_hat_loss: 0.5624 - val_loss: 123.1458 - val_fingerprint_loss: 40.0065 - val_key_hat_loss: 0.4152\n",
      "Epoch 34/50\n",
      "180000/180000 [==============================] - 201s 1ms/step - loss: 117.7845 - fingerprint_loss: 7.1413 - key_hat_loss: 0.5528 - val_loss: 122.2817 - val_fingerprint_loss: 39.1493 - val_key_hat_loss: 0.4152\n",
      "Epoch 35/50\n",
      "180000/180000 [==============================] - 180s 1ms/step - loss: 116.1778 - fingerprint_loss: 6.9345 - key_hat_loss: 0.5457 - val_loss: 121.6459 - val_fingerprint_loss: 39.0203 - val_key_hat_loss: 0.4126\n",
      "Epoch 36/50\n",
      "180000/180000 [==============================] - 199s 1ms/step - loss: 115.3743 - fingerprint_loss: 6.7720 - key_hat_loss: 0.5425 - val_loss: 121.4009 - val_fingerprint_loss: 39.8101 - val_key_hat_loss: 0.4075\n",
      "Epoch 37/50\n",
      "180000/180000 [==============================] - 215s 1ms/step - loss: 114.6512 - fingerprint_loss: 6.7365 - key_hat_loss: 0.5391 - val_loss: 123.0730 - val_fingerprint_loss: 39.0127 - val_key_hat_loss: 0.4198\n",
      "Epoch 38/50\n",
      "180000/180000 [==============================] - 185s 1ms/step - loss: 113.4763 - fingerprint_loss: 6.9613 - key_hat_loss: 0.5321 - val_loss: 121.1727 - val_fingerprint_loss: 39.7529 - val_key_hat_loss: 0.4066\n",
      "Epoch 39/50\n",
      "180000/180000 [==============================] - 194s 1ms/step - loss: 112.3100 - fingerprint_loss: 6.8838 - key_hat_loss: 0.5266 - val_loss: 118.4548 - val_fingerprint_loss: 38.9742 - val_key_hat_loss: 0.3969\n",
      "Epoch 40/50\n",
      "180000/180000 [==============================] - 179s 996us/step - loss: 111.0212 - fingerprint_loss: 6.8733 - key_hat_loss: 0.5202 - val_loss: 118.6121 - val_fingerprint_loss: 39.1041 - val_key_hat_loss: 0.3970\n",
      "Epoch 41/50\n",
      "180000/180000 [==============================] - 178s 989us/step - loss: 110.1372 - fingerprint_loss: 6.6967 - key_hat_loss: 0.5166 - val_loss: 118.1892 - val_fingerprint_loss: 38.8163 - val_key_hat_loss: 0.3963\n",
      "Epoch 42/50\n",
      "180000/180000 [==============================] - 178s 989us/step - loss: 108.5272 - fingerprint_loss: 6.7174 - key_hat_loss: 0.5085 - val_loss: 119.4842 - val_fingerprint_loss: 39.7464 - val_key_hat_loss: 0.3981\n",
      "Epoch 43/50\n",
      "180000/180000 [==============================] - 178s 989us/step - loss: 108.4241 - fingerprint_loss: 6.7980 - key_hat_loss: 0.5075 - val_loss: 123.1944 - val_fingerprint_loss: 40.6008 - val_key_hat_loss: 0.4124\n",
      "Epoch 44/50\n",
      "180000/180000 [==============================] - 181s 1ms/step - loss: 106.9811 - fingerprint_loss: 6.6443 - key_hat_loss: 0.5011 - val_loss: 118.2405 - val_fingerprint_loss: 39.0091 - val_key_hat_loss: 0.3956\n",
      "Epoch 45/50\n",
      "180000/180000 [==============================] - 182s 1ms/step - loss: 106.0380 - fingerprint_loss: 6.5895 - key_hat_loss: 0.4966 - val_loss: 117.7783 - val_fingerprint_loss: 39.0118 - val_key_hat_loss: 0.3932\n",
      "Epoch 46/50\n",
      "180000/180000 [==============================] - 185s 1ms/step - loss: 104.9505 - fingerprint_loss: 6.4926 - key_hat_loss: 0.4917 - val_loss: 119.3301 - val_fingerprint_loss: 39.4203 - val_key_hat_loss: 0.3989\n",
      "Epoch 47/50\n",
      "180000/180000 [==============================] - 180s 998us/step - loss: 103.4563 - fingerprint_loss: 6.5797 - key_hat_loss: 0.4837 - val_loss: 118.1895 - val_fingerprint_loss: 39.8209 - val_key_hat_loss: 0.3912\n",
      "Epoch 48/50\n",
      "180000/180000 [==============================] - 178s 987us/step - loss: 103.6279 - fingerprint_loss: 6.4534 - key_hat_loss: 0.4852 - val_loss: 117.6268 - val_fingerprint_loss: 39.6504 - val_key_hat_loss: 0.3892\n",
      "Epoch 49/50\n",
      "180000/180000 [==============================] - 178s 987us/step - loss: 103.4083 - fingerprint_loss: 6.5708 - key_hat_loss: 0.4835 - val_loss: 117.1989 - val_fingerprint_loss: 39.5940 - val_key_hat_loss: 0.3874\n",
      "Epoch 50/50\n",
      "180000/180000 [==============================] - 181s 1ms/step - loss: 101.9173 - fingerprint_loss: 6.5772 - key_hat_loss: 0.4760 - val_loss: 116.3165 - val_fingerprint_loss: 39.2491 - val_key_hat_loss: 0.3847\n",
      "Time to Fit the Model 9211.781424283981\n",
      "Ext Rate:   0.8761752350470094\n"
     ]
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "from keras import optimizers\n",
    "import datetime\n",
    "import time\n",
    "#regs = [1e-6]#0.01, 0.5, 1e-3, 1e-4, 1e-5, 1e-6 too. Looks like 1e-6 is the best (maybe 1e-7, 1e-8 too)\n",
    "models = []\n",
    "regularization = 1e-6\n",
    "weights = [1]#,100]#, 50, 100]\n",
    "weights_i = [200]#,600,900]#, 50, 100]\n",
    "for w in weights:\n",
    "    for w_i in weights_i:\n",
    "        beg_time = time.time()\n",
    "        x_fing_w, key_hat_w = w, w_i\n",
    "        epoch, batch = 50, 64\n",
    "        ## models[0] is for std =5, and models[1] is for std = 10\n",
    "\n",
    "        print(\"Date : \", datetime.datetime.now())\n",
    "\n",
    "        model_10 = get_encoder_decoder_conv_dense_slice(sample_size=sample_size, key_length=key_length, chunk=chunk, reg = regularization)\n",
    "        # losses.mean_squared_error# 0.001\n",
    "        adam = optimizers.Adam(lr = 1e-3, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0, amsgrad=False)\n",
    "\n",
    "        model_10.compile(optimizer=adam, loss={'fingerprint': mean_pred_loss, 'key_hat': losses.categorical_crossentropy},\n",
    "                  loss_weights={'fingerprint': x_fing_w, 'key_hat': key_hat_w})\n",
    "\n",
    "        # model.summary()\n",
    "        print(\"Model is Built and Compiled in %f\" % (time.time() - beg_time))\n",
    "        beg_time = time.time()\n",
    "\n",
    "        history = model_10.fit([X_train, train_keys, array_mult_train, array_sub_train, noise_for_train],\n",
    "              [y_train, train_keys], epochs=epoch, validation_split=0.1,\n",
    "              batch_size=batch)  # callbacks=callbacks_list, verbose=0)\n",
    "        models.append(model_10)\n",
    "\n",
    "        print(\"Time to Fit the Model\", time.time() - beg_time)\n",
    "\n",
    "        #### This is when we test encoder and decoder together using the same model: model_encoder_decoder\n",
    "\n",
    "        x_test = np.array(X_test_all[0:n_test-1]).reshape((-1, sample_size, 1))\n",
    "        key_options = selecting_valid_fingerprints(key_length=key_length)  # we use 100 keys.\n",
    "        test_keys = np.array(get_keys_for_fingerprinting_data(size=n_test, key_options=key_options))\n",
    "        noise_for_test = np.squeeze(noise_for_test)\n",
    "        pred = model_10.predict([x_test, test_keys, array_mult_test, array_sub_test, noise_for_test])\n",
    "\n",
    "        fingerprint_x22, keys_true = pred[0], pred[1]\n",
    "        ext_rate = compute_extract_rate(keys_true, true_keys=test_keys)\n",
    "\n",
    "        print(\"Ext Rate:  \", ext_rate)\n",
    "    \n",
    "\n",
    "\n",
    "# avg_d, max_d = compute_delay_on_each_packet(fingerprint_x2)\n",
    "\n",
    "#fingerprint_x2 = adjust_fingerprint_delays(fingerprint_x2)\n",
    "#avg_d, max_d = compute_delay_on_packets(fingerprint_x2)\n",
    "#learning_rates = [1e-3]# e-3 reached 0.21 when epochs = 50 [1e-2, e-3, e-5,e-4,1e-5, 5e-3, 2e-3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ext Rate:   0.8789757951590318\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(X_test_all[0:n_test-1]).reshape((-1, sample_size, 1))\n",
    "key_options = selecting_valid_fingerprints(key_length=key_length)  # we use 100 keys.\n",
    "test_keys = np.array(get_keys_for_fingerprinting_data(size=n_test, key_options=key_options))\n",
    "noise_for_test = np.squeeze(noise_for_test)\n",
    "pred = model_10.predict([x_test, test_keys, array_mult_test, array_sub_test, noise_for_test])\n",
    "\n",
    "fingerprint_x22, keys_true = pred[0], pred[1]\n",
    "ext_rate = compute_extract_rate(keys_true, true_keys=test_keys)\n",
    "\n",
    "print(\"Ext Rate:  \", ext_rate)# for 100,000 an 200,00 does not change the ext rate, but ks-test worsens shockinly\n",
    "# from 4 samples not pasisng th test to 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1031.2794], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(fingerprint_x22[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(400):\n",
    "    target = open(\"/home/fatemeh/MyProjects/Fingerprint/KS/Fingerprinted IPDs/lap_std10_alpha30_\" + str(i) + \".txt\", 'w')\n",
    "    array =  x_test[i] +fingerprint_x22[i]#+\n",
    "    array = np.squeeze(array)\n",
    "    write_array_to_file(array, target, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_same_keys_data(size, key):\n",
    "    selected_keys = []\n",
    "    for i in range(size):\n",
    "        selected_keys.append(key)\n",
    "\n",
    "    return selected_keys\n",
    "model_10 = models[1]\n",
    "x_test = np.array(X_test_all[0:n_test]).reshape((-1, sample_size, 1))\n",
    "key_options = selecting_valid_fingerprints(key_length=key_length)  # we use 100 keys.\n",
    "test_keys = np.array(get_same_keys_data(size=n_test, key=key_options[120]))\n",
    "noise_for_test = np.squeeze(noise_for_test)\n",
    "pred = model_10.predict([x_test, test_keys, array_mult_test, array_sub_test, noise_for_test])\n",
    "\n",
    "fingerprint_x22, keys_true = pred[0], pred[1]\n",
    "ext_rate = compute_extract_rate(keys_true, true_keys=test_keys)\n",
    "\n",
    "print(\"Ext Rate:  \", ext_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_10 = models[1]\n",
    "x_test = np.array(X_test_all[0:n_test]).reshape((-1, sample_size, 1))\n",
    "key_options = selecting_valid_fingerprints(key_length=key_length)  # we use 100 keys.\n",
    "test_keys = np.array(get_keys_for_fingerprinting_data(size=n_test, key_options=key_options))\n",
    "noise_for_test = np.squeeze(noise_for_test)\n",
    "pred = model_10.predict([x_test, test_keys, array_mult_test, array_sub_test, noise_for_test])\n",
    "\n",
    "fingerprint_x22, keys_true = pred[0], pred[1]\n",
    "ext_rate = compute_extract_rate(keys_true, true_keys=test_keys)\n",
    "\n",
    "print(\"Ext Rate:  \", ext_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#KS TEST, va CDF and PDF... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plot training & validation loss values\n",
    "#plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_key_hat_loss'],marker='o',markersize=12)\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Key_hat validation loss'])#Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise = np.array(noise_for_test[0:n_test]).reshape((-1, sample_size, 1))\n",
    "fing_ipds = fingerprint_x22 + x_test\n",
    "non_fing_ipds = noise + x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjust_fingerprint_delays(fingerprint_x2):\n",
    "    number_of_train, sample_size = len(fingerprint_x2), len(fingerprint_x2[0])\n",
    "    for f in range(0, len(fingerprint_x2)):\n",
    "            delay, min_delay = 0, 0\n",
    "            for p in range(0, len(fingerprint_x2[f])):\n",
    "                    delay += fingerprint_x2[f][p][0]\n",
    "                    #print(delay)\n",
    "                    if delay < min_delay:\n",
    "                        min_delay = delay \n",
    "            fingerprint_x2[f][0][0] -= 1.001 * min_delay\n",
    "           # print(\"min\",min_delay)\n",
    "           # break\n",
    "    return fingerprint_x2\n",
    "\n",
    "def compute_delay_on_packets(fingerprint_x2):\n",
    "#######Compute the average delay on each packet.        \n",
    "    average_delay = []\n",
    "    max_d, pos = 0, 0\n",
    "    for fing in fingerprint_x2:\n",
    "        delay, neg = 0, 0\n",
    "        delays = []\n",
    "        for n in fing:\n",
    "            delay += n[0]\n",
    "            if delay > max_d:\n",
    "                max_d = delay\n",
    "            delays.append(delay)\n",
    "\n",
    "            if delay < 0:\n",
    "                neg += 1\n",
    "        if neg == 0:\n",
    "            pos += 1\n",
    "            average_delay.append(sum(delays)/sample_size)\n",
    "   # print(sum(delays)/sample_size,\" Average Delay\")\n",
    "   # print(sample_size, number_of_train)\n",
    "    print(\"Negative delay: \",pos, \"Average delay for them:\",sum(average_delay)/pos, \"Max Delay: \", max_d)\n",
    "    return sum(average_delay)/pos, max_d\n",
    "fingerprint_x2 = adjust_fingerprint_delays(fingerprint_x2)\n",
    "avg_d, max_d = compute_delay_on_packets(fingerprint_x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise_for_test = np.array(noise_for_test[0:n_test]).reshape((-1, sample_size, 1))\n",
    "fingerprinted_ipds =  x_test + noise_for_test + fingerprint_x22 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: I want to check to see if I 0 the negative fingerprints, I can have the same extraction rate.\n",
    "model_decoder = load_decoder(key_length, sample_size)\n",
    "#model_decoder.set_weights(model.get_weights())\n",
    "beg_time = time.time()\n",
    "i = 0\n",
    "for layer in model_10.layers:\n",
    "  #  w_target=layer.get_weights()\n",
    "    for dec_layer in model_decoder.layers:\n",
    "        if layer.name ==dec_layer.name:\n",
    "            print(layer.name, i)\n",
    "            dec_layer.set_weights(layer.get_weights())\n",
    "    i += 1\n",
    "print(\"Time it takes to load the decoder: \", time.time() - beg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_fingerprinted_ipds():\n",
    "    path = '/home/fatemeh/MyProjects/Fingerprint/encoder/ext_ipds/'\n",
    "    all_ipds = []\n",
    "    for i in range(11):\n",
    "        string_ipds = read_from_file(path + str(i) + \".txt\").split(\" \")\n",
    "        ipds = convert_stringArrays_to_floatArray(string_ipds)\n",
    "        all_ipds.append(ipds)\n",
    "    return all_ipds\n",
    "all_ipds = read_fingerprinted_ipds()\n",
    "fingerprinted_ipds=np.array(all_ipds).reshape((-1, sample_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "noise_for_test = np.squeeze(noise_for_test)\n",
    "noise_for_test = get_noise_simulation_array(n_test, std=1, sample_size=sample_size)\n",
    "noise_for_test=np.array(noise_for_test[0:n_test]).reshape((-1, sample_size, 1))\n",
    "fingerprinted_ipds = fingerprint_x2 + x_test + noise_for_test\n",
    "'''\n",
    "\n",
    "\n",
    "key_hat = model_decoder.predict(fingerprinted_ipds)\n",
    "ext_rate = compute_extract_rate(key_hat, true_keys=test_keys)\n",
    "print(ext_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "# avg_d, max_d = compute_delay_on_each_packet(fingerprint_x2)\n",
    "rate=100\n",
    "with open('sep_results.csv', mode='a') as csv_file:\n",
    "    fieldnames = ['sample_size', 'key_length', \"number_training\", 'ext_rate', 'average_delay', 'max_delay',\n",
    "                  'packet_rate', 'std', 'max_train_noise','date']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "    #writer.writeheader()\n",
    "    writer.writerow(\n",
    "        {'sample_size': sample_size, 'key_length': key_length, \"number_training\": n_true_train, 'ext_rate': ext_rate,\n",
    "         'average_delay': avg_d, 'max_delay': max_d, 'packet_rate': rate, 'std': std, 'max_train_noise': max_fing_delay,'date': datetime.datetime.now()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(X_train[0][0:100])\n",
    "# print(fingerprint_x2[0][0:100])\n",
    "array_mult_test2, array_sub_test2, noise_for_test2 = get_arrays_mult_noise_sub(10,max_delay=5,chunk=10,std=5,sample_size=sample_size)\n",
    "\n",
    "d = 0\n",
    "import numpy as np\n",
    "for f in noise_for_test2:\n",
    "    dd = []\n",
    "    for p in f:\n",
    "        dd.append(p)\n",
    "    std = np.std(dd, axis=0)\n",
    "    print(std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ext_rate = compute_extract_rate(keys_true, true_keys = test_keys)\n",
    "\n",
    "print(\"Ext Rate:  \", ext_rate)\n",
    "compute_delay_on_each_packet(fingerprint_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras import optimizers\n",
    "n_false_train = 0\n",
    "x_fing_w, key_hat_w, epoch, batch = 1, 200, 50, 64\n",
    "\n",
    "beg_time = time.time()\n",
    "key_length = 100\n",
    "key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "sample_sizes = [1500, 1800]#, 600, 1200]\n",
    "#models = []\n",
    "trains = [60000, 10000]\n",
    "for sam in sample_sizes:\n",
    "    sample_size = sam\n",
    "    X_train_all = create_sample_size_dataset(all_ipds_for_train, sample_size = sample_size)\n",
    "    X_test_all = create_sample_size_dataset(all_ipds_for_test, sample_size = sample_size)\n",
    "    print(len(X_train_all),len(X_test_all), \"Numbre of training and testing data\")\n",
    "    model = get_encoder_decoder_conv_dense_slice(sample_size=sample_size, key_length=key_length, chunk=10)\n",
    "    ad = optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0, amsgrad=False)\n",
    "\n",
    "    model.compile(optimizer=ad, loss={'fingerprint':mean_pred_loss, 'key_hat': losses.categorical_crossentropy},\n",
    "                                    loss_weights={'fingerprint': x_fing_w, 'key_hat': key_hat_w})\n",
    "    X_train, y_train, train_keys = get_false_true_data(X_train_all, key_options)#get_only_true_data\n",
    "    train_keys = np.array(train_keys)\n",
    "    n_test = 4000\n",
    "    for t in trains:\n",
    "        # model.summary()\n",
    "        beg_time = time.time()\n",
    "        array_mult_train, array_sub_train, noise_for_train = get_arrays_mult_noise_sub(t,max_delay=5,chunk=10,std=1,sample_size=sample_size)\n",
    "        model.fit([X_train[0:t], train_keys[0:t], array_mult_train[0:t], array_sub_train[0:t], \n",
    "                   noise_for_train[0:t]], [y_train[0:t], train_keys[0:t]], epochs=epoch,\n",
    "                  validation_split=0.1, batch_size=batch,verbose =1)#, validation_split=0.1,callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "        print(\"Time to Fit the Model\", time.time() - beg_time)\n",
    "        models.append(model) \n",
    "        array_mult_test, array_sub_test, noise_for_test = get_arrays_mult_noise_sub(n_test,max_delay=5,chunk=10,std=1,sample_size=sample_size)\n",
    "\n",
    "        #### This is when we test encoder and decoder together using the same model: model_encoder_decoder\n",
    "        x_test = np.array(X_test_all[0:n_test]).reshape((-1, sample_size, 1))\n",
    "        key_options = selecting_valid_fingerprints(key_length = key_length)# we use 100 keys.\n",
    "        test_keys = np.array(get_keys_for_fingerprinting_data(size=n_test, key_options=key_options))\n",
    "        noise_for_test = np.squeeze(noise_for_test[0:n_test])\n",
    "        pred = model.predict([x_test, test_keys, array_mult_test, array_sub_test, noise_for_test])\n",
    "\n",
    "        fingerprint_x2, keys_true = pred[0],  pred[1]\n",
    "        ext_rate = compute_extract_rate(keys_true, true_keys = test_keys)\n",
    "\n",
    "        print(\"Ext Rate:  \", ext_rate)\n",
    "        compute_delay_on_each_packet(fingerprint_x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_test = 5000\n",
    "print(sample_size)\n",
    "array_mult_test, array_sub_test, noise_for_test = get_arrays_mult_noise_sub(n_test,max_delay=5,chunk=10,std=1,sample_size=sample_size)\n",
    "\n",
    "x_test = np.array(X_test_all[0:n_test]).reshape((-1, sample_size, 1))\n",
    "key_options = selecting_valid_fingerprints(key_length = 200)# we use 100 keys.\n",
    "test_keys = np.array(get_keys_for_fingerprinting_data(size=n_test, key_options=key_options))\n",
    "noise_for_test = np.squeeze(noise_for_test[0:n_test])\n",
    "pred = model.predict([x_test, test_keys, array_mult_test, array_sub_test, noise_for_test])\n",
    "\n",
    "fingerprint_x2, keys_true = pred[0],  pred[1]\n",
    "ext_rate = compute_extract_rate(keys_true, true_keys = test_keys)\n",
    "\n",
    "print(\"Ext Rate:  \", ext_rate)\n",
    "compute_delay_on_each_packet(fingerprint_x2)\n",
    "'''\n",
    "sample size = 1800, n_train = 10000, and key = 100:\n",
    "1800\n",
    "Ext Rate:   0.0842\n",
    "52.90532826509741\n",
    "1800 4444\n",
    "4444 47.894631279740935 1800 Max Delay:  182.74153697490692\n",
    "when we only change n_train = 60000:\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def decide_if_fingerprinted(keys, threshold):\n",
    "    fing = 0\n",
    "    for key in keys:\n",
    "        index = np.argmax(key)\n",
    "        if key[index] > threshold and index > 0:\n",
    "            fing += 1\n",
    "    return fing / float(len(keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Loading encoder takes too much time (hours), so we just use the model_encoder_decoder for encoding.\n",
    "model_decoder = load_decoder(key_length, sample_size)\n",
    "decoder_weights = []\n",
    "j = 0\n",
    "for i in range(0, 24):\n",
    "    if 'dec' in model.layers[-(24 - i)].name or 'key_hat' in model.layers[-(24 - i)].name:\n",
    "        model_decoder.layers[j].set_weights(model.layers[-(24 - i)].get_weights())\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise_for_test = noise_for_test.reshape((-1, sample_size, 1))\n",
    "\n",
    "output_fin = noise_for_test[0:n_test] + x_test\n",
    "keys_true_fp = model_decoder.predict([output_fin])\n",
    "\n",
    "###### True positve:\n",
    "output_fin = noise_for_test[0:n_test] + x_test + fingerprint_x2\n",
    "\n",
    "keys_true_tp = model_decoder.predict([output_fin])\n",
    "ext_rate = compute_extract_rate(keys_true_tp, test_keys)\n",
    "thresholds = [0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]\n",
    "\n",
    "\n",
    "for t in thresholds:\n",
    "    fp = decide_if_fingerprinted(keys_true_fp, t)\n",
    "    tp = decide_if_fingerprinted(keys_true_tp, t)\n",
    "    \n",
    "    print(fp, tp)\n",
    "print(ext_rate, 'Extraction Rate')\n",
    "'''\n",
    "\n",
    "key = 100 training with 1/100 number of false data. sample size = 1800, number of training=20000\n",
    "0.2395 0.9845\n",
    "0.156 0.9695\n",
    "0.093 0.9535\n",
    "0.043 0.931\n",
    "0.014 0.8975\n",
    "0.0055 0.867\n",
    "0.0005 0.766\n",
    "0.9685 Extraction Rate\n",
    "Ext Rate:   0.969\n",
    "1800 2000\n",
    "2000 138.32507355565957 1800 Max Delay:  556.0820367336273\n",
    "##############################################################################################\n",
    "\n",
    "\n",
    "sample size 3300, training data = 48000, key = 1000\n",
    "\n",
    "0.293 0.9925\n",
    "0.1925 0.98\n",
    "0.115 0.9685\n",
    "0.0655 0.941\n",
    "0.022 0.905\n",
    "0.007 0.8525\n",
    "0.0 0.734\n",
    "0.9695 Extraction Rate\n",
    "2000 80.0513701567251 3300 Max Delay:  369.6583148241043\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_false_train = 0\n",
    "x_fing_w, key_hat_w, epoch, batch = 1, 200, 100, 64\n",
    "model_name = str(sample_size) + \"_\" + str(key_length) + \"_\" + str(\n",
    "    n_true_train) + \"_\" + str(n_false_train) + \"_\" + str(epoch) + \"_\" + str(x_fing_w) + \"_\" + str(key_hat_w)\n",
    "\n",
    "beg_time = time.time()\n",
    "#models_key_length = []\n",
    "keys = [100]\n",
    "n_true_train = 30000\n",
    "for k in keys:\n",
    "\n",
    "    key_options = selecting_valid_fingerprints(key_length = k)# we use 100 keys.\n",
    "    X_train, y_train, train_keys = get_false_true_training(X_train_all[0:n_true_train], key_options)\n",
    "    train_keys = np.array(train_keys)\n",
    "    print(\"Finished radinf\")\n",
    "\n",
    "    model= get_encoder_decoder_conv_dense_slice(sample_size=sample_size, key_length=k)\n",
    "    #losses.mean_squared_error\n",
    "    ad = optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0, amsgrad=False)\n",
    "\n",
    "    model.compile(optimizer=ad, loss={'fingerprint':mean_pred_loss, 'key_hat': losses.categorical_crossentropy},\n",
    "                                    loss_weights={'fingerprint': x_fing_w, 'key_hat': key_hat_w})\n",
    "\n",
    "\n",
    "    # model.summary()\n",
    "    print(\"Model %s is Built and Compiled in %f\" % (model_name ,time.time() - beg_time))\n",
    "    beg_time = time.time()\n",
    "\n",
    "    model.fit([X_train, train_keys, array_mult_train[0:n_true_train], array_sub_train[0:n_true_train], noise_for_train[0:n_true_train]], [y_train, train_keys], epochs=epoch, validation_split=0.1, batch_size=batch)#, validation_split=0.1,callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "    print(\"Time to Fit the Model\", time.time() - beg_time)\n",
    "\n",
    "    \n",
    "    models_key_length.append(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_for_results = '/home/fatemeh/Dropbox/Fingerprint/Results/'\n",
    "def compute_ROC_data(n_train):\n",
    "    target_name = open(path_for_results + str(n_train)+\"_\" + str(sample_size)+\"_\"+str(key_length) + '.txt', 'w')\n",
    "    sample_size = 900\n",
    "    key_length = 100\n",
    "    n_test = 5000\n",
    "    thresholds = [0.6, 0.7, 0.8, 0.9]\n",
    "    X = create_sample_size_dataset(all_ipds, sample_size = sample_size)\n",
    "    key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    model_decoder, model_encoder = load_model_for_testing(key_length, sample_size, n_train)\n",
    "    X_test = np.expand_dims(X[n_train:n_train + n_test], axis=1)\n",
    "    test_keys = np.expand_dims(get_fingerprint_for_data(size = n_test, key_options = key_options), axis=1)\n",
    "    fingerprint_x = model_encoder.predict([test_keys])\n",
    "    false_poses, true_poses = [], []\n",
    "    \n",
    "     ########## True positve:\n",
    "    output_fin = add_gussian_noise_to_ipds_fingerprinted(fingerprint = fingerprint_x, x_test = X_test, std =10)\n",
    "    keys_true = model_decoder.predict([output_fin])\n",
    "\n",
    "    ########## False positve: \n",
    "    output_non = add_gussian_noise_to_ipds_non_fingerprinted(X_test, std =10)\n",
    "    keys_false = model_decoder.predict([output_non])\n",
    "    for t in thresholds:\n",
    "        true_pos = decide_if_fingerprinted(keys_true, threshold = t)\n",
    "        false_pos = decide_if_fingerprinted(keys_false, threshold = t)\n",
    "        false_poses.append(false_pos)\n",
    "        true_poses.append(true_pos)\n",
    "        \n",
    "    write_array_to_file(array = false_poses, target =target_name, delimiter =' ')\n",
    "    write_array_to_file(array = true_poses, target =target_name, delimiter =' ')\n",
    "    target_name.close()\n",
    "    \n",
    "compute_ROC_data(n_train=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_impact_of_jitter():\n",
    "    sample_size = 600\n",
    "    key_length = 10\n",
    "    n_train = 50000\n",
    "    n_test = 5000\n",
    "    X = create_sample_size_dataset(all_ipds, sample_size = sample_size)\n",
    "    key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "\n",
    "    target_name = open(path_for_results + str(n_train)+\"_\" + str(sample_size)+\"_\"+str(key_length) + '.txt', 'w')\n",
    "\n",
    "    jitters = [1, 10, 50, 100]\n",
    "   \n",
    "    model_decoder, model_encoder = load_model_for_testing(key_length, sample_size, n_train)\n",
    "    X_test = np.expand_dims(X[n_train:n_train + n_test], axis=1)\n",
    "    test_keys = np.expand_dims(get_fingerprint_for_data(size = n_test, key_options = key_options), axis=1)\n",
    "    fingerprint_x = model_encoder.predict([test_keys])\n",
    "    false_poses, true_poses, ext_rates = [], [], []\n",
    "\n",
    "    for std in jitters:      \n",
    "        ########## True positve:\n",
    "        output_fin = add_gussian_noise_to_ipds_fingerprinted(fingerprint = fingerprint_x, x_test = X_test, std =std)\n",
    "        keys_true = model_decoder.predict([output_fin])\n",
    "        true_pos = decide_if_fingerprinted(keys_true, threshold=4)\n",
    "        key_pred = extract_keys_from_key_hat(keys_true)\n",
    "        error_rate = compute_error_rate_flowwise(predict_key = key_pred, true_key = test_keys)\n",
    "\n",
    "        ########## False positve: \n",
    "        output_non = add_gussian_noise_to_ipds_non_fingerprinted(X_test, std =std)\n",
    "        keys_false = model_decoder.predict([output_non])\n",
    "        false_pos = decide_if_fingerprinted(keys_false, threshold=4)\n",
    "        false_poses.append(false_pos)\n",
    "        true_poses.append(true_pos)\n",
    "        ext_rates.append(1 - error_rate)\n",
    "    write_array_to_file(array = ext_rates, target =target_name, delimiter =' ')\n",
    "    write_array_to_file(array = false_poses, target =target_name, delimiter =' ')\n",
    "    write_array_to_file(array = true_poses, target =target_name, delimiter =' ')\n",
    "    target_name.close()\n",
    "# compute_impact_of_jitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_fing_w, key_hat_w, epoch = 1, 50, 100\n",
    "def call_fit_load_eval_Main():\n",
    "    n_all_true_trains =[5000]# [5000, 10000, 20000, 50000]#5000,, \n",
    "    sample_sizes = [600]#[400, 200, 600]\n",
    "    key_lengths = [10]#, 15, 20]\n",
    "\n",
    "    for sample_size in sample_sizes:\n",
    "        X = create_sample_size_dataset(all_ipds, sample_size = sample_size)\n",
    "        for key_length in key_lengths:\n",
    "            key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "            false_poses, true_poses, ext_rates = [], [], []\n",
    "            target_name = open(path_for_results + str(sample_size)+\"_\"+str(key_length) + '.txt', 'w')\n",
    "\n",
    "            for train_number in n_all_true_trains:\n",
    "                false_pos, true_pos, ext_rate = fit_model_load_evaulte(n_true_train =train_number, key_length=key_length, sample_size=sample_size,X=X,key_options=key_options)\n",
    "                false_poses.append(false_pos)\n",
    "                true_poses.append(true_pos)\n",
    "                ext_rates.append(ext_rate)\n",
    "                print(false_pos, true_pos, ext_rate)\n",
    "            write_array_to_file(array = ext_rates, target =target_name, delimiter =' ')\n",
    "            write_array_to_file(array = false_poses, target =target_name, delimiter =' ')\n",
    "            write_array_to_file(array = true_poses, target =target_name, delimiter =' ')\n",
    "            target_name.close()\n",
    "call_fit_load_eval_Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reload_model_for_more_epochs():\n",
    "    sample_size, key_length, n_true_train, epoch = 600, 10, 50000, 250\n",
    "    n_false_train = int(n_true_train/10)\n",
    "    x_fing_w, key_hat_w = 1, 50\n",
    "    model_name = \"march_10\" + str(sample_size) + \"_\" + str(key_length) + \"_\" + str(\n",
    "    n_true_train) + \"_\" + str(n_false_train) + \"_\" + str(epoch) + \"_\" + str(x_fing_w) + \"_\" + str(key_hat_w)\n",
    "\n",
    "    model = load_NN_model(path + model_name)\n",
    "    model_encoder_decoder.compile(optimizer='adam', \n",
    "                              loss=losses.mean_absolute_error,\n",
    "                                  loss_weights={'fingerprint':x_fing_w, 'key_hat':key_hat_w})\n",
    "\n",
    "    model_encoder_decoder.fit([X_train, training_keys], [y_train, training_keys],\n",
    "                        batch_size = 64, epochs = epoch + 250, verbose = 0)\n",
    "    #save_model_weights(model_encoder_decoder, name=model_name)\n",
    "# reload_model_for_more_epochs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_test_all_Main():\n",
    "    n_all_true_trains = [10000]# 5000, 10000, 20000, 50000]\n",
    "    sample_sizes = [600]\n",
    "    key_lengths = [10]\n",
    "    x_fing_w, key_hat_w, epoch = 1, 50, 100\n",
    "    n_test = 1000\n",
    "    for sample_size in sample_sizes:\n",
    "            \n",
    "            X = create_sample_size_dataset(all_ipds_for_test, sample_size = sample_size)\n",
    "            for key_length in key_lengths:\n",
    "                key_options = selecting_valid_fingerprints(key_length = key_length)\n",
    "                false_poses, true_poses, ext_rates = [], [], []\n",
    "                #target_name = open('/home/fatemeh/Dropbox/Fingerprint/Results/500_' + str(sample_size)+\"_\"+str(key_length) + '.txt', 'w')\n",
    "\n",
    "                for train_number in n_all_true_trains:\n",
    "                    model_decoder, model_encoder = load_model_for_testing(key_length, sample_size, train_number)\n",
    "                    false_pos, true_pos, ext_rate = evalute_encoder_decoder(model_decoder,model_encoder, X, sample_size, key_length,\n",
    "                                                                            key_options, train_number, int(train_number/10), n_test=n_test)\n",
    "                    false_poses.append(false_pos)\n",
    "                    true_poses.append(true_pos)\n",
    "                    ext_rates.append(ext_rate)\n",
    "                    print(sample_size, key_length, train_number, \"Result: \", false_pos, true_pos, ext_rate)\n",
    "#                 write_array_to_file(array = ext_rates, target =target_name, delimiter =' ')\n",
    "#                 write_array_to_file(array = false_poses, target =target_name, delimiter =' ')\n",
    "#                 write_array_to_file(array = true_poses, target =target_name, delimiter =' ')\n",
    "#                 target_name.close() \n",
    "load_test_all_Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_true_trains = [5000, 10000, 20000, 50000, 100000]\n",
    "sample_size, key_length = 600, 10\n",
    "epoch = 250\n",
    "for n_true_train in n_true_trains:\n",
    "    n_false_train = int(n_true_train/10)\n",
    "    key_hat_w, x_hat_w = 50, 1\n",
    "    model_name = \"march10_\" + str(sample_size) + \"_\" + str(key_length) + \"_\" + str(\n",
    "        n_true_train) + \"_\" + str(n_false_train) + \"_\" + str(epoch) + \"_\" + str(x_hat_w) + \"_\" + str(key_hat_w)\n",
    "\n",
    "    model = load_NN_model(path + model_name)\n",
    "\n",
    "    X_train, y_train, training_keys = get_false_true_training(n_true_train, n_false_train, key_length, X, key_options)\n",
    "    print(\"Finished reading dataset\")\n",
    "\n",
    "    model.compile(optimizer='adam', \n",
    "                                  loss=losses.mean_absolute_error,\n",
    "                                      loss_weights={'fingerprint':1, 'key_hat':50})\n",
    "    model.fit([X_train, training_keys], [y_train, training_keys],\n",
    "                            batch_size = 64, epochs = epoch, verbose = 0)\n",
    "    \n",
    "    model_name = \"march10_\" + str(sample_size) + \"_\" + str(key_length) + \"_\" + str(\n",
    "        n_true_train) + \"_\" + str(n_false_train) + \"_\" + str(500) + \"_\" + str(x_hat_w) + \"_\" + str(key_hat_w)\n",
    "\n",
    "    save_model_weights(model, name= model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_encoder(key_length, sample_size):\n",
    "    chunk, p = 10, 0\n",
    "    Input_ipd = Input(shape=(sample_size, 1), name='input1')  # this is needed just for the decoding\n",
    "    Input_key = Input(shape=(key_length,), name='input2')\n",
    "    fingerprint_mult = Input(shape=(chunk,), name='input3')\n",
    "    fingerprint_sub = Input(shape=(chunk,), name='input4')\n",
    "    \n",
    "    ipd = Flatten(name =\"ipd_flatten1\")(Input_ipd)\n",
    "    outputs = []\n",
    "    \n",
    "    quant = int(sample_size/chunk)\n",
    "    def slice(x):\n",
    "        return x[:, p * chunk:(1 + p) * chunk]\n",
    "    \n",
    "    key1 = Dense(32, name='key1')(Input_key)\n",
    "\n",
    "    sliced_ipd = Lambda(slice)(ipd)\n",
    "    x_fingerprint = sliced_ipd\n",
    "    for i in range(0, quant):\n",
    "        sliced_ipd = Lambda(slice)(ipd)\n",
    "        ss = Concatenate(name = 'concat'+ str(p))([x_fingerprint, sliced_ipd]) \n",
    "        ipd1 = Dense(32, name = 'dense'+ str(p))(ss)\n",
    "        batch_2 = BatchNormalization(name = 'batch'+ str(p))(ipd1)\n",
    "        relu_2 = Activation('relu', name = 'act'+ str(p))(batch_2)\n",
    "        \n",
    "        ipds_merged_all = Concatenate(name = 'concat_key_'+ str(p))([relu_2, key1])\n",
    "        dense_enc1 = Dense(64, name = 'dense_enc1' + str(p))(ipds_merged_all)\n",
    "        batch_2 = BatchNormalization(name = 'batch2_'+ str(p))(dense_enc1)\n",
    "        relu_2 = Activation('relu', name = 'act2_'+ str(p))(batch_2)\n",
    "        dense_drop_enc1 = Dropout(0.3, name = 'dense_drop_enc1' + str(p))(relu_2)\n",
    "        \n",
    "        x_fingerprint_sig = Dense(chunk, name = 'fingerprint_sig' + str(p), activation = 'sigmoid')(dense_drop_enc1)\n",
    "        x_fingerprint_mult = Multiply(name = 'fingerprint_mult' + str(p))([x_fingerprint_sig, fingerprint_mult])\n",
    "        x_fingerprint = Add(name = 'ipd_delay' + str(p))([x_fingerprint_mult, fingerprint_sub])\n",
    "        outputs.append(x_fingerprint)\n",
    "        p += 1\n",
    "    x_fingerprint = Concatenate(name = 'fingerprint2')(outputs)\n",
    "    x_fingerprint_output = Reshape((sample_size,1), name='fingerprint')(x_fingerprint)\n",
    "    model_encoder = Model(inputs=[Input_key,Input_ipd,  fingerprint_mult, fingerprint_sub], outputs=[x_fingerprint_output])\n",
    "    #model_encoder.load_weights(filepath=path + model_name + \".h5\", by_name=True)\n",
    "    return model_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
